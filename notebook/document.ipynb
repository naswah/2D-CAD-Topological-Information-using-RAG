{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae9e32d",
   "metadata": {},
   "source": [
    "# DATA INJESTION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cff22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c272f778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'sourcce': 'example.txt', 'pages': 1, 'author': ' me', 'date created': '2026-01-08'}, page_content='This is page content to create RAG')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc= Document(\n",
    "    page_content= \"This is page content to create RAG\",\n",
    "    metadata ={\n",
    "        \"sourcce\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\":\" me\",\n",
    "        \"date created\": \"2026-01-08\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83b06e",
   "metadata": {},
   "source": [
    "Create a simple txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79568059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok= True)\n",
    "\n",
    "sample_texts={\n",
    "\n",
    "   \"../data/text_files/python_intro.txt\": \"\"\"---------Python Introduction-------\n",
    "\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity, readability, and versatility. Created by Guido van Rossum and first released in 1991, Python emphasizes clean syntax and code readability, making it one of the most beginner-friendly languages while still being powerful enough for advanced applications.\n",
    "\n",
    "One of Python's biggest strengths is its rich ecosystem of libraries and frameworks. It is widely used in data science, machine learning, artificial intelligence, web development, automation, scientific computing, and software testing. Popular libraries such as NumPy, Pandas, TensorFlow, PyTorch, OpenCV, and Matplotlib make Python a top choice for AI and data-related projects, while frameworks like Django and Flask are used for backend web development.\n",
    "\n",
    "Python supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It is also platform-independent, meaning the same Python code can run on Windows, macOS, and Linux with minimal changes.\n",
    "\n",
    "Because of its large community support, extensive documentation, and rapid development capabilities, Python is widely used in academia, research, startups, and industry. Overall, Python is a powerful and flexible programming language that enables faster development and easier problem-solving across a wide range of domains.\"\"\",\n",
    "\n",
    "    \"../data/text_files/ml_intro.txt\":\n",
    "\n",
    " \"\"\"Machine Learning (ML) is a branch of artificial intelligence that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed. It focuses on building algorithms that improve performance automatically through experience.\n",
    "\n",
    "Machine learning is commonly categorized into supervised learning, unsupervised learning, and reinforcement learning. It is widely used in applications such as image and speech recognition, recommendation systems, fraud detection, medical diagnosis, and natural language processing.\n",
    "\n",
    "Using techniques like regression, classification, clustering, and deep learning, machine learning helps extract meaningful insights from large datasets and supports data-driven decision making. \"\"\"\n",
    "}\n",
    "\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "print(\"Sample file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380ef5b",
   "metadata": {},
   "source": [
    "REad the file using text loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84f5ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"---------Python Introduction-------\\n\\n\\nPython is a high-level, interpreted programming language known for its simplicity, readability, and versatility. Created by Guido van Rossum and first released in 1991, Python emphasizes clean syntax and code readability, making it one of the most beginner-friendly languages while still being powerful enough for advanced applications.\\n\\nOne of Python's biggest strengths is its rich ecosystem of libraries and frameworks. It is widely used in data science, machine learning, artificial intelligence, web development, automation, scientific computing, and software testing. Popular libraries such as NumPy, Pandas, TensorFlow, PyTorch, OpenCV, and Matplotlib make Python a top choice for AI and data-related projects, while frameworks like Django and Flask are used for backend web development.\\n\\nPython supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It is also platform-independent, meaning the same Python code can run on Windows, macOS, and Linux with minimal changes.\\n\\nBecause of its large community support, extensive documentation, and rapid development capabilities, Python is widely used in academia, research, startups, and industry. Overall, Python is a powerful and flexible programming language that enables faster development and easier problem-solving across a wide range of domains.\")]\n"
     ]
    }
   ],
   "source": [
    "#from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader= TextLoader(\"../data/text_files/python_intro.txt\", encoding = \"utf-8\")\n",
    "document= loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfe775",
   "metadata": {},
   "source": [
    "REad file using directory loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6db45dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content=\"---------Python Introduction-------\\n\\n\\nPython is a high-level, interpreted programming language known for its simplicity, readability, and versatility. Created by Guido van Rossum and first released in 1991, Python emphasizes clean syntax and code readability, making it one of the most beginner-friendly languages while still being powerful enough for advanced applications.\\n\\nOne of Python's biggest strengths is its rich ecosystem of libraries and frameworks. It is widely used in data science, machine learning, artificial intelligence, web development, automation, scientific computing, and software testing. Popular libraries such as NumPy, Pandas, TensorFlow, PyTorch, OpenCV, and Matplotlib make Python a top choice for AI and data-related projects, while frameworks like Django and Flask are used for backend web development.\\n\\nPython supports multiple programming paradigms, including procedural, object-oriented, and functional programming. It is also platform-independent, meaning the same Python code can run on Windows, macOS, and Linux with minimal changes.\\n\\nBecause of its large community support, extensive documentation, and rapid development capabilities, Python is widely used in academia, research, startups, and industry. Overall, Python is a powerful and flexible programming language that enables faster development and easier problem-solving across a wide range of domains.\"), Document(metadata={'source': '../data/text_files/ml_intro.txt'}, page_content='Machine Learning (ML) is a branch of artificial intelligence that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed. It focuses on building algorithms that improve performance automatically through experience.\\n\\nMachine learning is commonly categorized into supervised learning, unsupervised learning, and reinforcement learning. It is widely used in applications such as image and speech recognition, recommendation systems, fraud detection, medical diagnosis, and natural language processing.\\n\\nUsing techniques like regression, classification, clustering, and deep learning, machine learning helps extract meaningful insights from large datasets and supports data-driven decision making. ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader= DirectoryLoader(\n",
    "    \"../data/text_files\", \n",
    "    glob= \"**/*.txt\", #Pattern to match\n",
    "    loader_cls= TextLoader,\n",
    "    loader_kwargs= {'encoding': 'utf-8'},\n",
    "    show_progress= False\n",
    ")\n",
    "dir_doc= dir_loader.load()\n",
    "print(dir_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd170ec5",
   "metadata": {},
   "source": [
    "load pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8870332d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 0}, page_content='AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON\\nAGENTIC RAG\\nAditi Singh\\nDepartment of Computer Science\\nCleveland State University\\nCleveland, OH, USA\\na.singh22@csuohio.edu\\nAbul Ehtesham\\nThe Davey Tree Expert Company\\nKent, OH, USA\\nabul.ehtesham@davey.com\\nSaket Kumar\\nThe MathWorks Inc\\nNatick, MA, USA\\nsaketk@mathworks.com\\nTala Talaei Khoei\\nKhoury College of Computer Science\\nRoux Institute at Northeastern University\\nPortland, ME, USA\\nt.talaeikhoei@northeastern.edu\\nABSTRACT\\nLarge Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human-\\nlike text generation and natural language understanding. However, their reliance on static training\\ndata limits their ability to respond to dynamic, real-time queries, resulting in outdated or inaccurate\\noutputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\\nintegrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite\\nits promise, traditional RAG systems are constrained by static workflows and lack the adaptability\\nrequired for multi-step reasoning and complex task management.\\nAgentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding\\nautonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflec-\\ntion, planning, tool use, and multi-agent collaboration to dynamically manage retrieval strategies,\\niteratively refine contextual understanding, and adapt workflows through clearly defined operational\\nstructures ranging from sequential steps to adaptive collaboration. This integration enables Agentic\\nRAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\\napplications.\\nThis survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational\\nprinciples and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG archi-\\ntectures, highlights key applications in industries such as healthcare, finance, and education, and exam-\\nines practical implementation strategies. Additionally, it addresses challenges in scaling these systems,\\nensuring ethical decision-making, and optimizing performance for real-world applications, while\\nproviding detailed insights into frameworks and tools for implementing Agentic RAG 1. The GitHub\\nlink for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey.\\nKeywords Large Language Models (LLMs) · Artificial Intelligence (AI) · Natural Language Understanding ·\\nRetrieval-Augmented Generation (RAG) · Agentic RAG · Autonomous AI Agents · Reflection · Planning · Tool\\nUse · Multi-Agent Collaboration · Agentic Patterns · Contextual Understanding · Dynamic Adaptability · Scalability ·\\nReal-Time Data Retrieval · Taxonomy of Agentic RAG · Healthcare Applications · Finance Applications · Educational\\nApplications · Ethical AI Decision-Making · Performance Optimization · Multi-Step Reasoning\\n1GitHub link: https://github.com/asinghcsu/AgenticRAG-Survey\\narXiv:2501.09136v3  [cs.AI]  4 Feb 2025'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 1}, page_content='1\\nIntroduction\\nLarge Language Models (LLMs) [1, 2] [3], such as OpenAI’s GPT-4, Google’s PaLM, and Meta’s LLaMA, have signifi-\\ncantly transformed artificial intelligence (AI) with their ability to generate human-like text and perform complex natural\\nlanguage processing tasks. These models have driven innovation across diverse domains, including conversational\\nagents [4], automated content creation, and real-time translation. Recent advancements have extended their capabilities\\nto multimodal tasks, such as text-to-image and text-to-video generation [5], enabling the creation and editing of videos\\nand images from detailed prompts [6], which broadens the potential applications of generative AI.\\nDespite these advancements, LLMs face significant limitations due to their reliance on static pre-training data. This\\nreliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world\\nscenarios. These challenges emphasize the need for systems that can integrate real-time data and dynamically refine\\nresponses to maintain contextual relevance and accuracy.\\nRetrieval-Augmented Generation (RAG) [8, 9] emerged as a promising solution to these challenges. By combining\\nthe generative capabilities of LLMs with external retrieval mechanisms [10], RAG systems enhance the relevance and\\ntimeliness of responses. These systems retrieve real-time information from sources such as knowledge bases [11], APIs,\\nor the web, effectively bridging the gap between static training data and the demands of dynamic applications. However,\\ntraditional RAG workflows remain limited by their linear and static design, which restricts their ability to perform\\ncomplex multi-step reasoning, integrate deep contextual understanding, and iteratively refine responses.\\nThe evolution of agents [12] has significantly enhanced the capabilities of AI systems. Modern agents, including\\nLLM-powered and mobile agents [13], are intelligent entities capable of perceiving, reasoning, and autonomously\\nexecuting tasks. These agents leverage agentic patterns, such as reflection [14], planning [15], tool use, and multi-agent\\ncollaboration [16], to enhance decision-making and adaptability.\\nFurthermore, these agents employ agentic workflow patterns [12, 13], such as prompt chaining, routing, parallelization,\\norchestrator-worker models, and evaluator-optimizer , to structure and optimize task execution. By integrating these\\npatterns, Agentic RAG systems can efficiently manage dynamic workflows and address complex problem-solving\\nscenarios. The convergence of RAG and agentic intelligence has given rise to Agentic Retrieval-Augmented Generation\\n(Agentic RAG) [14], a paradigm that integrates agents into the RAG pipeline. Agentic RAG enables dynamic retrieval\\nstrategies, contextual understanding, and iterative refinement [15], allowing for adaptive and efficient information\\nprocessing. Unlike traditional RAG, Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant\\ninformation, and refine responses, excelling in scenarios requiring precision and adaptability. The overview of Agentic\\nRAG is in figure 1.\\nThis survey explores the foundational principles, taxonomy, and applications of Agentic RAG. It provides a comprehen-\\nsive overview of RAG paradigms, such as Naïve RAG, Modular RAG, and Graph RAG [16], alongside their evolution\\ninto Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications\\nacross domains such as healthcare [17, 18], finance, and education [19], and insights into implementation strategies,\\nbenchmarks, and ethical considerations.\\nThe structure of this paper is as follows: Section 2 introduces RAG and its evolution, highlighting the limitations of\\ntraditional approaches. Section 3 elaborates on the principles of agentic intelligence and agentic patterns. Section 4\\nelaborates agentic workflow patterns. Section 5 provides a taxonomy of Agentic RAG systems, including single-agent,\\nmulti-agent, and graph-based frameworks. Section 6 provides comparative analysis of Agentic RAG frameworks.\\nSection 7 examines applications of Agentic RAG, while Section 8 discusses implementation tools and frameworks.\\nSection 9 focuses on benchmarks and dataset, and Section 10 concludes with future directions for Agentic RAG systems.\\n2\\nFoundations of Retrieval-Augmented Generation\\n2.1\\nOverview of Retrieval-Augmented Generation (RAG)\\nRetrieval-Augmented Generation (RAG) represents a significant advancement in the field of artificial intelligence,\\ncombining the generative capabilities of Large Language Models (LLMs) with real-time data retrieval. While LLMs\\nhave demonstrated remarkable capabilities in natural language processing, their reliance on static pre-trained data\\noften results in outdated or incomplete responses. RAG addresses this limitation by dynamically retrieving relevant\\ninformation from external sources and incorporating it into the generative process, enabling contextually accurate and\\nup-to-date outputs.\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 2}, page_content='Figure 1: An Overview of Agentic RAG\\n2.2\\nCore Components of RAG\\nThe architecture of RAG systems integrates three primary components (Figure2):\\n• Retrieveal: Responsible for querying external data sources such as knowledge bases, APIs, or vector databases.\\nAdvanced retrievers leverage dense vector search and transformer-based models to improve retrieval precision\\nand semantic relevance.\\n• Augmentation: Processes retrieved data, extracting and summarizing the most relevant information to align\\nwith the query context.\\n• Generation: Combines retrieved information with the LLM’s pre-trained knowledge to generate coherent,\\ncontextually appropriate responses.\\n2.3\\nEvolution of RAG Paradigms\\nThe field of Retrieval-Augmented Generation (RAG) has evolved significantly to address the increasing complexity of\\nreal-world applications, where contextual accuracy, scalability, and multi-step reasoning are critical. What began as\\nsimple keyword-based retrieval has transitioned into sophisticated, modular, and adaptive systems capable of integrating\\ndiverse data sources and autonomous decision-making processes. This evolution underscores the growing need for\\nRAG systems to handle complex queries efficiently and effectively.\\nThis section examines the progression of RAG paradigms, presenting key stages of development—Naïve RAG,\\nAdvanced RAG, Modular RAG, Graph RAG, and Agentic RAG alongside their defining characteristics, strengths, and\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 3}, page_content='Figure 2: Core Components of RAG\\nlimitations. By understanding the evolution of these paradigms, readers can appreciate the advancements made in\\nretrieval and generative capabilities and their application in various domains\\n2.3.1\\nNaïve RAG\\nNaïve RAG [20] represents the foundational implementation of retrieval-augmented generation. Figure 3 illustrates the\\nsimple retrieve-read workflow of Naive RAG, focusing on keyword-based retrieval and static datasets.. These systems\\nrely on simple keyword-based retrieval techniques, such as TF-IDF and BM25, to fetch documents from static datasets.\\nThe retrieved documents are then used to augment the language model’s generative capabilities.\\nFigure 3: An Overview of Naive RAG.\\nNaïve RAG is characterized by its simplicity and ease of implementation, making it suitable for tasks involving\\nfact-based queries with minimal contextual complexity. However, it suffers from several limitations:\\n• Lack of Contextual Awareness: Retrieved documents often fail to capture the semantic nuances of the query\\ndue to reliance on lexical matching rather than semantic understanding.\\n• Fragmented Outputs: The absence of advanced preprocessing or contextual integration often leads to\\ndisjointed or overly generic responses.\\n• Scalability Issues: Keyword-based retrieval techniques struggle with large datasets, often failing to identify\\nthe most relevant information.\\nDespite these limitations, Naïve RAG systems provided a critical proof-of-concept for integrating retrieval with\\ngeneration, laying the foundation for more sophisticated paradigms.\\n2.3.2\\nAdvanced RAG\\nAdvanced RAG [20] systems build upon the limitations of Naïve RAG by incorporating semantic understanding and\\nenhanced retrieval techniques. Figure 4 highlights the semantic enhancements in retrieval and the iterative, context-\\naware pipeline of Advanced RAG. These systems leverage dense retrieval models, such as Dense Passage Retrieval\\n(DPR), and neural ranking algorithms to improve retrieval precision.\\nKey features of Advanced RAG include:\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 4}, page_content='Figure 4: Overview of Advanced RAG\\n• Dense Vector Search: Queries and documents are represented in high-dimensional vector spaces, enabling\\nbetter semantic alignment between the user query and retrieved documents.\\n• Contextual Re-Ranking: Neural models re-rank retrieved documents to prioritize the most contextually\\nrelevant information.\\n• Iterative Retrieval: Advanced RAG introduces multi-hop retrieval mechanisms, enabling reasoning across\\nmultiple documents for complex queries.\\nThese advancements make Advanced RAG suitable for applications requiring high precision and nuanced understanding,\\nsuch as research synthesis and personalized recommendations. However, challenges such as computational overhead\\nand limited scalability persist, particularly when dealing with large datasets or multi-step queries.\\n2.3.3\\nModular RAG\\nModular RAG [20] represents the latest evolution in RAG paradigms, emphasizing flexibility and customization.\\nThese systems decompose the retrieval and generation pipeline into independent, reusable components, enabling\\ndomain-specific optimization and task adaptability. Figure 5 demonstrates the modular architecture, showcasing hybrid\\nretrieval strategies, composable pipelines, and external tool integration.\\nKey innovations in Modular RAG include:\\n• Hybrid Retrieval Strategies: Combining sparse retrieval methods (e.g., a sparse encoder-BM25) with dense\\nretrieval techniques [21] (e.g., DPR - Dense Passage Retrieval ) to maximize accuracy across diverse query\\ntypes.\\n• Tool Integration: Incorporating external APIs, databases, or computational tools to handle specialized tasks,\\nsuch as real-time data analysis or domain-specific computations.\\n• Composable Pipelines: Modular RAG enables retrievers, generators, and other components to be replaced,\\nenhanced, or reconfigured independently, allowing high adaptability to specific use cases.\\nFor instance, a Modular RAG system designed for financial analytics might retrieve live stock prices via APIs, analyze\\nhistorical trends using dense retrieval, and generate actionable investment insights through a tailored language model.\\nThis modularity and customization make Modular RAG ideal for complex, multi-domain tasks, offering both scalability\\nand precision.\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 5}, page_content='Figure 5: Overview of Modular RAG\\n2.3.4\\nGraph RAG\\nGraph RAG [16] extends traditional Retrieval-Augmented Generation systems by integrating graph-based data structures\\nas illustrated in Figure 6. These systems leverage the relationships and hierarchies within graph data to enhance multi-\\nhop reasoning and contextual enrichment. By incorporating graph-based retrieval, Graph RAG enables richer and more\\naccurate generative outputs, particularly for tasks requiring relational understanding.\\nGraph RAG is characterized by its ability to:\\n• Node Connectivity: Captures and reasons over relationships between entities.\\n• Hierarchical Knowledge Management: Handles structured and unstructured data through graph-based\\nhierarchies.\\n• Context Enrichment: Adds relational understanding by leveraging graph-based pathways.\\nHowever, Graph RAG has some limitations:\\n• Limited Scalability: The reliance on graph structures can restrict scalability, especially with extensive data\\nsources.\\n• Data Dependency: High-quality graph data is essential for meaningful outputs, limiting its applicability in\\nunstructured or poorly annotated datasets.\\n• Complexity of Integration: Integrating graph data with unstructured retrieval systems increases design and\\nimplementation complexity.\\nGraph RAG is well-suited for applications such as healthcare diagnostics, legal research, and other domains where\\nreasoning over structured relationships is crucial.\\n2.3.5\\nAgentic RAG\\nAgentic RAG represents a paradigm shift by introducing autonomous agents capable of dynamic decision-making\\nand workflow optimization. Unlike static systems, Agentic RAG employs iterative refinement and adaptive retrieval\\nstrategies to address complex, real-time, and multi-domain queries. This paradigm leverages the modularity of retrieval\\nand generation processes while introducing agent-based autonomy.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 6}, page_content='Figure 6: Overview of Graph RAG\\nKey characteristics of Agentic RAG include:\\n• Autonomous Decision-Making: Agents independently evaluate and manage retrieval strategies based on\\nquery complexity.\\n• Iterative Refinement: Incorporates feedback loops to improve retrieval accuracy and response relevance.\\n• Workflow Optimization: Dynamically orchestrates tasks, enabling efficiency in real-time applications.\\nDespite its advancements, Agentic RAG faces some challenges:\\n• Coordination Complexity: Managing interactions between agents requires sophisticated orchestration\\nmechanisms.\\n• Computational Overhead: The use of multiple agents increases resource requirements for complex work-\\nflows.\\n• Scalability Limitations: While scalable, the dynamic nature of the system can strain computational resources\\nfor high query volumes.\\nAgentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\\ndynamic adaptability and contextual precision are paramount.\\n2.4\\nChallenges and Limitations of Traditional RAG Systems\\nTraditional Retrieval-Augmented Generation (RAG) systems have significantly expanded the capabilities of Large\\nLanguage Models (LLMs) by integrating real-time data retrieval. However, these systems still face critical challenges\\nthat hinder their effectiveness in complex, real-world applications. The most notable limitations revolve around\\ncontextual integration, multi-step reasoning, and scalability and latency issues.\\n2.4.1\\nContextual Integration\\nEven when RAG systems successfully retrieve relevant information, they often struggle to seamlessly incorporate it\\ninto generated responses. The static nature of retrieval pipelines and limited contextual awareness lead to fragmented,\\ninconsistent, or overly generic outputs.\\nExample: A query such as, \"What are the latest advancements in Alzheimer’s research and their implications for\\nearly-stage treatment?\" might yield relevant research papers and medical guidelines. However, traditional RAG\\nsystems often fail to synthesize these findings into a coherent explanation that connects the new treatments to specific\\npatient scenarios. Similarly, for a query like, \"What are the best sustainable practices for small-scale agriculture in\\narid regions?\", traditional systems might retrieve documents on general agricultural methods but overlook critical\\nsustainability practices tailored to arid environments.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 7}, page_content='Table 1: Comparative Analysis of RAG Paradigms\\nParadigm\\nKey Features\\nStrengths\\nNaïve RAG\\n• Keyword-based retrieval (e.g.,\\nTF-IDF, BM25)\\n• Simple and easy to implement\\n• Suitable for fact-based queries\\nAdvanced RAG\\n• Dense retrieval models (e.g.,\\nDPR)\\n• Neural ranking and re-ranking\\n• Multi-hop retrieval\\n• High precision retrieval\\n• Improved contextual relevance\\nModular RAG\\n• Hybrid retrieval (sparse and\\ndense)\\n• Tool and API integration\\n• Composable, domain-specific\\npipelines\\n• High flexibility and customization\\n• Suitable for diverse applications\\n• Scalable\\nGraph RAG\\n• Integration\\nof\\ngraph-based\\nstructures\\n• Multi-hop reasoning\\n• Contextual\\nenrichment\\nvia\\nnodes\\n• Relational reasoning capabilities\\n• Mitigates hallucinations\\n• Ideal for structured data tasks\\nAgentic RAG\\n• Autonomous agents\\n• Dynamic decision-making\\n• Iterative refinement and work-\\nflow optimization\\n• Adaptable to real-time changes\\n• Scalable for multi-domain tasks\\n• High accuracy\\n2.4.2\\nMulti-Step Reasoning\\nMany real-world queries require iterative or multi-hop reasoning—retrieving and synthesizing information across\\nmultiple steps. Traditional RAG systems are often ill-equipped to refine retrieval based on intermediate insights or user\\nfeedback, resulting in incomplete or disjointed responses.\\nExample: A complex query like, \"What lessons from renewable energy policies in Europe can be applied to developing\\nnations, and what are the potential economic impacts?\" demands the orchestration of multiple types of information,\\nincluding policy data, contextualization for developing regions, and economic analysis. Traditional RAG systems\\ntypically fail to connect these disparate elements into a cohesive response.\\n2.4.3\\nScalability and Latency Issues\\nAs the volume of external data sources grows, querying and ranking large datasets becomes increasingly computationally\\nintensive. This results in significant latency, which undermines the system’s ability to provide timely responses in\\nreal-time applications.\\nExample: In time-sensitive settings such as financial analytics or live customer support, delays caused by querying\\nmultiple databases or processing large document sets can hinder the system’s overall utility. For example, a delay in\\nretrieving market trends during high-frequency trading could result in missed opportunities.\\n2.5\\nAgentic RAG: A Paradigm Shift\\nTraditional RAG systems, with their static workflows and limited adaptability, often struggle to handle dynamic, multi-\\nstep reasoning and complex real-world tasks. These limitations have spurred the integration of agentic intelligence,\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 8}, page_content='resulting in Agentic RAG. By incorporating autonomous agents capable of dynamic decision-making, iterative reasoning,\\nand adaptive retrieval strategies, Agentic RAG builds on the modularity of earlier paradigms while overcoming their\\ninherent constraints. This evolution enables more complex, multi-domain tasks to be addressed with enhanced precision\\nand contextual understanding, positioning Agentic RAG as a cornerstone for next-generation AI applications. In\\nparticular, Agentic RAG systems reduce latency through optimized workflows and refine outputs iteratively, tackling\\nthe very challenges that have historically hindered traditional RAG’s scalability and effectiveness.\\n3\\nCore Principles and Background of Agentic Intelligence\\nAgentic Intelligence forms the foundation of Agentic Retrieval-Augmented Generation (RAG) systems, enabling them\\nto transcend the static and reactive nature of traditional RAG. By integrating autonomous agents capable of dynamic\\ndecision-making, iterative reasoning, and collaborative workflows, Agentic RAG systems exhibit enhanced adaptability\\nand precision. This section explores the core principles underpinning agentic intelligence.\\nComponents of an AI Agent.\\nIn essence, an AI agent comprises (Figure. 7):\\n• LLM (with defined Role and Task): Serves as the agent’s primary reasoning engine and dialogue interface.\\nIt interprets user queries, generates responses, and maintains coherence.\\n• Memory (Short-Term and Long-Term): Captures context and relevant data across interactions. Short-term\\nmemory [22] tracks immediate conversation state, while long-term memory [22]stores accumulated knowledge\\nand agent experiences.\\n• Planning (Reflection & Self-Critique): Guides the agent’s iterative reasoning process through reflection,\\nquery routing, or self-critique[23], ensuring that complex tasks are broken down effectively [24].\\n• Tools Vector Search, Web Search, APIs, etc.): Expands the agent’s capabilities beyond text generation,\\nenabling access to external resources, real-time data, or specialized computations.\\nFigure 7: An Overview of AI Agents\\nAgentic Patterns [25, 26] provide structured methodologies that guide the behavior of agents in Agentic Retrieval-\\nAugmented Generation (RAG) systems. These patterns enable agents to dynamically adapt, plan, and collaborate,\\nensuring that the system can handle complex, real-world tasks with precision and scalability. Four key patterns underpin\\nagentic workflows:\\n3.1\\nReflection\\nReflection is a foundational design pattern in agentic workflows, enabling agents to iteratively evaluate and refine their\\noutputs. By incorporating self-feedback mechanisms, agents can identify and address errors, inconsistencies, and areas\\nfor improvement, enhancing performance across tasks like code generation, text production, and question answering (\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 9}, page_content='as shown in Figure 8). In practical use, Reflection involves prompting an agent to critique its outputs for correctness,\\nstyle, and efficiency, then incorporating this feedback into subsequent iterations. External tools, such as unit tests or\\nweb searches, can further enhance this process by validating results and highlighting gaps.\\nIn multi-agent systems, Reflection can involve distinct roles, such as one agent generating outputs while another\\ncritiques them, fostering collaborative improvement. For instance, in legal research, agents can iteratively refine\\nresponses by re-evaluating retrieved case law, ensuring accuracy and comprehensiveness. Reflection has demonstrated\\nsignificant performance improvements in studies like Self-Refine [27], Reflexion [28], and CRITIC [23].\\nFigure 8: An Overview of Agentic Self- Reflection\\n3.2\\nPlanning\\nPlanning [24] is a key design pattern in agentic workflows that enables agents to autonomously decompose complex tasks\\ninto smaller, manageable subtasks. This capability is essential for multi-hop reasoning and iterative problem-solving in\\ndynamic and uncertain scenarios as shown in Figure 9a.\\nBy leveraging planning, agents can dynamically determine the sequence of steps needed to accomplish a larger objective.\\nThis adaptability allows agents to handle tasks that cannot be predefined, ensuring flexibility in decision-making.\\nWhile powerful, Planning can produce less predictable outcomes compared to deterministic workflows like Reflection.\\nPlanning is particularly suited for tasks that require dynamic adaptation, where predefined workflows are insufficient.\\nAs the technology matures, its potential to drive innovative applications across domains will continue to grow.\\n3.3\\nTool Use\\nTool Use enables agents to extend their capabilities by interacting with external tools, APIs, or computational resources\\nas illustrated in 9b. This pattern allows agents to gather information, perform computations, and manipulate data beyond\\ntheir pre-trained knowledge. By dynamically integrating tools into workflows, agents can adapt to complex tasks and\\nprovide more accurate and contextually relevant outputs.\\nModern agentic workflows incorporate tool use for a variety of applications, including information retrieval, computa-\\ntional reasoning, and interfacing with external systems. The implementation of this pattern has evolved significantly\\nwith advancements like GPT-4’s function calling capabilities and systems capable of managing access to numerous\\ntools. These developments facilitate sophisticated workflows where agents autonomously select and execute the most\\nrelevant tools for a given task.\\nWhile tool use significantly enhances agentic workflows, challenges remain in optimizing the selection of tools,\\nparticularly in contexts with a large number of available options. Techniques inspired by retrieval-augmented generation\\n(RAG), such as heuristic-based selection, have been proposed to address this issue.\\n3.4\\nMulti-Agent\\nMulti-agent collaboration [29] is a key design pattern in agentic workflows that enables task specialization and parallel\\nprocessing. Agents communicate and share intermediate results, ensuring the overall workflow remains efficient and\\ncoherent. By distributing subtasks among specialized agents, this pattern improves the scalability and adaptability\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 10}, page_content='(a) An Overview of Agentic Planning\\n(b) An Overview of Tool Use\\nFigure 9: Overview of Agentic Planning and Tool Use\\nof complex workflows. Multi-agent systems allow developers to decompose intricate tasks into smaller, manageable\\nsubtasks assigned to different agents. This approach not only enhances task performance but also provides a robust\\nframework for managing complex interactions. Each agent operates with its own memory and workflow, which can\\ninclude the use of tools, reflection, or planning, enabling dynamic and collaborative problem-solving (see Figure 10).\\nWhile multi-agent collaboration offers significant potential, it is a less predictable design pattern compared to more\\nmature workflows like Reflection and Tool Use. Nevertheless, emerging frameworks such as AutoGen, Crew AI, and\\nLangGraph are providing new avenues for implementing effective multi-agent solutions.\\nFigure 10: An Overview of MultiAgent\\nThese design patterns form the foundation for the success of Agentic RAG systems. By structuring workflows—from\\nsimple, sequential steps to more adaptive, collaborative processes—these patterns enable systems to dynamically\\nadapt their retrieval and generative strategies to the diverse and ever-changing demands of real-world environments.\\nLeveraging these patterns, agents are capable of handling iterative, context-aware tasks that significantly exceed the\\ncapabilities of traditional RAG systems.\\n4\\nAgentic Workflow Patterns: Adaptive Strategies for Dynamic Collaboration\\nAgentic workflow patterns, [12, 13] structure LLM-based applications to optimize performance, accuracy, and efficiency.\\nDifferent approaches are suitable depending on task complexity and processing requirements.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 11}, page_content='4.1\\nPrompt Chaining: Enhancing Accuracy Through Sequential Processing\\nPrompt chaining [12, 13] decomposes a complex task into multiple steps, where each step builds upon the previous\\none. This structured approach improves accuracy by simplifying each subtask before moving forward. However, it may\\nincrease latency due to sequential processing.\\nFigure 11: Illustration of Prompt Chaining Workflow\\nWhen to Use: This workflow is most effective when a task can be broken down into fixed subtasks, each contributing\\nto the final output. It is particularly useful in scenarios where step-by-step reasoning enhances accuracy.\\nExample Applications:\\n• Generating marketing content in one language and then translating it into another while preserving nuances.\\n• Structuring document creation by first generating an outline, verifying its completeness, and then developing\\nthe full text.\\n4.2\\nRouting:Directing Inputs to Specialized Processes\\nRouting [12, 13] involves classifying an input and directing it to an appropriate specialized prompt or process. This\\nmethod ensures distinct queries or tasks are handled separately, improving efficiency and response quality.\\nFigure 12: Illustration Routing Workflow\\nWhen to Use: Ideal for scenarios where different types of input require distinct handling strategies, ensuring optimized\\nperformance for each category.\\nExample Applications:\\n• Directing customer service queries into categories such as technical support, refund requests, or general\\ninquiries.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 12}, page_content='• Assigning simple queries to smaller models for cost efficiency, while complex requests go to advanced models.\\n4.3\\nParallelization: Speeding Up Processing Through Concurrent Execution\\nParallelization [12, 13] divides a task into independent processes that run simultaneously, reducing latency and\\nimproving throughput. It can be categorized into sectioning (independent subtasks) and voting (multiple outputs for\\naccuracy).\\nFigure 13: Illustration of Parallelization Workflow\\nWhen to Use: Useful when tasks can be executed independently to enhance speed or when multiple outputs improve\\nconfidence.\\nExample Applications:\\n• Sectioning: Splitting tasks like content moderation, where one model screens input while another generates a\\nresponse.\\n• Voting: Using multiple models to cross-check code for vulnerabilities or analyze content moderation decisions.\\n4.4\\nOrchestrator-Workers: Dynamic Task Delegation\\nThis workflow [12, 13] features a central orchestrator model that dynamically breaks tasks into subtasks, assigns them\\nto specialized worker models, and compiles the results. Unlike parallelization, it adapts to varying input complexity.\\nFigure 14: Illustration of Orchestrator-Workers Workflow\\nWhen to Use: Best suited for tasks requiring dynamic decomposition and real-time adaptation, where subtasks are not\\npredefined.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 13}, page_content='Example Applications:\\n• Automatically modifying multiple files in a codebase based on the nature of requested changes.\\n• Conducting real-time research by gathering and synthesizing relevant information from multiple sources.\\n4.5\\nEvaluator-Optimizer: Refining Output Through Iteration\\nThe evaluator-optimizer [12, 13] workflow iteratively improves content by generating an initial output and refining it\\nbased on feedback from an evaluation model.\\nFigure 15: Illustration of Evaluator-Optimizer Workflow\\nWhen to Use: Effective when iterative refinement significantly enhances response quality, especially when clear\\nevaluation criteria exist.\\nExample Applications:\\n• Improving literary translations through multiple evaluation and refinement cycles.\\n• Conducting multi-round research queries where additional iterations refine search results.\\n5\\nTaxonomy of Agentic RAG Systems\\nAgentic Retrieval-Augmented Generation (RAG) systems can be categorized into distinct architectural frameworks\\nbased on their complexity and design principles. These include single-agent architectures, multi-agent systems, and hi-\\nerarchical agentic architectures. Each framework is tailored to address specific challenges and optimize performance for\\ndiverse applications. This section provides a detailed taxonomy of these architectures, highlighting their characteristics,\\nstrengths, and limitations.\\n5.1\\nSingle-Agent Agentic RAG: Router\\nA Single-Agent Agentic RAG: [30] serves as a centralized decision-making system where a single agent manages the\\nretrieval, routing, and integration of information (as shown in Figure. 16). This architecture simplifies the system by\\nconsolidating these tasks into one unified agent, making it particularly effective for setups with a limited number of\\ntools or data sources.\\nWorkflow\\n1. Query Submission and Evaluation: The process begins when a user submits a query. A coordinating\\nagent (or master retrieval agent) receives the query and analyzes it to determine the most suitable sources of\\ninformation.\\n2. Knowledge Source Selection: Based on the query’s type, the coordinating agent chooses from a variety of\\nretrieval options:\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 14}, page_content='• Structured Databases: For queries requiring tabular data access, the system may use a Text-to-SQL\\nengine that interacts with databases like PostgreSQL or MySQL.\\n• Semantic Search: When dealing with unstructured information, it retrieves relevant documents (e.g.,\\nPDFs, books, organizational records) using vector-based retrieval.\\n• Web Search: For real-time or broad contextual information, the system leverages a web search tool to\\naccess the latest online data.\\n• Recommendation Systems: For personalized or contextual queries, the system taps into recommendation\\nengines that provide tailored suggestions.\\n3. Data Integration and LLM Synthesis: Once the relevant data is retrieved from the chosen sources, it is\\npassed to a Large Language Model (LLM). The LLM synthesizes the gathered information, integrating insights\\nfrom multiple sources into a coherent and contextually relevant response.\\n4. Output Generation: Finally, the system delivers a comprehensive, user-facing answer that addresses the\\noriginal query. This response is presented in an actionable, concise format and may optionally include\\nreferences or citations to the sources used.\\nFigure 16: An Overview of Single Agentic RAG\\nKey Features and Advantages.\\n• Centralized Simplicity: A single agent handles all retrieval and routing tasks, making the architecture\\nstraightforward to design, implement, and maintain.\\n• Efficiency & Resource Optimization: With fewer agents and simpler coordination, the system demands\\nfewer computational resources and can handle queries more quickly.\\n• Dynamic Routing: The agent evaluates each query in real-time, selecting the most appropriate knowledge\\nsource (e.g., structured DB, semantic search, web search).\\n• Versatility Across Tools: Supports a variety of data sources and external APIs, enabling both structured and\\nunstructured workflows.\\n• Ideal for Simpler Systems: Suited for applications with well-defined tasks or limited integration requirements\\n(e.g., document retrieval, SQL-based workflows).\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 15}, page_content='Use Case: Customer Support\\nPrompt: Can you tell me the delivery status of my order?\\nSystem Process (Single-Agent Workflow):\\n1. Query Submission and Evaluation:\\n• The user submits the query, which is received by the coordinating agent.\\n• The coordinating agent analyzes the query and determines the most appropriate sources of\\ninformation.\\n2. Knowledge Source Selection:\\n• Retrieves tracking details from the order management database.\\n• Fetches real-time updates from the shipping provider’s API.\\n• Optionally conducts a web search to identify local conditions affecting delivery, such as weather\\nor logistical delays.\\n3. Data Integration and LLM Synthesis:\\n• The relevant data is passed to the LLM, which synthesizes the information into a coherent\\nresponse.\\n4. Output Generation:\\n• The system generates an actionable and concise response, providing live tracking updates and\\npotential alternatives.\\nResponse:\\nIntegrated Response: “Your package is currently in transit and expected to arrive tomorrow evening. The live\\ntracking from UPS indicates it is at the regional distribution center.”\\n5.2\\nMulti-Agent Agentic RAG Systems:\\nMulti-Agent RAG [30] represents a modular and scalable evolution of single-agent architectures, designed to handle\\ncomplex workflows and diverse query types by leveraging multiple specialized agents (as shown in Figure 17). Instead\\nof relying on a single agent to manage all tasks—reasoning, retrieval, and response generation—this system distributes\\nresponsibilities across multiple agents, each optimized for a specific role or data source.\\nWorkflow\\n1. Query Submission: The process begins with a user query, which is received by a coordinator agent or master\\nretrieval agent. This agent acts as the central orchestrator, delegating the query to specialized retrieval agents\\nbased on the query’s requirements.\\n2. Specialized Retrieval Agents: The query is distributed among multiple retrieval agents, each focusing on a\\nspecific type of data source or task. Examples include:\\n• Agent 1: Handles structured queries, such as interacting with SQL-based databases like PostgreSQL or\\nMySQL.\\n• Agent 2: Manages semantic searches for retrieving unstructured data from sources like PDFs, books, or\\ninternal records.\\n• Agent 3: Focuses on retrieving real-time public information from web searches or APIs.\\n• Agent 4: Specializes in recommendation systems, delivering context-aware suggestions based on user\\nbehavior or profiles.\\n3. Tool Access and Data Retrieval: Each agent routes the query to the appropriate tools or data sources within\\nits domain, such as:\\n• Vector Search: For semantic relevance.\\n• Text-to-SQL: For structured data.\\n• Web Search: For real-time public information.\\n• APIs: For accessing external services or proprietary systems.\\nThe retrieval process is executed in parallel, allowing for efficient processing of diverse query types.\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 16}, page_content='Figure 17: An Overview of Multi-Agent Agentic RAG Systems\\n4. Data Integration and LLM Synthesis: Once retrieval is complete, the data from all agents is passed to a\\nLarge Language Model (LLM). The LLM synthesizes the retrieved information into a coherent and contextually\\nrelevant response, integrating insights from multiple sources seamlessly.\\n5. Output Generation: The system generates a comprehensive response, which is delivered back to the user in\\nan actionable and concise format.\\nKey Features and Advantages.\\n• Modularity: Each agent operates independently, allowing for seamless addition or removal of agents based on\\nsystem requirements.\\n• Scalability: Parallel processing by multiple agents enables the system to handle high query volumes efficiently.\\n• Task Specialization: Each agent is optimized for a specific type of query or data source, improving accuracy\\nand retrieval relevance.\\n• Efficiency: By distributing tasks across specialized agents, the system minimizes bottlenecks and enhances\\nperformance for complex workflows.\\n• Versatility: Suitable for applications spanning multiple domains, including research, analytics, decision-\\nmaking, and customer support.\\nChallenges\\n• Coordination Complexity: Managing inter-agent communication and task delegation requires sophisticated\\norchestration mechanisms.\\n• Computational Overhead: Parallel processing of multiple agents can increase resource usage.\\n• Data Integration: Synthesizing outputs from diverse sources into a cohesive response is non-trivial and\\nrequires advanced LLM capabilities.\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 17}, page_content='Use Case: Multi-Domain Research Assistant\\nPrompt: What are the economic and environmental impacts of renewable energy adoption in Europe?\\nSystem Process (Multi-Agent Workflow):\\n• Agent 1: Retrieves statistical data from economic databases using SQL-based queries.\\n• Agent 2: Searches for relevant academic papers using semantic search tools.\\n• Agent 3: Performs a web search for recent news and policy updates on renewable energy.\\n• Agent 4: Consults a recommendation system to suggest related content, such as reports or expert\\ncommentary.\\nResponse:\\nIntegrated Response: “Adopting renewable energy in Europe has led to a 20% reduction in greenhouse gas\\nemissions over the past decade, according to EU policy reports. Economically, renewable energy investments\\nhave generated approximately 1.2 million jobs, with significant growth in solar and wind sectors. Recent\\nacademic studies also highlight potential trade-offs in grid stability and energy storage costs.”\\n5.3\\nHierarchical Agentic RAG Systems\\nHierarchical Agentic RAG: [14] systems employ a structured, multi-tiered approach to information retrieval and\\nprocessing, enhancing both efficiency and strategic decision-making as shown in Figure 18. Agents are organized in\\na hierarchy, with higher-level agents overseeing and directing lower-level agents. This structure enables multi-level\\ndecision-making, ensuring that queries are handled by the most appropriate resources.\\nFigure 18: An illustration of Hierarchical Agentic RAG\\nWorkflow\\n1. Query Reception: A user submits a query, received by a top-tier agent responsible for initial assessment and\\ndelegation.\\n2. Strategic Decision-Making: The top-tier agent evaluates the query’s complexity and decides which subor-\\ndinate agents or data sources to prioritize. Certain databases, APIs, or retrieval tools may be deemed more\\nreliable or relevant based on the query’s domain.\\n3. Delegation to Subordinate Agents: The top-tier agent assigns tasks to lower-level agents specialized in\\nparticular retrieval methods (e.g., SQL databases, web search, or proprietary systems). These agents execute\\ntheir assigned tasks independently.\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 18}, page_content='4. Aggregation and Synthesis: The results from subordinate agents are collected and integrated by the higher-\\nlevel agent, which synthesizes the information into a coherent response.\\n5. Response Delivery: The final, synthesized answer is returned to the user, ensuring that the response is both\\ncomprehensive and contextually relevant.\\nKey Features and Advantages.\\n• Strategic Prioritization: Top-tier agents can prioritize data sources or tasks based on query complexity,\\nreliability, or context.\\n• Scalability: Distributing tasks across multiple agent tiers enables handling of highly complex or multi-faceted\\nqueries.\\n• Enhanced Decision-Making: Higher-level agents apply strategic oversight to improve overall accuracy and\\ncoherence of responses.\\nChallenges\\n• Coordination Complexity: Maintaining robust inter-agent communication across multiple levels can increase\\norchestration overhead.\\n• Resource Allocation: Efficiently distributing tasks among tiers to avoid bottlenecks is non-trivial.\\nUse Case: Financial Analysis System\\nPrompt: What are the best investment options given the current market trends in renewable energy?\\nSystem Process (Hierarchical Agentic Workflow):\\n1. Top-Tier Agent: Assesses the query’s complexity and prioritizes reliable financial databases and\\neconomic indicators over less validated data sources.\\n2. Mid-Level Agent: Retrieves real-time market data (e.g., stock prices, sector performance) from\\nproprietary APIs and structured SQL databases.\\n3. Lower-Level Agent(s): Conducts web searches for recent policy announcements and consults recom-\\nmendation systems that track expert opinions and news analytics.\\n4. Aggregation and Synthesis: The top-tier agent compiles the results, integrating quantitative data with\\npolicy insights.\\nResponse:\\nIntegrated Response: “Based on current market data, renewable energy stocks have shown a 15% growth over\\nthe past quarter, driven by supportive government policies and heightened investor interest. Analysts suggest\\nthat wind and solar sectors, in particular, may experience continued momentum, while emerging technologies\\nlike green hydrogen present moderate risk but potentially high returns.”\\n5.4\\nAgentic Corrective RAG\\nCorrective RAG : introduces mechanisms to self-correct retrieval results, enhancing document utilization and improving\\nresponse generation quality as demonstrated in Figure 19. By embedding intelligent agents into the workflow, Corrective\\nRAG [31] [32] ensures iterative refinement of context documents and responses, minimizing errors and maximizing\\nrelevance.\\nKey Idea of Corrective RAG:\\nThe core principle of Corrective RAG lies in its ability to evaluate retrieved documents\\ndynamically, perform corrective actions, and refine queries to enhance the quality of generated responses. Corrective\\nRAG adjusts its approach as follows:\\n• Document Relevance Evaluation: Retrieved documents are assessed for relevance by the Relevance Evalua-\\ntion Agent. Documents below the relevance threshold trigger corrective steps.\\n• Query Refinement and Augmentation: Queries are refined by the Query Refinement Agent, which leverages\\nsemantic understanding to optimize retrieval for better results.\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 19}, page_content='Figure 19: Overview of Agentic Corrective RAG\\n• Dynamic Retrieval from External Sources: When context is insufficient, the External Knowledge Retrieval\\nAgent performs web searches or accesses alternative data sources to supplement the retrieved documents.\\n• Response Synthesis: All validated and refined information is passed to the Response Synthesis Agent for final\\nresponse generation.\\nWorkflow:\\nThe Corrective RAG system is built on five key agents:\\n1. Context Retrieval Agent: Responsible for retrieving initial context documents from a vector database.\\n2. Relevance Evaluation Agent: Assesses the retrieved documents for relevance and flags any irrelevant or\\nambiguous documents for corrective actions.\\n3. Query Refinement Agent: Rewrites queries to improve retrieval, leveraging semantic understanding to\\noptimize results.\\n4. External Knowledge Retrieval Agent: Performs web searches or accesses alternative data sources when the\\ncontext documents are insufficient.\\n5. Response Synthesis Agent: Synthesizes all validated information into a coherent and accurate response.\\nKey Features and Advantages:\\n• Iterative Correction: Ensures high response accuracy by dynamically identifying and correcting irrelevant or\\nambiguous retrieval results.\\n• Dynamic Adaptability: Incorporates real-time web searches and query refinement for enhanced retrieval\\nprecision.\\n• Agentic Modularity: Each agent performs specialized tasks, ensuring efficient and scalable operation.\\n• Factuality Assurance: By validating all retrieved and generated content, Corrective RAG minimizes the risk\\nof hallucination or misinformation.\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 20}, page_content='Use Case: Academic Research Assistant\\nPrompt: What are the latest findings in generative AI research?\\nSystem Process (Corrective RAG Workflow):\\n1. Query Submission: A user submits the query to the system.\\n2. Context Retrieval:\\n• The Context Retrieval Agent retrieves initial documents from a database of published papers on\\ngenerative AI.\\n• The retrieved documents are passed to the next step for evaluation.\\n3. Relevance Evaluation:\\n• The Relevance Evaluation Agent assesses the documents for alignment with the query.\\n• Documents are classified into relevant, ambiguous, or irrelevant categories. Irrelevant documents\\nare flagged for corrective actions.\\n4. Corrective Actions (if needed):\\n• The Query Refinement Agent rewrites the query to improve specificity and relevance.\\n• The External Knowledge Retrieval Agent performs web searches to fetch additional papers and\\nreports from external sources.\\n5. Response Synthesis:\\n• The Response Synthesis Agent integrates validated documents into a coherent and comprehensive\\nsummary.\\nResponse:\\nIntegrated Response: “Recent findings in generative AI highlight advancements in diffusion models, reinforce-\\nment learning for text-to-video tasks, and optimization techniques for large-scale model training. For more\\ndetails, refer to studies published in NeurIPS 2024 and AAAI 2025.”\\n5.5\\nAdaptive Agentic RAG\\nAdaptive Retrieval-Augmented Generation (Adaptive RAG) [33] enhances the flexibility and efficiency of large\\nlanguage models (LLMs) by dynamically adjusting query handling strategies based on the complexity of the incoming\\nquery. Unlike static retrieval workflows, Adaptive RAG [34] employs a classifier to assess query complexity and\\ndetermine the most appropriate approach, ranging from single-step retrieval to multi-step reasoning, or even bypassing\\nretrieval altogether for straightforward queries as illustrated in Figure 20.\\nFigure 20: An Overview of Adaptive Agentic RAG\\nKey Idea of Adaptive RAG\\nThe core principle of Adaptive RAG lies in its ability to dynamically tailor retrieval\\nstrategies based on the complexity of the query. Adaptive RAG adjusts its approach as follows:\\n21'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 21}, page_content='• Straightforward Queries: For fact-based questions that require no additional retrieval (e.g., \"What is the\\nboiling point of water?\"), the system directly generates an answer using pre-existing knowledge.\\n• Simple Queries: For moderately complex tasks requiring minimal context (e.g., \"What is the status of my\\nlatest electricity bill?\"), the system performs a single-step retrieval to fetch the relevant details.\\n• Complex Queries: For multi-layered queries requiring iterative reasoning (e.g., \"How has the population of\\nCity X changed over the past decade, and what are the contributing factors?\"), the system employs multi-step\\nretrieval, progressively refining intermediate results to provide a comprehensive answer.\\nWorkflow:\\nThe Adaptive RAG system is built on three primary components:\\n1. Classifier Role:\\n• A smaller language model analyzes the query to predict its complexity.\\n• The classifier is trained using automatically labeled datasets, derived from past model outcomes and\\nquery patterns.\\n2. Dynamic Strategy Selection:\\n• For straightforward queries, the system avoids unnecessary retrieval, directly leveraging the LLM for\\nresponse generation.\\n• For simple queries, it employs a single-step retrieval process to fetch relevant context.\\n• For complex queries, it activates multi-step retrieval to ensure iterative refinement and enhanced reasoning.\\n3. LLM Integration:\\n• The LLM synthesizes retrieved information into a coherent response.\\n• Iterative interactions between the LLM and the classifier enable refinement for complex queries.\\nKey Features and Advantages\\n• Dynamic Adaptability: Adjusts retrieval strategies based on query complexity, optimizing both computational\\nefficiency and response accuracy.\\n• Resource Efficiency: Minimizes unnecessary overhead for simple queries while ensuring thorough processing\\nfor complex ones.\\n• Enhanced Accuracy: Iterative refinement ensures that complex queries are resolved with high precision.\\n• Flexibility: Can be extended to incorporate additional pathways, such as domain-specific tools or external\\nAPIs.\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 22}, page_content='Use Case: Customer Support Assistant\\nPrompt: Why is my package delayed, and what alternatives do I have?\\nSystem Process (Adaptive RAG Workflow):\\n1. Query Classification:\\n• The classifier analyzes the query and determines it to be complex, requiring multi-step reasoning.\\n2. Dynamic Strategy Selection:\\n• The system activates a multi-step retrieval process based on the complexity classification.\\n3. Multi-Step Retrieval:\\n• Retrieves tracking details from the order database.\\n• Fetches real-time status updates from the shipping provider API.\\n• Conducts a web search for external factors such as weather conditions or local disruptions.\\n4. Response Synthesis:\\n• The LLM integrates all retrieved information, synthesizing a comprehensive and actionable\\nresponse.\\nResponse:\\nIntegrated Response: “Your package is delayed due to severe weather conditions in your region. It is currently\\nat the local distribution center and will be delivered in 2 days. Alternatively, you may opt for a local pickup\\nfrom the facility.”\\n5.6\\nGraph-Based Agentic RAG\\n5.6.1\\nAgent-G: Agentic Framework for Graph RAG\\nAgent-G [8]: introduces a novel agentic architecture that integrates graph knowledge bases with unstructured document\\nretrieval. By combining structured and unstructured data sources, this framework enhances retrieval-augmented\\ngeneration (RAG) systems with improved reasoning and retrieval accuracy. It employs modular retriever banks,\\ndynamic agent interaction, and feedback loops to ensure high-quality outputs as shown in Figure 21.\\nFigure 21: An Overview of Agent-G: Agentic Framework for Graph RAG [8]\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 23}, page_content='Key Idea of Agent-G\\nThe core principle of Agent-G lies in its ability to dynamically assign retrieval tasks to\\nspecialized agents, leveraging both graph knowledge bases and textual documents. Agent-G adjusts its retrieval strategy\\nas follows:\\n• Graph Knowledge Bases: Structured data is used to extract relationships, hierarchies, and connections (e.g.,\\ndisease-to-symptom mappings in healthcare).\\n• Unstructured Documents: Traditional text retrieval systems provide contextual information to complement\\ngraph data.\\n• Critic Module: Evaluates the relevance and quality of retrieved information, ensuring alignment with the\\nquery.\\n• Feedback Loops: Refines retrieval and synthesis through iterative validation and re-querying.\\nWorkflow:\\nThe Agent-G system is built on four primary components:\\n1. Retriever Bank:\\n• A modular set of agents specializes in retrieving graph-based or unstructured data.\\n• Agents dynamically select relevant sources based on the query’s requirements.\\n2. Critic Module:\\n• Validates retrieved data for relevance and quality.\\n• Flags low-confidence results for re-retrieval or refinement.\\n3. Dynamic Agent Interaction:\\n• Task-specific agents collaborate to integrate diverse data types.\\n• Ensures cohesive retrieval and synthesis across graph and text sources.\\n4. LLM Integration:\\n• Synthesizes validated data into a coherent response.\\n• Iterative feedback from the critic ensures alignment with the query’s intent.\\nKey Features and Advantages\\n• Enhanced Reasoning: Combines structured relationships from graphs with contextual information from\\nunstructured documents.\\n• Dynamic Adaptability: Adjusts retrieval strategies dynamically based on query requirements.\\n• Improved Accuracy: Critic module reduces the risk of irrelevant or low-quality data in responses.\\n• Scalable Modularity: Supports the addition of new agents for specialized tasks, enhancing scalability.\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 24}, page_content='Use Case: Healthcare Diagnostics\\nPrompt: What are the common symptoms of Type 2 Diabetes, and how are they related to heart disease?\\nSystem Process (Agent-G Workflow):\\n1. Query Reception and Assignment: The system receives the query and identifies the need for both\\ngraph-structured and unstructured data to answer the question comprehensively.\\n2. Graph Retriever:\\n• Extracts relationships between Type 2 Diabetes and heart disease from a medical knowledge\\ngraph.\\n• Identifies shared risk factors such as obesity and high blood pressure by exploring graph hierar-\\nchies and relationships.\\n3. Document Retriever:\\n• Retrieves descriptions of Type 2 Diabetes symptoms (e.g., increased thirst, frequent urination,\\nfatigue) from medical literature.\\n• Adds contextual information to complement the graph-based insights.\\n4. Critic Module:\\n• Evaluates the relevance and quality of the retrieved graph data and document data.\\n• Flags low-confidence results for refinement or re-querying.\\n5. Response Synthesis: The LLM integrates validated data from the Graph Retriever and Document\\nRetriever into a coherent response, ensuring alignment with the query’s intent.\\nResponse:\\nIntegrated Response: “Type 2 Diabetes symptoms include increased thirst, frequent urination, and fatigue.\\nStudies show a 50% correlation between diabetes and heart disease, primarily through shared risk factors such\\nas obesity and high blood pressure.”\\n5.6.2\\nGeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation\\nGeAR [35]: introduces an agentic framework that enhances traditional Retrieval-Augmented Generation (RAG) systems\\nby incorporating graph-based retrieval mechanisms. By leveraging graph expansion techniques and an agent-based\\narchitecture, GeAR addresses challenges in multi-hop retrieval scenarios, improving the system’s ability to handle\\ncomplex queries as shown in Figure 22.\\nKey Idea of GeAR\\nGeAR advances RAG performance through two primary innovations:\\n• Graph Expansion: Enhances conventional base retrievers (e.g., BM25) by expanding the retrieval process to\\ninclude graph-structured data, enabling the system to capture complex relationships and dependencies between\\nentities.\\n• Agent Framework: Incorporates an agent-based architecture that utilizes graph expansion to manage retrieval\\ntasks more effectively, allowing for dynamic and autonomous decision-making in the retrieval process.\\nWorkflow:\\nThe GeAR system operates through the following components:\\n1. Graph Expansion Module:\\n• Integrates graph-based data into the retrieval process, allowing the system to consider relationships\\nbetween entities during retrieval.\\n• Enhances the base retriever’s ability to handle multi-hop queries by expanding the search space to include\\nconnected entities.\\n2. Agent-Based Retrieval:\\n• Employs an agent framework to manage the retrieval process, enabling dynamic selection and combination\\nof retrieval strategies based on the query’s complexity.\\n• Agents can autonomously decide to utilize graph-expanded retrieval paths to improve the relevance and\\naccuracy of retrieved information.\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 25}, page_content='3. LLM Integration:\\n• Combines the retrieved information, enriched by graph expansion, with the capabilities of a Large\\nLanguage Model (LLM) to generate coherent and contextually relevant responses.\\n• The integration ensures that the generative process is informed by both unstructured documents and\\nstructured graph data.\\nFigure 22: An Overview of GeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation[35]\\nKey Features and Advantages\\n• Enhanced Multi-Hop Retrieval: GeAR’s graph expansion allows the system to handle complex queries that\\nrequire reasoning over multiple interconnected pieces of information.\\n• Agentic Decision-Making: The agent framework enables dynamic and autonomous selection of retrieval\\nstrategies, improving efficiency and relevance.\\n• Improved Accuracy: By incorporating structured graph data, GeAR enhances the precision of retrieved\\ninformation, leading to more accurate and contextually appropriate responses.\\n• Scalability: The modular nature of the agent framework allows for the integration of additional retrieval\\nstrategies and data sources as needed.\\n26'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 26}, page_content='Use Case: Multi-Hop Question Answering\\nPrompt: Which author influenced the mentor of J.K. Rowling?\\nSystem Process (GeAR Workflow):\\n1. Top-Tier Agent: Evaluates the query’s multi-hop nature and determines that a combination of graph\\nexpansion and document retrieval is necessary to answer the question.\\n2. Graph Expansion Module:\\n• Identifies that J.K. Rowling’s mentor is a key entity in the query.\\n• Traces the literary influences on that mentor by exploring graph-structured data on literary\\nrelationships.\\n3. Agent-Based Retrieval:\\n• An agent autonomously selects the graph-expanded retrieval path to gather relevant information\\nabout the mentor’s influences.\\n• Integrates additional context by querying textual data sources for unstructured details about the\\nmentor and their influences.\\n4. Response Synthesis: Combines insights from the graph and document retrieval processes using the\\nLLM to generate a response that accurately reflects the complex relationships in the query.\\nResponse:\\nIntegrated Response: “J.K. Rowling’s mentor, [Mentor Name], was heavily influenced by [Author Name],\\nknown for their [notable works or genre]. This connection highlights the layered relationships in literary history,\\nwhere influential ideas often pass through multiple generations of authors.”\\n5.7\\nAgentic Document Workflows in Agentic RAG\\nAgentic Document Workflows (ADW) [36] extend traditional Retrieval-Augmented Generation (RAG) paradigms by\\nenabling end-to-end knowledge work automation. These workflows orchestrate complex document-centric processes,\\nintegrating document parsing, retrieval, reasoning, and structured outputs with intelligent agents (see Figure 23). ADW\\nsystems address limitations of Intelligent Document Processing (IDP) and RAG by maintaining state, coordinating\\nmulti-step workflows, and applying domain-specific logic to documents.\\nWorkflow\\n1. Document Parsing and Information Structuring:\\n• Documents are parsed using enterprise-grade tools (e.g., LlamaParse) to extract relevant data fields such\\nas invoice numbers, dates, vendor information, line items, and payment terms.\\n• Structured data is organized for downstream processing.\\n2. State Maintenance Across Processes:\\n• The system maintains state about document context, ensuring consistency and relevance across multi-step\\nworkflows.\\n• Tracks the progression of the document through various processing stages.\\n3. Knowledge Retrieval:\\n• Relevant references are retrieved from external knowledge bases (e.g., LlamaCloud) or vector indexes.\\n• Retrieves real-time, domain-specific guidelines for enhanced decision-making.\\n4. Agentic Orchestration:\\n• Intelligent agents apply business rules, perform multi-hop reasoning, and generate actionable recommen-\\ndations.\\n• Orchestrates components such as parsers, retrievers, and external APIs for seamless integration.\\n5. Actionable Output Generation:\\n• Outputs are presented in structured formats, tailored to specific use cases.\\n• Recommendations and extracted insights are synthesized into concise and actionable reports.\\n27'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 27}, page_content='Figure 23: An Overview of Agentic Document Workflows (ADW)\\n[36]\\nUse Case: Invoice Payments Workflow\\nPrompt: Generate a payment recommendation report based on the submitted invoice and associated vendor\\ncontract terms.\\nSystem Process (ADW Workflow):\\n1. Parse the invoice to extract key details such as invoice number, date, vendor information, line items,\\nand payment terms.\\n2. Retrieve the corresponding vendor contract to verify payment terms and identify any applicable\\ndiscounts or compliance requirements.\\n3. Generate a payment recommendation report that includes original amount due, potential early payment\\ndiscounts, budget impact analysis, and strategic payment actions.\\nResponse: Integrated Response: \"Invoice INV-2025-045 for $15,000.00 has been processed. An early payment\\ndiscount of 2% is available if paid by 2025-04-10, reducing the amount due to $14,700.00. A bulk order discount\\nof 5% was applied as the subtotal exceeded $10,000.00. It is recommended to approve early payment to save\\n2% and ensure timely fund allocation for upcoming project phases.\"\\nKey Features and Advantages\\n• State Maintenance: Tracks document context and workflow stage, ensuring consistency across processes.\\n• Multi-Step Orchestration: Handles complex workflows involving multiple components and external tools.\\n• Domain-Specific Intelligence: Applies tailored business rules and guidelines for precise recommendations.\\n• Scalability: Supports large-scale document processing with modular and dynamic agent integration.\\n• Enhanced Productivity: Automates repetitive tasks while augmenting human expertise in decision-making.\\n28'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 28}, page_content='6\\nComparative Analysis of Agentic RAG Frameworks\\nTable 2 provides a comprehensive comparative analysis of the three architectural frameworks: Traditional RAG, Agentic\\nRAG, and Agentic Document Workflows (ADW). This analysis highlights their respective strengths, weaknesses, and\\nbest-fit scenarios, offering valuable insights into their applicability across diverse use cases.\\nTable 2: Comparative Analysis: Traditional RAG vs Agentic RAG vs Agentic Document Workflows (ADW)\\nFeature\\nTraditional RAG\\nAgentic RAG\\nAgentic Document\\nWorkflows (ADW)\\nFocus\\nIsolated retrieval and\\ngeneration tasks\\nMulti-agent\\ncollaboration and\\nreasoning\\nDocument-centric\\nend-to-end workflows\\nContext Maintenance\\nLimited\\nEnabled through\\nmemory modules\\nMaintains state across\\nmulti-step workflows\\nDynamic Adaptability\\nMinimal\\nHigh\\nTailored to document\\nworkflows\\nWorkflow\\nOrchestration\\nAbsent\\nOrchestrates multi-agent\\ntasks\\nIntegrates multi-step\\ndocument processing\\nUse of External\\nTools/APIs\\nBasic integration (e.g.,\\nretrieval tools)\\nExtends via tools like\\nAPIs and knowledge\\nbases\\nDeeply integrates business\\nrules and domain-specific\\ntools\\nScalability\\nLimited to small\\ndatasets or queries\\nScalable for multi-agent\\nsystems\\nScales for multi-domain\\nenterprise workflows\\nComplex Reasoning\\nBasic (e.g., simple\\nQ&A)\\nMulti-step reasoning\\nwith agents\\nStructured reasoning across\\ndocuments\\nPrimary Applications\\nQA systems, knowledge\\nretrieval\\nMulti-domain\\nknowledge and\\nreasoning\\nContract review, invoice\\nprocessing, claims analysis\\nStrengths\\nSimplicity, quick setup\\nHigh accuracy,\\ncollaborative reasoning\\nEnd-to-end automation,\\ndomain-specific intelligence\\nChallenges\\nPoor contextual\\nunderstanding\\nCoordination\\ncomplexity\\nResource overhead, domain\\nstandardization\\nThe comparative analysis underscores the evolutionary trajectory from Traditional RAG to Agentic RAG and further to\\nAgentic Document Workflows (ADW). While Traditional RAG offers simplicity and ease of deployment for basic tasks,\\nAgentic RAG introduces enhanced reasoning and scalability through multi-agent collaboration. ADW builds upon these\\nadvancements by providing robust, document-centric workflows that facilitate end-to-end automation and integration\\nwith domain-specific processes. Understanding the strengths and limitations of each framework is crucial for selecting\\nthe most appropriate architecture to meet specific application requirements and operational demands.\\n7\\nApplications of Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems have demonstrated transformative potential across a variety\\nof domains. By combining real-time data retrieval, generative capabilities, and autonomous decision-making, these\\nsystems address complex, dynamic, and multi-modal challenges. This section explores the key applications of Agentic\\nRAG, providing detailed insights into how these systems are shaping industries such as customer support, healthcare,\\nfinance, education, legal workflows, and creative industries.\\n7.1\\nCustomer Support and Virtual Assistants\\nAgentic RAG systems are revolutionizing customer support by enabling real-time, context-aware query resolution.\\nTraditional chatbots and virtual assistants often rely on static knowledge bases, leading to generic or outdated responses.\\n29'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 29}, page_content='By contrast, Agentic RAG systems dynamically retrieve the most relevant information, adapt to the user’s context, and\\ngenerate personalized responses.\\nUse Case: Twitch Ad Sales Enhancement [37]\\nFor instance, Twitch leveraged an agentic workflow with RAG on Amazon Bedrock to streamline ad sales. The system\\ndynamically retrieved advertiser data, historical campaign performance, and audience demographics to generate detailed\\nad proposals, significantly boosting operational efficiency.\\nKey Benefits:\\n• Improved Response Quality: Personalized and context-aware replies enhance user engagement.\\n• Operational Efficiency: Reduces the workload on human support agents by automating complex queries.\\n• Real-Time Adaptability: Dynamically integrates evolving data, such as live service outages or pricing\\nupdates.\\n7.2\\nHealthcare and Personalized Medicine\\nIn healthcare, the integration of patient-specific data with the latest medical research is critical for informed decision-\\nmaking. Agentic RAG systems enable this by retrieving real-time clinical guidelines, medical literature, and patient\\nhistory to assist clinicians in diagnostics and treatment planning.\\nUse Case: Patient Case Summary [38]\\nAgentic RAG systems have been applied in generating patient case summaries. For example, by integrating electronic\\nhealth records (EHR) and up-to-date medical literature, the system generates comprehensive summaries for clinicians\\nto make faster and more informed decisions.\\nKey Benefits:\\n• Personalized Care: Tailors recommendations to individual patient needs.\\n• Time Efficiency: Streamlines the retrieval of relevant research, saving valuable time for healthcare providers.\\n• Accuracy: Ensures recommendations are based on the latest evidence and patient-specific parameters.\\n7.3\\nLegal and Contract Analysis\\nAgentic RAG systems are redefining how legal workflows are conducted, offering tools for rapid document analysis and\\ndecision-making.\\nUse Case: Contract Review [39]\\nA legal agentic RAG system can analyze contracts, extract critical clauses, and identify potential risks. By combining\\nsemantic search capabilities with legal knowledge graphs, it automates the tedious process of contract review, ensuring\\ncompliance and mitigating risks.\\nKey Benefits:\\n• Risk Identification: Automatically flags clauses that deviate from standard terms.\\n• Efficiency: Reduces the time spent on contract review processes.\\n• Scalability: Handles large volumes of contracts simultaneously.\\n7.4\\nFinance and Risk Analysis\\nAgentic RAG systems are transforming the finance industry by providing real-time insights for investment decisions,\\nmarket analysis, and risk management. These systems integrate live data streams, historical trends, and predictive\\nmodeling to generate actionable outputs.\\nUse Case: Auto Insurance Claims Processing [40]\\nIn auto insurance, Agentic RAG can automate claim processing. For example, by retrieving policy details and combining\\nthem with accident data, it generates claim recommendations while ensuring compliance with regulatory requirements.\\nKey Benefits:\\n• Real-Time Analytics: Delivers insights based on live market data.\\n30'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 30}, page_content='• Risk Mitigation: Identifies potential risks using predictive analysis and multi-step reasoning.\\n• Enhanced Decision-Making: Combines historical and live data for comprehensive strategies.\\n7.5\\nEducation and Personalized Learning\\nEducation is another domain where Agentic RAG systems are making significant strides. These systems enable adaptive\\nlearning by generating explanations, study materials, and feedback tailored to the learner’s progress and preferences.\\nUse Case: Research Paper Generation [41]\\nIn higher education, Agentic RAG has been used to assist researchers by synthesizing key findings from multiple\\nsources. For instance, a researcher querying, “What are the latest advancements in quantum computing?” receives a\\nconcise summary enriched with references, enhancing the quality and efficiency of their work.\\nKey Benefits:\\n• Tailored Learning Paths: Adapts content to individual student needs and performance levels.\\n• Engaging Interactions: Provides interactive explanations and personalized feedback.\\n• Scalability: Supports large-scale deployments for diverse educational environments.\\n7.6\\nGraph-Enhanced Applications in Multimodal Workflows\\nGraph-Enhanced Agentic RAG (GEAR) combines graph structures with retrieval mechanisms, making it particularly\\neffective in multimodal workflows where interconnected data sources are essential.\\nUse Case: Market Survey Generation\\nGEAR enables the synthesis of text, images, and videos for marketing campaigns. For example, querying, “What\\nare the emerging trends in eco-friendly products?” generates a detailed report enriched with customer preferences,\\ncompetitor analysis, and multimedia content.\\nKey Benefits:\\n• Multi-Modal Capabilities: Integrates text, image, and video data for comprehensive outputs.\\n• Enhanced Creativity: Generates innovative ideas and solutions for marketing and entertainment.\\n• Dynamic Adaptability: Adapts to evolving market trends and customer needs.\\nThe applications of Agentic RAG systems span a wide range of industries, showcasing their versatility and transformative\\npotential. From personalized customer support to adaptive education and graph-enhanced multimodal workflows, these\\nsystems address complex, dynamic, and knowledge-intensive challenges. By integrating retrieval, generation, and\\nagentic intelligence, Agentic RAG systems are paving the way for next-generation AI applications.\\n8\\nTools and Frameworks for Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant evolution in combining retrieval,\\ngeneration, and agentic intelligence. These systems extend the capabilities of traditional RAG by integrating decision-\\nmaking, query reformulation, and adaptive workflows. The following tools and frameworks provide robust support for\\ndeveloping Agentic RAG systems, addressing the complex requirements of real-world applications.\\nKey Tools and Frameworks:\\n• LangChain and LangGraph: LangChain [42] provides modular components for building RAG pipelines,\\nseamlessly integrating retrievers, generators, and external tools. LangGraph complements this by introducing\\ngraph-based workflows that support loops, state persistence, and human-in-the-loop interactions, enabling\\nsophisticated orchestration and self-correction mechanisms in agentic systems.\\n• LlamaIndex: LlamaIndex’s [43] Agentic Document Workflows (ADW) enable end-to-end automation of\\ndocument processing, retrieval, and structured reasoning. It introduces a meta-agent architecture where\\nsub-agents manage smaller document sets, coordinating through a top-level agent for tasks such as compliance\\nanalysis and contextual understanding.\\n• Hugging Face Transformers and Qdrant: Hugging Face [44] offers pre-trained models for embedding and\\ngeneration tasks, while Qdrant [45] enhances retrieval workflows with adaptive vector search capabilities,\\nallowing agents to optimize performance by dynamically switching between sparse and dense vector methods.\\n31'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 31}, page_content='• CrewAI and AutoGen: These frameworks emphasize multi-agent architectures. CrewAI [46] supports\\nhierarchical and sequential processes, robust memory systems, and tool integrations. AG2 [47] (formerly\\nknows as AutoGen [48, 49]) excels in multi-agent collaboration with advanced support for code generation,\\ntool execution, and decision-making.\\n• OpenAI Swarm Framework: An educational framework designed for ergonomic, lightweight multi-agent\\norchestration [50], emphasizing agent autonomy and structured collaboration.\\n• Agentic RAG with Vertex AI: Developed by Google, Vertex AI [51] integrates seamlessly with Agentic\\nRetrieval-Augmented Generation (RAG), providing a platform to build, deploy, and scale machine learning\\nmodels while leveraging advanced AI capabilities for robust, contextually aware retrieval and decision-making\\nworkflows.\\n• Semantic Kernel: Semantic Kernel [52, 53] is an open-source SDK by Microsoft that integrates large language\\nmodels (LLMs) into applications. It supports agentic patterns, enabling the creation of autonomous AI agents\\nfor natural language understanding, task automation, and decision-making. It has been used in scenarios like\\nServiceNow’s P1 incident management to facilitate real-time collaboration, automate task execution, and\\nretrieve contextual information seamlessly\\n• Amazon Bedrock for Agentic RAG: Amazon Bedrock [37] provides a robust platform for implementing\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n• IBM Watson and Agentic RAG: IBM’s watsonx.ai [54] supports building Agentic RAG systems, exemplified\\nby using the Granite-3-8B-Instruct model to answer complex queries by integrating external information and\\nenhancing response accuracy.\\n• Neo4j and Vector Databases: Neo4j, a prominent open-source graph database, excels in handling complex\\nrelationships and semantic queries. Alongside Neo4j, vector databases like Weaviate, Pinecone, Milvus, and\\nQdrant provide efficient similarity search and retrieval capabilities, forming the backbone of high-performance\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n9\\nBenchmarks and Datasets\\nCurrent benchmarks and datasets provide valuable insights into evaluating Retrieval-Augmented Generation (RAG)\\nsystems, including those with agentic and graph-based enhancements. While some are explicitly designed for RAG,\\nothers are adapted to test retrieval, reasoning, and generation capabilities in diverse scenarios. Datasets are crucial for\\ntesting the retrieval, reasoning, and generation components of RAG systems. Table 3 discusses some key datasets based\\non the dowstream task for RAG Evaluation.\\nBenchmarks play a critical role in standardizing the evaluation of RAG systems by providing structured tasks and\\nmetrics. The following benchmarks are particularly relevant:\\n• BEIR (Benchmarking Information Retrieval): A versatile benchmark designed for evaluating embedding\\nmodels on a variety of information retrieval tasks, encompassing 17 datasets across diverse domains like\\nbioinformatics, finance, and question answering [55].\\n• MS MARCO (Microsoft Machine Reading Comprehension): Focused on passage ranking and question\\nanswering, this benchmark is widely used for dense retrieval tasks in RAG systems [56].\\n• TREC (Text REtrieval Conference, Deep Learning Track): Provides datasets for passage and document\\nretrieval, emphasizing the quality of ranking models in retrieval pipelines [57].\\n• MuSiQue (Multihop Sequential Questioning): A benchmark for multihop reasoning across multiple\\ndocuments, emphasizing the importance of retrieving and synthesizing information from disconnected contexts\\n[58].\\n• 2WikiMultihopQA: A dataset designed for multihop QA tasks over two Wikipedia articles, focusing on the\\nability to connect knowledge across multiple sources [59].\\n• AgentG (Agentic RAG for Knowledge Fusion): Tailored for agentic RAG tasks, this benchmark assesses\\ndynamic information synthesis across multiple knowledge bases [8].\\n• HotpotQA: A multi-hop QA benchmark requiring retrieval and reasoning over interconnected contexts, ideal\\nfor evaluating complex RAG workflows[60].\\n• RAGBench: A large-scale, explainable benchmark featuring 100,000 examples across industry domains, with\\na TRACe evaluation framework for actionable RAG metrics [61].\\n32'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 32}, page_content='• BERGEN (Benchmarking Retrieval-Augmented Generation): A library for systematically benchmarking\\nRAG systems with standardized experiments [62].\\n• FlashRAG Toolkit: Implements 12 RAG methods and includes 32 benchmark datasets to support efficient\\nand standardized RAG evaluation [63].\\n• GNN-RAG: This benchmark evaluates graph-based RAG systems on tasks like node-level and edge-level\\npredictions, focusing on retrieval quality and reasoning performance in Knowledge Graph Question Answering\\n(KGQA) [64].\\nTable 3: Downstream Tasks and Datasets for RAG Evaluation (Adapted from [20]\\nCategory\\nTask Type\\nDatasets and References\\nQA\\nSingle-hop QA\\nNatural Questions (NQ) [65], TriviaQA [66], SQuAD [67],\\nWeb Questions (WebQ) [68], PopQA [69], MS MARCO\\n[56]\\nMulti-hop QA\\nHotpotQA [60], 2WikiMultiHopQA [59], MuSiQue [58]\\nLong-form QA\\nELI5 [70], NarrativeQA (NQA) [71], ASQA [72], QM-\\nSum [73]\\nDomain-specific QA\\nQasper [74], COVID-QA [75], CMB/MMCU Medical\\n[76]\\nMulti-choice QA\\nQuALITY [77], ARC (No reference available), Common-\\nsenseQA [78]\\nGraph-based QA\\nGraph QA\\nGraphQA [79]\\nEvent Argument Extraction\\nWikiEvent [80], RAMS [81]\\nDialog\\nOpen-domain Dialog\\nWizard of Wikipedia (WoW) [82]\\nPersonalized Dialog\\nKBP [83], DuleMon [84]\\nTask-oriented Dialog\\nCamRest [85]\\nRecommendation\\nPersonalized Content\\nAmazon Datasets (Toys, Sports, Beauty) [86]\\nReasoning\\nCommonsense Reasoning\\nHellaSwag [87], CommonsenseQA [78]\\nCoT Reasoning\\nCoT Reasoning [88]\\nComplex Reasoning\\nCSQA [89]\\nOthers\\nLanguage Understanding\\nMMLU (No reference available), WikiText-103 [65]\\nFact Checking/Verification\\nFEVER [90], PubHealth [91]\\nStrategy QA\\nStrategyQA [92]\\nSummarization\\nText Summarization\\nWikiASP [93], XSum [94]\\nLong-form Summarization\\nNarrativeQA (NQA) [71], QMSum [73]\\nText Generation\\nBiography\\nBiography Dataset (No reference available)\\nText Classification\\nSentiment Analysis\\nSST-2 [95]\\nGeneral Classification\\nVioLens[96], TREC [57]\\nCode Search\\nProgramming Search\\nCodeSearchNet [97]\\nRobustness\\nRetrieval Robustness\\nNoMIRACL [98]\\nLanguage Modeling Robustness\\nWikiText-103 [99]\\nMath\\nMath Reasoning\\nGSM8K [100]\\nMachine Translation\\nTranslation Tasks\\nJRC-Acquis [101]\\n10\\nConclusion\\nAgentic Retrieval-Augmented Generation (RAG) represents a transformative advancement in artificial intelligence,\\naddressing the limitations of traditional RAG systems through the integration of autonomous agents. By leveraging\\n33'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 33}, page_content='agentic intelligence, these systems introduce capabilities such as dynamic decision-making, iterative reasoning, and\\ncollaborative workflows, enabling them to tackle complex, real-world tasks with enhanced precision and adaptability.\\nThis survey explored the evolution of RAG systems, from their initial implementations to advanced paradigms like\\nModular RAG, highlighting the contributions and limitations of each. The integration of agents into the RAG pipeline\\nhas emerged as a pivotal development, resulting in Agentic RAG systems that overcome static workflows and limited\\ncontextual adaptability. Applications across healthcare, finance, education, and creative industries demonstrate the\\ntransformative potential of these systems, showcasing their ability to deliver personalized, real-time, and context-aware\\nsolutions.\\nDespite their promise, Agentic RAG systems face challenges that require further research and innovation. Coordination\\ncomplexity in multi-agent architectures, scalability, and latency issues, as well as ethical considerations, must be\\naddressed to ensure robust and responsible deployment. Additionally, the lack of specialized benchmarks and datasets\\ntailored to evaluate agentic capabilities poses a significant hurdle. Developing evaluation methodologies that capture\\nthe unique aspects of Agentic RAG, such as multi-agent collaboration and dynamic adaptability, will be crucial for\\nadvancing the field.\\nLooking ahead, the convergence of retrieval-augmented generation and agentic intelligence has the potential to redefine\\nAI’s role in dynamic and complex environments. By addressing these challenges and exploring future directions,\\nresearchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for transformative\\napplications across industries and domains. As AI systems continue to evolve, Agentic RAG stands as a cornerstone for\\ncreating adaptive, context-aware, and impactful solutions that meet the demands of a rapidly changing world.\\nReferences\\n[1] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and\\nJianfeng Gao. Large language models: A survey, 2024.\\n[2] Aditi Singh. Exploring language models: A comprehensive survey and analysis. In 2023 International Con-\\nference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication\\nEngineering (RMKMATE), pages 1–4, 2023.\\n[3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\\nJunjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,\\nYifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language\\nmodels, 2024.\\n[4] Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. A complete survey on llm-based ai\\nchatbots, 2024.\\n[5] Aditi Singh. A survey of ai text-to-image and ai text-to-video generators. In 2023 4th International Conference\\non Artificial Intelligence, Robotics and Control (AIRC), pages 32–36, 2023.\\n[6] Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and Tala Talaei Khoei.\\nExploring prompt engineering: A systematic review with swot analysis, 2024.\\n[7] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles,\\ntaxonomy, challenges, and open questions. ACM Transactions on Information Systems, November 2024.\\n[8] Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala,\\nand Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented generation, 2024.\\n[9] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao\\nZhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey, 2024.\\n[10] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan,\\nand Graham Neubig. Active retrieval augmented generation, 2023.\\n[11] Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n[12] Anthropic.\\nBuilding\\neffective\\nagents,\\n2024.\\nhttps://www.anthropic.com/research/\\nbuilding-effective-agents. Accessed: February 2, 2025.\\n[13] LangChain.\\nLanggraph workflows tutorial, 2025.\\nhttps://langchain-ai.github.io/langgraph/\\ntutorials/workflows/. Accessed: February 2, 2025.\\n34'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 34}, page_content='[14] Chidaksh Ravuru, Sagar Srinivas Sakhinana, and Venkataramana Runkana.\\nAgentic retrieval-augmented\\ngeneration for time series analysis, 2024.\\n[15] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey, 2023.\\n[16] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang.\\nGraph retrieval-augmented generation: A survey, 2024.\\n[17] Aditi Singh, Abul Ehtesham, Saifuddin Mahmud, and Jong-Hoon Kim. Revolutionizing mental health care\\nthrough langchain: A journey with a large language model. In 2024 IEEE 14th Annual Computing and\\nCommunication Workshop and Conference (CCWC), pages 0073–0078, 2024.\\n[18] Gaurav Kumar Gupta, Aditi Singh, Sijo Valayakkad Manikandan, and Abul Ehtesham. Digital diagnostics: The\\npotential of large language models in recognizing symptoms of common illnesses. AI, 6(1), 2025.\\n[19] Aditi Singh, Abul Ehtesham, Saket Kumar, Gaurav Kumar Gupta, and Tala Talaei Khoei. Encouraging responsible\\nuse of generative ai in education: A reward-based learning approach. In Tim Schlippe, Eric C. K. Cheng, and\\nTianchong Wang, editors, Artificial Intelligence in Education Technologies: New Development and Innovative\\nPractices, pages 404–413, Singapore, 2025. Springer Nature Singapore.\\n[20] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and\\nHaofen Wang. Retrieval-augmented generation for large language models: A survey, 2024.\\n[21] Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen\\ntau Yih. Dense passage retrieval for open-domain question answering, 2020.\\n[22] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong\\nWen. A survey on the memory mechanism of large language model based agents, 2024.\\n[23] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large\\nlanguage models can self-correct with tool-interactive critiquing, 2024.\\n[24] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang,\\nand Enhong Chen. Understanding the planning of llm agents: A survey, 2024.\\n[25] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Enhancing ai systems with agentic workflows\\npatterns in large language model. In 2024 IEEE World AI IoT Congress (AIIoT), pages 527–532, 2024.\\n[26] DeepLearning.AI. How agents can improve llm performance. https://www.deeplearning.ai/the-batch/\\nhow-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io,\\n2024.\\nAc-\\ncessed: 2025-01-13.\\n[27] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,\\nSean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.\\n[28] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning, 2023.\\n[29] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and\\nXiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges, 2024.\\n[30] Weaviate Blog. What is agentic rag? https://weaviate.io/blog/what-is-agentic-rag#:~:text=is%\\n20Agentic%20RAG%3F-,%E2%80%8B,of%20the%20non%2Dagentic%20pipeline. Accessed: 2025-01-14.\\n[31] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation, 2024.\\n[32] LangGraph CRAG Tutorial. Langgraph crag: Contextualized retrieval-augmented generation tutorial. https:\\n//langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/. Accessed: 2025-01-14.\\n[33] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. Adaptive-rag: Learning to adapt\\nretrieval-augmented large language models through question complexity, 2024.\\n[34] LangGraph Adaptive RAG Tutorial. Langgraph adaptive rag: Adaptive retrieval-augmented generation tu-\\ntorial.\\nhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/.\\nAccessed: 2025-01-14.\\n[35] Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Damien Graux, Dandan\\nTu, Zeren Jiang, Ruofei Lai, Yang Ren, and Jeff Z. Pan. Gear: Graph-enhanced agent for retrieval-augmented\\ngeneration, 2024.\\n[36] LlamaIndex.\\nIntroducing agentic\\ndocument\\nworkflows.\\nhttps://www.llamaindex.ai/blog/\\nintroducing-agentic-document-workflows, 2025. Accessed: 2025-01-13.\\n35'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 35}, page_content='[37] AWS\\nMachine\\nLearning\\nBlog.\\nHow\\ntwitch\\nused\\nagentic\\nworkflow\\nwith\\nrag\\non\\namazon\\nbedrock\\nto\\nsupercharge\\nad\\nsales.\\nhttps://aws.amazon.com/blogs/machine-learning/\\nhow-twitch-used-agentic-workflow-with-rag-on-amazon-bedrock-to-supercharge-ad-sales/,\\n2025. Accessed: 2025-01-13.\\n[38] LlamaCloud Demo Repository.\\nPatient case summary workflow using llamacloud.\\nhttps:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/\\npatient_case_summary/patient_case_summary.ipynb, 2025. Accessed: 2025-01-13.\\n[39] LlamaCloud Demo Repository.\\nContract review workflow using llamacloud.\\nhttps://github.com/\\nrun-llama/llamacloud-demo/blob/main/examples/document_workflows/contract_review/\\ncontract_review.ipynb, 2025. Accessed: 2025-01-13.\\n[40] LlamaCloud Demo Repository.\\nAuto insurance claims workflow using llamacloud.\\nhttps:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/auto_\\ninsurance_claims/auto_insurance_claims.ipynb, 2025. Accessed: 2025-01-13.\\n[41] LlamaCloud Demo Repository.\\nResearch paper report generation workflow using llamacloud.\\nhttps://github.com/run-llama/llamacloud-demo/blob/main/examples/report_generation/\\nresearch_paper_report_generation.ipynb, 2025. Accessed: 2025-01-13.\\n[42] LangGraph Agentic RAG Tutorial. Langgraph agentic rag: Nodes and edges tutorial. https://langchain-ai.\\ngithub.io/langgraph/tutorials/rag/langgraph_agentic_rag/#nodes-and-edges.\\nAccessed:\\n2025-01-14.\\n[43] LlamaIndex\\nBlog.\\nAgentic\\nrag\\nwith\\nllamaindex.\\nhttps://www.llamaindex.ai/blog/\\nagentic-rag-with-llamaindex-2721b8a49ff6. Accessed: 2025-01-14.\\n[44] Hugging Face Cookbook. Agentic rag: Turbocharge your retrieval-augmented generation with query reformula-\\ntion and self-query. https://huggingface.co/learn/cookbook/en/agent_rag. Accessed: 2025-01-14.\\n[45] Qdrant Blog. Agentic rag: Combining rag with agents for enhanced information retrieval. https://qdrant.\\ntech/articles/agentic-rag/. Accessed: 2025-01-14.\\n[46] crewAI Inc. crewai: A github repository for ai projects. https://github.com/crewAIInc/crewAI, 2025.\\nAccessed: 2025-01-15.\\n[47] AG2AI Contributors. Ag2: A github repository for advanced generative ai research. https://github.com/\\nag2ai/ag2, 2025. Accessed: 2025-01-15.\\n[48] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun\\nZhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling\\nnext-gen llm applications via multi-agent conversation framework. 2023.\\n[49] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Training\\nlanguage model agents without modifying language models. ICML’24, 2024.\\n[50] OpenAI. Swarm: Lightweight multi-agent orchestration framework. https://github.com/openai/swarm.\\nAccessed: 2025-01-14.\\n[51] LlamaIndex Documentation. Agentic rag using vertex ai. https://docs.llamaindex.ai/en/stable/\\nexamples/agent/agentic_rag_using_vertex_ai/. Accessed: 2025-01-14.\\n[52] Microsoft. Semantic kernel overview, 2025. https://learn.microsoft.com/en-us/semantic-kernel/\\noverview/. Accessed: February 2, 2025.\\n[53] Microsoft. Semantic kernel github repository, 2025. https://github.com/microsoft/semantic-kernel.\\nAccessed: February 2, 2025.\\n[54] IBM Granite Community.\\nAgentic rag: Ai agents with ibm granite models.\\nhttps://github.com/\\nibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_\\nRAG.ipynb. Accessed: 2025-01-14.\\n[55] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous\\nbenchmark for zero-shot evaluation of information retrieval models, 2021.\\n[56] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew\\nMcNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong\\nWang. Ms marco: A human generated machine reading comprehension dataset, 2018.\\n[57] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff.\\nOverview of the trec 2022 deep learning track. In Text REtrieval Conference (TREC). NIST, TREC, March 2023.\\n36'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 36}, page_content='[58] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\\nvia single-hop question composition, 2022.\\n[59] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset\\nfor comprehensive evaluation of reasoning steps, 2020.\\n[60] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\\n[61] Robert Friel, Masha Belyi, and Atindriyo Sanyal. Ragbench: Explainable benchmark for retrieval-augmented\\ngeneration systems, 2024.\\n[62] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and Stéphane\\nClinchant. Bergen: A benchmarking library for retrieval-augmented generation, 2024.\\n[63] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient\\nretrieval-augmented generation research, 2024.\\n[64] Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning,\\n2024.\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei\\nChang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question\\nanswering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019.\\n[66] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\\nchallenge dataset for reading comprehension, 2017.\\n[67] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\\ncomprehension of text, 2016.\\n[68] Jonathan Berant, Andrew K. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-\\nanswer pairs. In Conference on Empirical Methods in Natural Language Processing, 2013.\\n[69] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to\\ntrust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers,\\nJordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada, July 2023. Association\\nfor Computational Linguistics.\\n[70] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form\\nquestion answering, 2019.\\n[71] Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward\\nGrefenstette. The narrativeqa reading comprehension challenge. 2017.\\n[72] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form\\nanswers, 2023.\\n[73] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. pages 5905–5921, June 2021.\\n[74] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-\\nseeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke\\nZettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\\nZhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, pages 4599–4610, Online, June 2021. Association\\nfor Computational Linguistics.\\n[75] Timo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. COVID-QA: A question answering\\ndataset for COVID-19. In ACL 2020 Workshop on Natural Language Processing for COVID-19 (NLP-COVID),\\n2020.\\n[76] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang,\\nJianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. Cmb: A comprehensive medical benchmark in chinese,\\n2024.\\n[77] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh\\nPadmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R. Bowman. Quality: Question answering with\\nlong input texts, yes!, 2022.\\n37'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 37}, page_content='[78] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering\\nchallenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,\\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics.\\n[79] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan\\nHooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\n2024.\\n[80] Sha Li, Heng Ji, and Jiawei Han. Document-level event argument extraction by conditional generation, 2021.\\n[81] Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. Multi-sentence argument\\nlinking, 2020.\\n[82] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia:\\nKnowledge-powered conversational agents, 2019.\\n[83] Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan,\\nIrwin King, and Kam-Fai Wong. Large language models as source planner for personalized knowledge-grounded\\ndialogue, 2023.\\n[84] Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. Long time\\nno see! open-domain conversation with long-term persona memory, 2022.\\n[85] Tsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David\\nVandyke, and Steve Young. Conditional generation and snapshot learning in neural dialogue systems. In\\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2153–2162,\\nAustin, Texas, November 2016. Association for Computational Linguistics.\\n[86] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class\\ncollaborative filtering. In Proceedings of the 25th International Conference on World Wide Web, WWW ’16, page\\n507–517, Republic and Canton of Geneva, CHE, 2016. International World Wide Web Conferences Steering\\nCommittee.\\n[87] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really\\nfinish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4791–4800, Florence, Italy, July 2019.\\nAssociation for Computational Linguistics.\\n[88] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The\\ncot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning,\\n2023.\\n[89] Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex\\nsequential question answering: Towards learning to converse over linked question answer pairs with a knowledge\\ngraph. 2018.\\n[90] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for\\nfact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018\\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n[91] Neema Kotonya and Francesca Toni. Explainable automated fact-checking for public health claims, 2020.\\n[92] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop?\\na question answering benchmark with implicit reasoning strategies, 2021.\\n[93] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig. Wikiasp:\\nA dataset for multi-domain aspect-based summarization, 2020.\\n[94] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware\\nconvolutional neural networks for extreme summarization, 2018.\\n[95] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher\\nPotts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky,\\nTimothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013\\nConference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington,\\nUSA, October 2013. Association for Computational Linguistics.\\n38'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 38}, page_content='[96] Sourav Saha, Jahedul Alam Junaed, Maryam Saleki, Arnab Sen Sharma, Mohammad Rashidujjaman Rifat,\\nMohamed Rahouti, Syed Ishtiaque Ahmed, Nabeel Mohammed, and Mohammad Ruhul Amin. Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms of communal violence and its evaluation. In\\nFiroj Alam, Sudipta Kar, Shammur Absar Chowdhury, Farig Sadeque, and Ruhul Amin, editors, Proceedings\\nof the First Workshop on Bangla Language Processing (BLP-2023), pages 72–84, Singapore, December 2023.\\nAssociation for Computational Linguistics.\\n[97] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet\\nchallenge: Evaluating the state of semantic code search, 2020.\\n[98] Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo,\\nXiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, and Jimmy Lin. \"knowing when you don’t know\":\\nA multilingual relevance assessment dataset for robust retrieval-augmented generation, 2024.\\n[99] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\\n[100] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\\nsolve math word problems, 2021.\\n[101] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, Dan Tufi¸s, and Dániel Varga.\\nThe JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Nicoletta Calzolari, Khalid\\nChoukri, Aldo Gangemi, Bente Maegaard, Joseph Mariani, Jan Odijk, and Daniel Tapias, editors, Proceedings of\\nthe Fifth International Conference on Language Resources and Evaluation (LREC‘06), Genoa, Italy, May 2006.\\nEuropean Language Resources Association (ELRA).\\n39'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 4}, page_content='MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n6'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 6}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\nResidual Dropout\\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [15]\\n23.75\\nDeep-Att + PosUnk [32]\\n39.2\\n1.0 · 1020\\nGNMT + RL [31]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [8]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [26]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [32]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [31]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [8]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.0\\n2.3 · 1019\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 9}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'), Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 10}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 0}, page_content='ScienceDirect\\nAvailable online at www.sciencedirect.com\\nProcedia Computer Science 246 (2024) 3781–3790\\n1877-0509 © 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge \\nBased and Intelligent information and Engineering Systems\\n10.1016/j.procs.2024.09.178\\nKeywords: Large Language Models (LLMs); Natural Language Processing (NLP); Retrieval-Augmented Generation (RAG); Text generation; \\nDigital transformation. \\n1. Introduction \\nDigital transformation signifies the incorporation of digital technology across different facets of a business, \\nreshaping its operations and value delivery to customers [1]. At the forefront of driving such transformative \\npractices are Large Language Models (LLMs), advanced machine learning models trained extensively on textual \\ndata to comprehend and produce human-like text [1]. LLMs, such as the Generative Pre-training Transformer (GPT) \\n \\n \\n* Corresponding author. Tel.: +33 03 80 39 50 00; fax: +33 03 80 39 50 69.  \\nE-mail address: muhammad.arslan@u-bourgogne.fr \\n28th International Conference on Knowledge-Based and Intelligent Information & Engineering \\nSystems (KES 2024) \\nA Survey on RAG with LLMs \\nMuhammad Arslana*, Hussam Ghanema, Saba Munawarb and Christophe Cruza \\naLaboratoire Interdisciplinaire Carnot de Bourgogne (ICB), Dijon, France \\nbNational University of Computer and Emerging Sciences (NUCES), Islamabad, Pakistan                                                                 \\nAbstract \\nIn the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain \\ncompetitive and foster growth. Large Language Models (LLMs) have emerged as game-changers across industries, \\nrevolutionizing various sectors by harnessing extensive text data to analyze and generate human-like text. Despite their \\nimpressive capabilities, LLMs often encounter challenges when dealing with domain-specific queries, potentially leading to \\ninaccuracies in their outputs. In response, Retrieval-Augmented Generation (RAG) has emerged as a viable solution. By \\nseamlessly integrating external data retrieval into text generation processes, RAG aims to enhance the accuracy and relevance of \\nthe generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG, \\noverlooking a comprehensive exploration of its applications. This paper seeks to address this gap by providing a thorough review \\nof RAG applications, encompassing both task-specific and discipline-specific studies, while also outlining potential avenues for \\nfuture research. By shedding light on current RAG research and outlining future directions, this review aims to catalyze further \\nexploration and development in this dynamic field, thereby contributing to ongoing digital transformation efforts. \\n© 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge Based and \\nIntelligent information and Engineering Systems'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 1}, page_content='3782\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nseries [2, 3] and others, have demonstrated remarkable capabilities in NLP tasks [4]. However, these models face \\nchallenges when dealing with domain-specific queries, often generating inaccurate or irrelevant information, \\ncommonly referred to as “hallucinations”, particularly when data is sparse [5]. This limitation makes deploying \\nLLMs in real-world settings impractical, as the generated output may not be reliable [4].  \\nIn the middle of 2020, Lewis et al. [6] introduced RAG, a significant advancement in the field of LLMs for \\nimproving generative tasks (see Fig. 1 (a)). RAG incorporates an initial step where LLMs search an external data \\nsource to retrieve relevant information before producing text or answering questions. RAG addresses these \\nlimitations by integrating external data retrieval into the generative process, thereby enhancing the accuracy and \\nrelevance of the generated output. By dynamically retrieving information from knowledge bases during inference, \\nRAG provides a more informed and evidence-based approach to language generation, significantly reducing the risk \\nof hallucinations and improving the overall quality of the generated text [4, 6]. This approach has the potential to \\nmake LLMs more practical for real-world applications, as it ensures that the generated output is grounded in \\nretrieved evidence, leading to more reliable and accurate results. Fig. 1 (b) showcases how real-time business \\nsystems can leverage the RAG with LLM architecture. As an example, without RAG, the system lacks access to \\nreal-time or updated information. However, with RAG integration, leveraging external data sources such as news \\narticles, the system can respond to current business events, presenting opportunities for business intelligence \\nanalysts. \\n \\n                                  (a)                                                                                             (b) \\nFig. 1. (a) A generic RAG architecture, where users’ queries, potentially in different modalities (e.g., text, code, image, etc.), are inputted into \\nboth the retriever and the generator. The retriever scans for relevant data sources in storage, while the generator engages with the retrieval \\noutcomes, ultimately generating results across various modalities [6]; Fig. 1. (b) illustrates how RAG integration with the LLM handles queries \\nthat fall outside the scope of the LLM’s training data.  \\nWhile the field of RAG has seen substantial growth, several online surveys [4, 7, 8, 9] have explored \\ntechnological advancements in RAG. Although these surveys provide valuable insights and references, they offer \\nonly a limited overview of RAG applications. To address this gap, this paper aims to provide an exhaustive \\noverview of RAG applications, including both task-specific and discipline-specific studies, as well as future \\ndirections. By highlighting the current state of RAG research and its potential future directions, this review aims to \\ninspire further investigation and development in this exciting field. \\nThe paper’s structure is as follows: Section 2 presents the adopted research methodology for this survey. In \\nSection 3, we provide an overview of RAG applications, followed by a detailed discussion in Section 4. The paper \\nconcludes in Section 5, summarizing the key findings and implications of the study. \\n2. Background \\nThe research method (see Fig. 2) employed in this paper involves a thorough review and analysis of research \\npublications related to RAG. The main objective is to identify and categorize its applications across various NLP \\ntasks and disciplines. The paper begins by collecting research publications specific to RAG, focusing on their \\napplications. Since the RAG with LLM domain is relatively new and emerging, with many studies available as pre-'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 2}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3783\\nprints online, limiting the search to platforms such as Scopus or IEEE would greatly reduce the number of studies. \\nTherefore, Google Scholar was utilized to access the studies on RAG. However, in cases where both pre-print and \\npublished versions of a study were available, the published version was chosen to cover the maximum number of \\npeer-reviewed studies. Each study underwent manual review to assess its comprehensiveness and depth, excluding \\nshort studies. It is important to note that the purpose of the survey is not to cover the most optimal studies, but rather \\nto provide an overview of how this field has attained significant attention in a short period, with researchers \\nexploring diverse application scenarios.  \\nThe keywords used to collect research publications included “retrieval augmented generation”, “RAG \\napplications”, “generative models with retrieval”, “external data retrieval in text generation”, “enhancing text \\ngeneration with retrieval”, “integrating retrieval into generative models”, “external knowledge in text generation”, \\n“retrieval-based text generation”, “information retrieval for text generation”, and “contextualized retrieval in \\nlanguage models”. These publications are then classified into two principal categories: task-based classification and \\ndiscipline-based classification. Task-based classification focuses on categorizing RAG studies according to their \\nexecution of information processing tasks, particularly within NLP. Conversely, discipline-based classification \\ncategorizes studies based on their application to specific domains. Under the task-based classification, the \\npublications are further subdivided into categories such as Question Answering (QA), Text Generation and \\nSummarization, Information Retrieval and Extraction, Text Analysis and Processing, Software Development and \\nMaintenance (SDM), Decision Making and Applications, and Other Categories. Similarly, under the discipline-\\nbased classification, the publications are further subdivided into categories such as Medical/Biomedical, Financial, \\nEducational, Technology and Software Development, Social and Communication, Literature, and Other Categories. \\nThese categories are selected based on an understanding of the context of the studies and the underlying problems \\nthey address. Within both classification methods, “software development” stands out as a common category. It \\ninvolves programming information processing tasks under task-based classification and encompasses systems for \\ndeveloping various applications across different domains under discipline-based classification. Figure 3 illustrates \\nthe number of publications related to RAG applications from 2020 to February 2024. Specifically, there was a single \\npublication found in 2020, 6 publications in 2022, 28 publications in 2023, and 16 publications until February 2024, \\nindicating a growing interest and research activity in the field of RAG applications. \\nFig. 2. Research Method'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 3}, page_content='3784\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Evolution of Research Publications on RAG Applications \\n3. Applications of RAG with LLMs \\nUpon thorough examination of the selected papers focusing on RAG applications, we uncovered a vast array of \\ndiverse applications. These findings are distilled into a comprehensive table format (see Table 1), detailing three \\ncrucial aspects: 1) Use case with RAG, 2) Used datasets/benchmarks, and 3) Application area. Noteworthy \\napplications span various domains, including biomedical, financial, and medical inquiries, alongside text \\nsummarization and book review generation. RAG’s versatility extends to commonsense QA, table-based queries, \\nand clinical decision-making, among others. It further encompasses educational decision making, textbook question \\nanswering, and enterprise search functionalities. RAG is instrumental in sentiments classification, health education, \\nand generating biomedical explanations, while also enhancing user writing accuracy and speed. Its utility spans \\nhumanitarian assistance, generating informative dialogues, crafting realistic images and intricate plotlines, and much \\nmore.  \\nAdditionally, RAG aids in natural language QA, disease identification, and information extraction. It handles \\ndecision-making tasks, hashtag management, hate speech detection, and scientific document classification. RAG \\nexcels in entity description generation, text correction, and SQL translation, while also enhancing open-domain QA \\nand professional knowledge inquiries. Also, it extends the capabilities of machine translation tasks beyond text-to-\\nSQL, such as neural text re-ranking [6]. Moreover, it supports multicultural enterprise queries, e-commerce \\nsearches, and personalized dialogue systems. Furthermore, RAG facilitates event argument extraction, intelligence \\nreport generation, short-form QA, automated transactions, and private data handling. Lastly, it contributes to science \\nQA, clinical writing, and pharmaceutical regulatory compliance inquiries. \\nAfter compiling all the applications of RAG, the subsequent step involves categorizing them based on the \\nspecific nature of the NLP tasks they tackle (see Table 2 and Fig. 4). From the compiled publications, it was \\nobserved that 20 studies were dedicated to QA, 6 to Text Generation and Summarization, 6 to Information Retrieval \\nand Extraction, 5 to Text Analysis and Processing, 4 to SDM, and 5 to Decision Making and Applications, while the \\nremaining 6 studies were classified under \"Other Categories.\" This classification is significant as it helps in \\nunderstanding the distribution and focus of RAG applications across different NLP tasks. Additionally, since RAG \\napplications span various disciplines, further classification (see Table 3 and Fig. 5) reveals that 9 publications were \\nrelated to Medical/Biomedical, 2 to Financial, 2 to Educational, 9 to Technology and Software Development, 7 to \\nSocial and Communication, and 3 to Literature, with the remaining falling into \"Other Categories\". \\nTable 1. Applications of RAG  \\nNo. \\nUse case with RAG \\nUsed datasets / benchmarks \\nApplication area \\n1 \\nMIRAGE: Medical information RAG [10]  \\nMedical QA datasets \\nBiomedical QA \\n2 \\nRAG for improved context accuracy [11]  \\nFinancial reports \\nFinancial QA \\n3 \\nRetrieval-augmented Electrocardiography (ECG) [12]  \\nCardiac symptoms and sleep apnea diagnosis \\nMedical QA \\n4 \\nRepresentative Vector Summarization (RVS) [13]  \\nPDFs, text documents, spreadsheets, etc.  \\nMedical text summarization \\n5 \\nRetrieval-augmented controllable reviews [14] \\nAmazon book reviews \\nBook review generation'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3785\\n6 \\nRetrieval-augmented knowledge graph reasoning [15] \\nCommonsense QA and OpenBookQA. \\nCommonsense QA \\n7 \\nAnswers from table corpus via RAG [16] \\nWikipedia data \\nTable QA  \\n8 \\nLiVersa: a liver disease specific LLM using RAG [17] \\nLiver Diseases  \\nMedical QA \\n9 \\nAlmanac: RAG for clinical medicine [18] \\nGuidelines and treatment recommendations. \\nClinical decision-making \\n10 \\nAssessment of tutoring practices [19] \\nDialogue transcripts from a middle-school.  \\nEducational decision making \\n11 \\nHandling out of domain scenarios [20] \\nLife science, earth science, etc. lessons.  \\nTextbook QA \\n12 \\nAutomated form filling [21] \\nRequest forms for IT projects \\nEnterprise search \\n13 \\nFinancial sentiment analysis [22]  \\nTwitter financial news and FiQA datasets \\nSentiments classification \\n14 \\nFrontline health worker capacity building [23] \\nPregnancy-related guidelines \\nHealth education QA \\n15 \\nSelf-BioRAG: a framework for biomedical text [24] \\nBiomedical instruction sets \\nBiomedical Informatics \\n16 \\nHybrid RAG for real-time composition assistance [25] \\nWikiText-103, Enron Emails, etc.  \\nWriting speed and accuracy \\n17 \\nRAG-Fusion to obtain product information [26] \\nProduct datasheets \\nTechnical information QA \\n18 \\nCommit message generation for code intelligence [27]  \\nMCMD dataset  \\nSDM \\n19 \\nFloodBrain: Flood disaster reporting [28] \\nReliefWeb reports \\nHumanitarian assistance \\n20 \\nRich answer encoding [29] \\nMSMARCO QA and WoW dataset. \\nGenerative QA  \\n21 \\nText-to-image generator [30] \\nCOCO and WikiImages datasets. \\nRealistic images generation \\n22 \\nCode completion framework [31] \\nCodeXGLUE and CodeNet datasets. \\nSDM \\n23 \\nComplex story generation framework [32] \\nIMDB movie details dataset \\nGenerate stories  \\n24 \\nTRAC: Trustworthy retrieval augmented chatbot [33] \\nNatural Question dataset \\nNatural QA \\n25 \\nClinfo.ai using scientific literature [34] \\nPubMed dataset \\nMedical QA \\n26 \\nRealGen for controllable traffic scenarios [35] \\nnuScenes dataset \\nCritical traffic scenarios \\n27 \\nZero-shot disease phenotyping [36]  \\nClinical notes \\nIdentifying diseases \\n28 \\nRAP-Gen for automatic program repair [37]  \\nTFix, Defects4J, etc. datasets \\nSDM \\n29 \\nCode4UIE : retrieval-augmented code generation [38] \\nACE04, ACE05, CoNLL03, etc. datasets \\nInformation extraction  \\n30 \\nRAP: retrieval-augmented planning [39] \\nALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31 \\nRIGHT for mainstream hashtag recommendation [40] \\nTwitter and Weibo data.  \\nRetrieval-enhanced hashtags \\n32 \\nRAUCG for counter narrative generation for hate speech \\n[41] \\nMultitargetCONAN dataset \\nCombating hate speech \\n33 \\nWeakly-supervised scientific document classification \\n[42] \\nAGNews and MeSH datasets.  \\nScientific documents \\nclassification \\n34 \\nrT5 for Chinese entity description generation [43] \\nXunZi and MengZi datasets.  \\nEntity description generation \\n35 \\nRSpell: domain adaptive Chinese spelling check [44]  \\nCSC dataset \\nText error correction \\n36 \\nXRICL: cross-lingual retrieval-augmented in-context \\nlearning for cross-lingual text-to-SQL semantic parsing \\n[45] \\nXSPIDER and XKAGGLE-DBQA datasets. \\nText-to-SQL translation \\n37 \\nSELF-RAG: learning to retrieve, generate, and \\ncritique through self-reflection [46] \\nOpen-Instruct processed data. \\nOpen-domain QA and fact \\nverification \\n38 \\nChatDOC with enhanced PDF structure recognition [47] \\nAcademic papers, financial reports, \\ntextbooks, and legislative materials \\nProfessional knowledge QA \\n39 \\nG-Retriever for textual graph understanding [48] \\nGraphQA (ExplaGraphs, SceneGraphs and \\nWebQSP)  \\nChat with graphs \\n40 \\nEnhancing multilingual information retrieval in \\nmixed Human Resources (HR) environments [49] \\nHR standard operating procedures and \\nQuality Assurance (QA) documents \\nMulticultural enterprise QA \\n41 \\nDifferentiable RAG [50] \\nUser-clicked logs \\nE-commerce search (query \\nintent classification) \\n42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data \\nSDM \\n43 \\nUniMS-RAG: a unified multi-source RAG [52] \\nDuLeMon and KBP datasets \\nPersonalized dialogue \\nsystems \\n44 \\nRAG QA for event argument extraction [53] \\nACE 2005 and WikiEvent datasets \\nEvent argument (answer) \\nextraction \\n45 \\nFABULA: retrieval-augmented narrative construction \\n[54] \\nOntoNotes and Pile datasets \\nIntelligence report \\ngeneration \\n46 \\nTime-Aware Adaptive Retrieval (TA-ARE) [55] \\nRetrievalQA dataset \\nShort-form open-domain \\nQA \\n47 \\nCash transaction booking via RAG [56] \\nCash Management Software (CMS) \\ntransactions. \\nAutomated cash transaction \\nbooking \\n48 \\nRetrieval-Augmented Thought Process (RATP) [57] \\nBoolq and emrQA datasets. \\nQA with private data \\n49 \\nATLANTIC for interdisciplinary science [58] \\nS2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50 \\nWriting documents for clinical trials [59] \\nFDA guidance database, ClinicalTrials.gov, \\nand AACT database.  \\nClinical-related writing \\n51 \\nQA RAG model [60] \\nFDA Q&A datasets  \\nPharma industry regulatory \\ncompliance QA'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 5}, page_content='3786\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n4. Discussion  \\nThe classification of RAG applications according to the specific NLP tasks they target holds significant \\nimportance for several reasons. Firstly, it offers valuable insights into the distribution and focus of RAG applications \\nacross various tasks within the field of NLP. By quantifying the number of studies dedicated to each task, \\nresearchers gain a deeper understanding of where efforts and resources are predominantly concentrated within the \\nRAG domain. By analyzing the distribution of RAG applications, researchers can discern prevailing trends in \\nresearch interest and identify emerging areas of importance. The classification of RAG applications based on \\ndiscipline offers valuable insights into its widespread adoption across various domains. This classification not only \\nprovides a comprehensive understanding of RAG’s applicability but also underscores its potential to revolutionize \\nvarious domains, thereby contributing significantly to the advancement of NLP technologies.  \\nWhile this survey offers a comprehensive overview of RAG applications across various NLP tasks and \\ndisciplines, it also has its limitations. 1) Given that RAG technology is still emerging, the majority of RAG-based \\nstudies are available in pre-print formats on platforms like arXiv, lacking peer review. This raises questions about \\ntheir authenticity. 2) Additionally, the survey overlooks the technical implementation details and challenges \\nassociated with using RAG technology alongside open-source LLMs. Organizations may find RAG implementation \\ncostly if they do not opt for open-source LLM architectures, especially considering the expense of querying the \\nLLM via Application Programming Interface (API). 3) Furthermore, the performance of RAG concerning the \\nvolume and variety of datasets has not been discussed. Deploying RAG with large datasets of varying structures \\n(e.g., structured, semi-structured, or non-structured) may lead to processing delays, warranting further exploration \\nbefore selecting a RAG with LLM integrated solution for organizational deployment.  \\n4) Additionally, this survey did not cover the diverse range of RAG architectures and technologies available for \\nintegration with different LLMs. Future work should delve into these options to discuss how various RAG solutions \\ncan be adapted with LLMs for different NLP tasks and applications. 5) Furthermore, the survey did not address the \\naccuracy of information obtained from RAG with LLM solutions. It is essential to explore the reliability of these \\nsystems and assess the organizations’ dependency on their generated responses. LLMs often generate responses with \\nhigh confidence, making it challenging to evaluate the accuracy of the information provided. 6) While the survey \\nprimarily focuses on task-based and discipline-based applications of RAG, there is a need for further research to \\nexplore ethical considerations associated with its usage, especially when dealing with sensitive datasets. For \\nexample, in the biomedical domain, RAG has the potential to accidentally expose private information to analysts, \\nraising concerns about data privacy and security. Additionally, in the legal domain, RAG may mistakeably reveal \\nprivileged information during document analysis, potentially violating client confidentiality and attorney-client \\nprivilege. Therefore, future studies should delve deeper into these ethical implications to ensure responsible and \\nethical use of RAG technology across various domains. \\n5. Conclusion  \\nThis article offers a thorough examination of the applications of RAG with LLMs, showcasing their potential to \\ndrive digital transformation across diverse industries. Initially, it gathers the latest publications on RAG from online \\nrepositories. These publications are then classified based on task-oriented and discipline-oriented criteria. A notable \\ntrend observed is the increasing number of research papers on RAG deposited in open-access sources, particularly \\nsince 2023. However, many works remain unpublished or are in the preprint stage, awaiting review by various \\njournals. A significant portion of these studies primarily focus on the task of QA in NLP. Conversely, there is a \\nnoticeable gap in research exploring Entity Linking, an essential NLP task that contributes to knowledge graph \\ndevelopment. Addressing this gap could unlock numerous applications in the realm of linked data. Regarding \\ndisciplines, the majority of research applications are concentrated in the fields of Medical/Biomedical and \\nTechnology and Software Development. In contrast, disciplines such as Business and Agriculture receive \\ncomparatively less attention. Future research endeavors should aim to bridge this gap by addressing the specific \\nneeds of these underrepresented disciplines.'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 6}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3787\\nTable 2. Task-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 4. Task-based classification of RAG applications with count of publications. The word cloud is generated based on the publication counts \\nlisted under various headings in Table 2. \\n \\n \\n \\n \\n1) Question Answering (QA)  \\n- Biomedical QA [1] \\n- Financial QA [2] \\n- Medical QA [3] \\n- Commonsense QA [6] \\n- Textbook QA [11] \\n- Health education QA [14] \\n- Technical product information QA [17] \\n- Natural QA [24] \\n- Professional knowledge QA [38] \\n- Multicultural enterprise QA [40] \\n- Open-domain QA and fact verification [46] \\n- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29] \\n- Pharma industry regulatory compliance QA [51] \\n- Science QA and document classification [49] \\n- Clinical-related writing [50] \\n- Personalized dialogue systems [43] \\n2) Text Generation and Summarization \\n- Medical text summarization [4] \\n- Book review generation [5] \\n- Biomedical Informatics [15] \\n- Generate stories with complex plots [23] \\n- Generate realistic and faithful images [21] \\n- Entity description generation [34] \\n \\n3) Information Retrieval and Extraction \\n- Table QA [7] \\n- Enterprise search [12] \\n- Retrieval-enhanced hashtags [31] \\n- Information extraction [29] \\n- Event argument (answer) extraction [44] \\n- E-commerce search (query intent classification) [41] \\n4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Scientific documents classification [33] \\n- Combating online hate speech [32] \\n \\n5) Software Development and \\nMaintenance \\n- Code intelligence [18] \\n- Code completion [22] \\n- Automatic program repair [28] \\n- Elevate low-code developer skills [42] \\n \\n6) Decision Making and Applications \\n- Clinical decision-making [9] \\n- Educational decision making [10] \\n- Decision-making applications [30] \\n- Automated cash transaction booking [47] \\n- Intelligence report generation [45] \\n \\n7) Other Categories: \\n- Editing and crafting diverse behaviors, \\nincluding critical traffic scenarios [26] \\n- Identifying diseases [27] \\n- Chat with graphs [39] \\nTask: (Count of Publications)  \\nQuestion Answering (QA): (20) \\nText Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6) \\nText Analysis and Processing: (5) \\nSoftware Development and Maintenance: (4) \\nDecision Making and Applications: (5) \\nOther Categories: (6)'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 7}, page_content='3788\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nTable 3. Discipline-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 5. Discipline-based classification of RAG applications with count of publications. The word cloud is generated based on the publication \\ncounts listed under various headings in Table 3. \\nAcknowledgements \\nThe authors thank the French Government and the National Research Agency (ANR) for their funding. \\nReferences \\n[1] Roumeliotis KI, Tselikas ND, & Nasiopoulos DK. (2024). “LLMs in e-commerce: a comparative analysis of GPT and LLaMA models in \\nproduct review evaluation,” Natural Language Processing Journal:1-6:100056. \\n[2] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). “Language Models are Few-Shot \\nLearners,” Advances in Neural Information Processing Systems 33 (NeurIPS 2020).  \\n[3] OpenAI, R. (2023). “Gpt-4 technical report,” arxiv 2303.08774. View in Article: 2(5). \\n1) Medical / Biomedical \\n- Biomedical QA [1] \\n- Medical QA [3] \\n- Medical text summarization [4] \\n- Health education QA [14] \\n- Identifying diseases [27] \\n- Clinical decision-making [9] \\n- Clinical-related writing [50] \\n- Science QA and scientific document classification [49] \\n- Pharma industry regulatory compliance QA [51] \\n2) Financial \\n- Financial QA [2] \\n- Automated cash transaction booking [47] \\n3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11] \\n4) Technology and Software Development \\n- Table QA [7] \\n- Technical product information QA [17] \\n- Software development and maintenance [18, 22, 28, 42] \\n- Generative QA and informative conversations [20] \\n- Information extraction [29] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Personalized dialogue systems [43] \\n- Event argument (answer) extraction [44] \\n5) Social and Communication \\n- Commonsense QA [6] \\n- Sentiments classification [13] \\n- Combating online hate speech [32] \\n- Retrieval-enhanced hashtags [31] \\n- Humanitarian assistance [19] \\n- Chat with graphs [39] \\n- Multicultural enterprise QA [40] \\n6) Literature  \\n- Book review generation guided by reference documents [5] \\n- Enhance user writing speed and accuracy [16] \\n- Generate stories with complex plots [23] \\n7) Other Categories   \\n- Enterprise search [12] \\n- Generate realistic and faithful images [21] \\n- Decision-making applications [30] \\n- Open-domain question answering and fact verification [37] \\n- Professional knowledge QA [38] \\n- Intelligence report generation [45] \\n- Short-form open-domain QA [46] \\n- Question answering with private data [48] \\n \\nDiscipline: (Count of Publications)  \\nMedical / Biomedical: (9) \\nFinancial: (2) \\nEducational: (2) \\nTechnology and Software Development: (9) \\nSocial and Communication: (7) \\nLiterature (3) \\nOther Categories: (8)'), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content=\"Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3789\\n[4] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., ... & Wang, H. (2023). “Retrieval-augmented generation for large language models: A \\nsurvey,” arXiv preprint arXiv:2312.10997. \\n[5] Kandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2023). “Large language models struggle to learn long-tail knowledge,” In \\nInternational Conference on Machine Learning. PMLR: 5696-15707. \\n[6] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). “Retrieval-augmented generation for knowledge-\\nintensive nlp tasks,” Advances in Neural Information Processing Systems 33: 9459-9474. \\n[7] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). “A survey on retrieval-augmented text generation,” arXiv preprint arXiv:2202.01110. \\n[8] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., ... & Scialom, T. (2023). “Augmented language models: a \\nsurvey,” arXiv preprint arXiv:2302.07842. \\n[9] Zhao, R., Chen, H., Wang, W., Jiao, F., Do, X. L., Qin, C., ... & Joty, S. (2023). “Retrieving multimodal information for augmented \\ngeneration: A survey,” arXiv preprint arXiv:2303.10868. \\n[10] Xiong, G., Jin, Q., Lu, Z., & Zhang, A. (2024). “Benchmarking retrieval-augmented generation for medicine,” arXiv preprint \\narXiv:2402.13178. \\n[11] Jimeno Yepes, A., You, Y., Milczek, J., Laverde, S., & Li, L. (2024). “Financial Report Chunking for Effective Retrieval Augmented \\nGeneration,” arXiv e-prints, arXiv-2402. \\n[12] Yu, H., Guo, P., & Sano, A. (2023). “Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation,” \\nIn Machine Learning for Health (ML4H) PMLR: 650-663. \\n[13] Manathunga, S. S., & Illangasekara, Y. A. (2023). “Retrieval Augmented Generation and Representative Vector Summarization for large \\nunstructured textual data in Medical Education,” arXiv preprint arXiv:2308.00479. \\n[14] Kim, J., Choi, S., Amplayo, R. K., & Hwang, S. W. (2020). “Retrieval-augmented controllable review generation,” In Proceedings of the \\n28th International Conference on Computational Linguistics: 2284-2295. \\n[15] Sha, Y., Feng, Y., He, M., Liu, S., & Ji, Y. (2023). “Retrieval-augmented Knowledge Graph Reasoning for Commonsense Question \\nAnswering,” Mathematics 11(15): 3269; https://doi.org/10.3390/math11153269.  \\n[16] Pan, F., Canim, M., Glass, M., Gliozzo, A., & Hendler, J. (2022). “End-to-End Table Question Answering via Retrieval-Augmented \\nGeneration,” arXiv preprint arXiv:2203.16714. \\n[17] Ge, J., Sun, S., Owens, J., Galvez, V., Gologorskaya, O., Lai, J. C., ... & Lai, K. (2023). “Development of a Liver Disease-Specific Large \\nLanguage Model Chat Interface using Retrieval Augmented Generation,” medRxiv. \\n[18] Zakka, C., Shad, R., Chaurasia, A., Dalal, A. R., Kim, J. L., Moor, M., ... & Hiesinger, W. (2024). “Almanac—retrieval-augmented language \\nmodels for clinical medicine,” NEJM AI 1(2), AIoa2300068. \\n[19] Han, Z. FeiFei, Lin, J., Gurung, A., Thomas, D. R., Chen, E., Borchers, C., Gupta, S., & Koedinger, K. R. (2024). “Improving Assessment of \\nTutoring Practices using Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.14594. \\n[20] Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A., & Jamal, A. (2024). “Enhancing Textbook Question Answering Task with \\nLarge Language Models and Retrieval Augmented Generation,” arXiv preprint arXiv:2402.05128. \\n[21] Bucur, M. (2023). “Exploring Large Language Models and Retrieval Augmented Generation for Automated Form Filling,” (Bachelor's \\nthesis, University of Twente). \\n[22] Zhang, B., Yang, H., Zhou, T., Ali Babar, M., & Liu, X. Y. (2023). “Enhancing financial sentiment analysis via retrieval augmented large \\nlanguage models,” In Proceedings of the Fourth ACM International Conference on AI in Finance: 349-356. \\n[23] Al Ghadban, Y., Lu, H. Y., Adavi, U., Sharma, A., Gara, S., Das, N., ... & Hirst, J. E. (2023). “Transforming healthcare education: \\nHarnessing large language models for frontline health worker capacity building using retrieval-augmented generation,” medRxiv, 2023-12. \\n[24] Jeong, M., Sohn, J., Sung, M., & Kang, J. (2024). “Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-\\nAugmented Large Language Models,” arXiv preprint arXiv:2401.15269. \\n[25] Xia, M., Zhang, X., Couturier, C., Zheng, G., Rajmohan, S., & Ruhle, V. (2023). “Hybrid retrieval-augmented generation for real-time \\ncomposition assistance,” arXiv preprint arXiv:2308.04215. \\n[26] Rackauckas, Z. (2024). “RAG-Fusion: A New Take on Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.03367. \\n[27] Shi, E., Wang, Y., Tao, W., Du, L., Zhang, H., Han, S., ... & Sun, H. (2022). “RACE: Retrieval-Augmented Commit Message \\nGeneration,” arXiv preprint arXiv:2203.02700. \\n[28] Colverd, G., Darm, P., Silverberg, L., & Kasmanoff, N. (2023). “FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented \\nGeneration with an LLM,” arXiv preprint arXiv:2311.02597. \\n[29] Huang, W., Lapata, M., Vougiouklis, P., Papasarantopoulos, N., & Pan, J. (2023). “Retrieval Augmented Generation with Rich Answer \\nEncoding,” In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-\\nPacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers): 1012-1025. \\n[30] Chen, W., Hu, H., Saharia, C., & Cohen, W. W. (2022). “Re-imagen: Retrieval-augmented text-to-image generator,” arXiv preprint \\narXiv:2209.14491. \\n[31] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S. W., & Svyatkovskiy, A. (2022). “Reacc: A retrieval-augmented code completion \\nframework,” arXiv preprint arXiv:2203.07722. \\n[32] Wen, Z., Tian, Z., Wu, W., Yang, Y., Shi, Y., Huang, Z., & Li, D. (2023). “Grove: a retrieval-augmented complex story generation \\nframework with a forest of evidence,” arXiv preprint arXiv:2310.05388.\"), Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='3790\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n[33] Li, S., Park, S., Lee, I., & Bastani, O. (2023). “TRAC: Trustworthy Retrieval Augmented Chatbot,” arXiv preprint arXiv:2307.04642. \\n[34] Lozano, A., Fleming, S. L., Chiang, C. C., & Shah, N. (2023). “Clinfo. ai: An open-source retrieval-augmented large language model system \\nfor answering medical questions using scientific literature,” In Pacific symposium on Biocomputing 2024: 8-23. \\n[35] Ding, W., Cao, Y., Zhao, D., Xiao, C., & Pavone, M. (2023). “RealGen: Retrieval Augmented Generation for Controllable Traffic \\nScenarios,” arXiv preprint arXiv:2312.13303. \\n[36] Thompson, W. E., Vidmar, D. M., De Freitas, J. K., Pfeifer, J. M., Fornwalt, B. K., Chen, R., ... & Miotto, R. (2023). “Large Language \\nModels with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,” arXiv preprint arXiv:2312.06457. \\n[37] Wang, W., Wang, Y., Joty, S., & Hoi, S. C. (2023). “Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program \\nrepair,” In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software \\nEngineering: 146-158. \\n[38] Guo, Y., Li, Z., Jin, X., Liu, Y., Zeng, Y., Liu, W., ... & Cheng, X. (2023). “Retrieval-augmented code generation for universal information \\nextraction,” arXiv preprint arXiv:2311.02962. \\n[39] Kagaya, T., Yuan, T. J., Lou, Y., Karlekar, J., Pranata, S., Kinose, A., ... & You, Y. (2024). “RAP: Retrieval-Augmented Planning with \\nContextual Memory for Multimodal LLM Agents,” arXiv preprint arXiv:2402.03610. \\n[40] Fan, R. Z., Fan, Y., Chen, J., Guo, J., Zhang, R., & Cheng, X. (2023). “RIGHT: Retrieval-augmented Generation for Mainstream Hashtag \\nRecommendation,” arXiv preprint arXiv:2312.10466. \\n[41] Jiang, S., Tang, W., Chen, X., Tanga, R., Wang, H., & Wang, W. (2023). Raucg: Retrieval-augmented unsupervised counter narrative \\ngeneration for hate speech. arXiv preprint arXiv:2310.05650. \\n[42] Xu, R., Yu, Y., Ho, J., & Yang, C. (2023). “Weakly-supervised scientific document classification via retrieval-augmented multi-stage \\ntraining,” In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval: 2501-\\n2505. \\n[43] Hu, M., Zhao, X., Wei, J., Wu, J., Sun, X., Li, Z., ... & Zhang, Y. (2023). “rT5: A Retrieval-Augmented Pre-trained Model for Ancient \\nChinese Entity Description Generation,” In International Conference on NLP and Chinese Computing. Cham: Springer: 736-748. \\n[44] Song, S., Lv, Q., Geng, L., Cao, Z., & Fu, G. (2023). “RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling \\nCheck,” In CCF International Conference on Natural Language Processing and Chinese Computing. Cham: Springer: 551-562.  \\n[45] Shi, P., Zhang, R., Bai, H., & Lin, J. (2022). “Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql \\nsemantic parsing,” arXiv preprint arXiv:2210.13693. \\n[46] Asai, A., Wu, Z., Wang, Y., Sil, A., & Hajishirzi, H. (2023). “Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection,” arXiv preprint arXiv:2310.11511. \\n[47] Lin, D. (2024). “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,” arXiv preprint \\narXiv:2401.12599. \\n[48] He, X., Tian, Y., Sun, Y., Chawla, N. V., Laurent, T., LeCun, Y., ... & Hooi, B. (2024). “G-Retriever: Retrieval-Augmented Generation for \\nTextual Graph Understanding and Question Answering,” arXiv preprint arXiv:2402.07630. \\n[49] Ahmad, S. R. (2024). “Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model \\nImplementation for Multicultural Enterprise,” arXiv preprint arXiv:2401.01511. \\n[50] Zhao, C., Jiang, Y., Qiu, Y., Zhang, H., & Yang, W. Y. (2023). “Differentiable Retrieval Augmentation via Generative Language Modeling \\nfor E-commerce Query Intent Classification,” In Proceedings of the 32nd ACM International Conference on Information and Knowledge \\nManagement: 4445-4449. \\n[51] Nakhod, o. Using retrieval-augmented generation to elevate low-code developer skills. https://doi.org/10.15407/jai2023.03.126 \\n[52] Wang, H., Huang, W., Deng, Y., Wang, R., Wang, Z., Wang, Y., ... & Wong, K. F. (2024). “UniMS-RAG: A Unified Multi-source \\nRetrieval-Augmented Generation for Personalized Dialogue Systems,” arXiv preprint arXiv:2401.13256. \\n[53] Du, X., & Ji, H. (2022). “Retrieval-augmented generative question answering for event argument extraction,” arXiv preprint \\narXiv:2211.07067. \\n[54] Ranade, P., & Joshi, A. (2023). “FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction,” arXiv \\npreprint arXiv:2310.13848. \\n[55] Zhang, Z., Fang, M., & Chen, L. (2024). “RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain \\nQuestion Answering,” arXiv preprint arXiv:2402.16457. \\n[56] Zhang, S., Yadav, D., & Jin, T. (2023). “Cash transaction booking via retrieval augmented LLM. KDD 2023 Workshop on Robust NLP for \\nFinance (RobustFin),” https://www.amazon.science/publications/cash-transaction-booking-via-retrieval-augmented-llm \\n[57] Pouplin, T., Sun, H., Holt, S., & Van der Schaar, M. (2024). “Retrieval-Augmented Thought Process as Sequential Decision Making,” arXiv \\npreprint arXiv:2402.07812. \\n[58] Munikoti, S., Acharya, A., Wagle, S., & Horawalavithana, S. (2023). “ATLANTIC: Structure-Aware Retrieval-Augmented Language Model \\nfor Interdisciplinary Science,” arXiv preprint arXiv:2311.12289. \\n[59] Markey, N., El-Mansouri, I., Rensonnet, G., van Langen, C., & Meier, C. (2024). “From RAGs to riches: Using large language models to \\nwrite documents for clinical trials,” arXiv preprint arXiv:2402.16406. \\n[60] Kim, J., & Min, M. (2024). “From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process,” arXiv \\npreprint arXiv:2402.01717.'), Document(metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'source': '../data/pdf/sample.pdf', 'file_path': '../data/pdf/sample.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'sample', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20080701052447Z00'00'\", 'trapped': '', 'modDate': \"D:20080701052447Z00'00'\", 'creationDate': \"D:20080701052447Z00'00'\", 'page': 0}, page_content='Sample PDF\\nThis is a simple PDF ﬁle. Fun fun fun.\\nLorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. \\nCurabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget \\npharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. \\nInteger a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. \\nVestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla \\nerat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. \\nVivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique \\naccumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, \\nhendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. \\nSuspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \\npulvinar quis, nisl.\\nPellentesque sit amet lectus. Praesent pulvinar, nunc quis iaculis sagittis, justo quam \\nlobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. \\nPhasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh \\nante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis \\nsem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin \\nvelit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, \\nfeugiat eu, gravida eu, consequat molestie, ipsum. Nullam vel est ut ipsum volutpat \\nfeugiat. Aenean pellentesque.\\nIn mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, \\nscelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat \\nvarius. Donec lacinia, neque a luctus aliquet, pede massa imperdiet ante, at varius lorem \\npede sed sapien. Fusce erat nibh, aliquet in, eleifend eget, commodo eget, erat. Fusce \\nconsectetuer. Cras risus tortor, porttitor nec, tristique sed, convallis semper, eros. Fusce \\nvulputate ipsum a mauris. Phasellus mollis. Curabitur sed urna. Aliquam nec sapien non \\nnibh pulvinar convallis. Vivamus facilisis augue quis quam. Proin cursus aliquet metus. \\nSuspendisse lacinia. Nulla at tellus ac turpis eleifend scelerisque. Maecenas a pede vitae \\nenim commodo interdum. Donec odio. Sed sollicitudin dui vitae justo.\\nMorbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus \\nmolestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque \\npenatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit \\namet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa \\neget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, \\nquam.'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Towards Agentic RAG with Deep Reasoning:\\nA Survey of RAG-Reasoning Systems in LLMs\\nYangning Li1*, Weizhi Zhang2*, Yuyao Yang2, Wei-Chieh Huang2, Yaozu Wu3\\nJunyu Luo4, Yuanchen Bei5, Henry Peng Zou2, Xiao Luo6, Yusheng Zhao4\\nChunkit Chan7, Yankai Chen2, Zhongfen Deng2, Yinghui Li1, Hai-Tao Zheng1,\\nDongyuan Li3, Renhe Jiang3, Ming Zhang4, Yangqiu Song7, Philip S. Yu1\\n1Tsinghua University 2University of Illinois Chicago 3The University of Tokyo\\n4Peking University 5University of Illinois Urbana-Champaign\\n6University of California, Los Angeles 7HKUST\\nynli23@mails.tsinghua.edu.cn, wzhan42@uic.edu\\nAbstract\\nRetrieval-Augmented Generation (RAG) lifts\\nthe factuality of Large Language Models\\n(LLMs) by injecting external knowledge, yet\\nit falls short on problems that demand multi-\\nstep inference; conversely, purely reasoning-\\noriented approaches often hallucinate or mis-\\nground facts. This survey synthesizes both\\nstrands under a unified reasoning-retrieval per-\\nspective. We first map how advanced reason-\\ning optimizes each stage of RAG (Reasoning-\\nEnhanced RAG). Then, we show how re-\\ntrieved knowledge of different type supply\\nmissing premises and expand context for\\ncomplex inference (RAG-Enhanced Reason-\\ning).\\nFinally, we spotlight emerging Syn-\\nergized RAG-Reasoning frameworks, where\\n(agentic) LLMs iteratively interleave search\\nand reasoning to achieve state-of-the-art per-\\nformance across knowledge-intensive bench-\\nmarks.\\nWe categorize methods, datasets,\\nand open challenges, and outline research av-\\nenues toward deeper RAG-Reasoning systems\\nthat are more effective, multimodally-adaptive,\\ntrustworthy, and human-centric. The collec-\\ntion is available at https://github.com/\\nDavidZWZ/Awesome-RAG-Reasoning.\\n1\\nIntroduction\\nThe remarkable progress in Large Language Mod-\\nels (LLMs) has transformed a wide array of fields,\\nshowcasing unprecedented capabilities across di-\\nverse tasks (Zhao et al., 2023). Despite these ad-\\nvancements, the effectiveness of LLMs remains\\nhindered by two fundamental limitations: knowl-\\nedge hallucinations, due to the static and parametric\\nmanner of their knowledge storage (Huang et al.,\\n2025b); and struggles with complex reasoning, es-\\npecially when tackling real-world problems (Chang\\net al., 2024). These limitations have driven the\\n* Equal Contribution.\\ndevelopment of two major directions: Retrieval-\\nAugmented Generation (RAG) (Fan et al., 2024a),\\nwhich provides LLMs with external knowledge;\\nand various methods aimed at enhancing their in-\\nherent reasoning abilities (Chen et al., 2025c).\\nThe two limitations are inherently intertwined:\\nmissing knowledge can impede reasoning, and\\nflawed reasoning hinders knowledge utilization\\n(Tonmoy et al., 2024). Naturally, researchers have\\nincreasingly explored combining retrieval with rea-\\nsoning, though early work followed two separate,\\none-way enhancements.\\nThe first, Reasoning-\\nenhanced RAG (Gao et al., 2023b) (Reasoning\\n→RAG), leverages reasoning to improve specific\\nstages of the RAG pipeline. The second path, RAG-\\nenhanced Reasoning (Fan et al., 2024a) (RAG →\\nReasoning), supplies external factual grounding or\\ncontextual cues to bolster LLM reasoning.\\nWhile beneficial, the above methods remain\\nbound to a static Retrieval-Then-Reasoning (RTR)\\nframework, offering only localized improvements\\nto individual components. Several inherent limi-\\ntations persist: (1) Retrieval Adequacy and Accu-\\nracy cannot be guaranteed; Pre-retrieved knowl-\\nedge may fail to align with the actual knowledge\\nneeds that emerge during reasoning, especially in\\ncomplex tasks (Zheng et al., 2025; Li et al., 2025d).\\n(2) Reasoning Depth remains constrained. When\\nretrieved knowledge contains errors or conflicts,\\nit can adversely interfere with the model’s inher-\\nent reasoning capabilities (Li et al., 2025b; Chen\\net al., 2025a). (3) System Adaptability proves in-\\nsufficient. The RTR framework lacks mechanisms\\nfor iterative feedback or dynamic retrieval during\\nreasoning. This rigidity limits its effectiveness in\\nscenarios that require adaptive reasoning, such as\\nopen-domain QA or scientific discovery (Xiong\\net al., 2025; Alzubi et al., 2025).\\nAs shown in Figure 1, these shortcomings have\\ncatalyzed a paradigm shift toward Synergized Re-\\n1\\narXiv:2507.09477v2  [cs.CL]  16 Jul 2025'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Query\\nKnowledge\\nSource\\nDense/Sparse Retriever\\nRetrieval\\nIntegration\\nRe-rank \\n& Filter\\nData Fusion\\nGeneration\\nRetrieval Optimization\\n§3 Reasoning Enhanced RAG (Reasoning → RAG) \\nReasoning Enhanced\\n§4 RAG Enhanced Reasoning (RAG → Reasoning) \\nKnowledge Base\\nWeb Retrieval\\nTool Using\\nPrior Experience\\nExample & Training Data\\n§5 Synergized RAG and Reasoning (RAG ⇔ Reasoning) \\n§5.1 Reasoning Workflow\\n(a) Chain Based\\n(b) Tree Based\\n(c) Graph Based\\n§5.2 Agent Orchestration\\n(a) Single Agent\\nRAG Pipeline\\nLLM Reasoning\\nKnowledge 1: he is a South African \\npilot, sailor, consultant, who partly …\\nLet‘s Think Step by Step! …….\\nFind the solution!\\nRAG Enhanced\\n🎯Target: Accurate, and \\nReasoning-aware Retrieval\\n• Retrieve info tailored for \\nreasoning\\n• Filter and fuse evidence \\nlogically\\n• Faithful, logic-grounded output\\n🎯Target: Deeper, and \\nGrounded Reasoning\\n• Grounded by external \\nknowledge\\n• Verified by tools and evidence\\n• Guided by memory or \\nexamples\\n☹Drawback: \\nIrrelevant Knowledge \\ndisrupts reasoning accuracy.\\nQuery\\n❌\\nOutput\\n…\\nAction\\nObservation\\nAgent\\nRAG\\n(b) Decentralized \\nMulti Agent\\n(c) Centralized \\nMulti Agent\\nReasoning\\nRAG\\nQuery\\n❌\\n❓\\n✅\\nIntegration Enhancement\\nGeneration\\nEnhancement\\n☹Drawback: \\nShallow Reasoning leads to \\ninaccurate retrieval.\\nAgent 1\\nAgent 2\\nAgent 3\\nAgent 4\\nRAG\\nManager\\nAgent\\nWork Agent 1\\nWork Agent 2\\nWork Agent 3\\nRAG\\nQuery\\nOutput\\n…\\nFigure 1: Overview of the RAG-Reasoning System. The Reasoning-Enhanced RAG methods and RAG-Enhanced\\nReasoning methods represent one-way enhancements. In contrast, the Synergized RAG-Reasoning System performs\\nreasoning and retrieval iteratively, enabling mutual enhancements.\\ntrieval and Reasoning within LLMs (RAG ⇔\\nReasoning). These methods support a dynamic,\\niterative interplay where reasoning actively guides\\nretrieval, and newly retrieved knowledge, in turn,\\ncontinuously refines the reasoning process. This\\ntrend is further exemplified by recent ”Deep Re-\\nsearch” products from OpenAI1, Gemini2, Perplex-\\nity3, and others, which emphasize tightly coupled\\nretrieval and reasoning (Zhang et al., 2025f). These\\nsystems employ agentic capabilities to orchestrate\\nmulti-step web search and leverage reasoning to\\ncomprehensively interpret retrieved content, solv-\\ning problems demanding in-depth investigation.\\nThis survey charts the shift from isolated en-\\nhancements to cutting-edge synergized frameworks\\nwhere retrieval and reasoning are deeply interwo-\\nven and co-evolve. While surveys on RAG (Fan\\net al., 2024a; Gao et al., 2023b) and LLM Reason-\\ning (Chen et al., 2025c; Li et al., 2025e) exist, a\\ndedicated synthesis focusing on their integration\\nremains lacking. Our goal is to provide a compre-\\nhensive overview of how the symbiosis between\\nretrieval and reasoning is advancing LLM capabili-\\nties, with particular emphasis on the move towards\\na synergized RAG and Reasoning framework.\\nThe survey is structured as follows: Section 2\\nintroduces the background; Section 3 and 4 review\\ntwo one-way enhancements, respectively. Section 5\\n1https://openai.com/index/\\nintroducing-deep-research/\\n2https://gemini.google/overview/\\ndeep-research/\\n3https://www.perplexity.ai/hub/blog/\\nintroducing-perplexity-deep-research\\nunifies both lines into synergized RAG–Reasoning\\nframeworks. Section 6 lists benchmarks, and Sec-\\ntion 7 outlines open challenges.\\n2\\nBackground and Preliminary\\nRAG mitigates knowledge cut-off of LLMs through\\nthree sequential stages: (i) Retrieval, fetching task-\\nrelevant content from external knowledge stores;\\n(ii) Integration, deduplicating, resolving conflicts,\\nand re-ranking the retrieved content; and (iii) Gen-\\neration, reasoning over the curated context to pro-\\nduce the final answer.\\nConcurrently, Chain-of-\\nThought technique has significantly enhanced the\\nreasoning capabilities of modern LLMs by encour-\\naging them to “think step by step” before answering.\\nThe synergy between the structured RAG pipeline\\nand these multi-step reasoning capacities grounds\\nthe emerging RAG-Reasoning paradigm explored\\nin this survey.\\n3\\nReasoning-Enhanced RAG\\nTraditional RAG methods first retrieve relevant doc-\\numents, then concatenate the retrieved knowledge\\nwith the original query to generate the final answer.\\nThese methods often fail to capture the deeper con-\\ntext or intricate relationships necessary for complex\\nreasoning tasks. By integrating reasoning capabili-\\nties across Retrieval, Integration, and Generation\\nstages of the RAG pipeline, the system can identify\\nand fetch the most relevant information, reducing\\nhallucinations and improving response accuracy.4\\n4If reasoning only serves to better leverage fixed retrieved\\nknowledge in a unidirectional manner, it is considered within\\n2'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='A survey of RAG and Reasoning\\nReasoning-enhanced\\nRAG (§3)\\nRetrieval\\nOptimization (§3.1)\\nReasoning-Aware Query Reformulation (§3.1.1)\\ne.g. Collab-RAG (Xu et al., 2025b), DynQR (Anonymous, 2025), DeepRetrieval (Jiang et al., 2025)\\nRetrieval Strategy and Planning (§3.1.2)\\ne.g. PAR-RAG (Zhang et al., 2025d), LPKG (Wang et al., 2024b), FIND (Jia et al., 2025)\\nRetrieval Model Enhancement (§3.1.3)\\ne.g. GNN-RAG (Mavromatis and Karypis, 2024), RuleRAG (Chen et al., 2024c),\\nIntegration\\nEnhancement (§3.2)\\nRelevance Assessment & Filtering (§3.2.1)\\ne.g. SEER (Zhao et al., 2024c), M-RAG-R (Yoran et al., 2024)\\nInformation Synthesis & Fusion (§3.2.2)\\ne.g. BeamAggR (Chu et al., 2024), DualRAG (Cheng et al., 2025) , CRP-RAG (Xu et al., 2024)\\nGeneration\\nEnhancement (§3.3)\\nContext-Aware Generation (§3.3.1)\\ne.g. Open-RAG (Islam et al., 2024), RARE (Wang et al., 2025d), Self-Reasoning (Xia et al., 2025b)\\nGrounded Generation Control (§3.3.2)\\ne.g. RARR (Gao et al., 2023a), TRACE (Fang et al., 2024), AlignRAG (Wei et al., 2025b)\\nRAG-enhanced\\nReasoning (§4)\\nExternal Knowledge\\nRetrieval (§4.1)\\nKnowledge Base (§4.1.1)\\ne.g. Premise-Retrieval (Tao et al., 2025), ReaRAG (Lee et al., 2025), CBR-RAG (Wiratunga et al., 2024)\\nWeb Retrieval (§4.1.2)\\ne.g. ALR2 (Li et al., 2024d) , RARE (Tran et al., 2024), Open-RAG (Islam et al., 2024)\\nTool Using (§4.1.3)\\ne.g. TATU (Li et al., 2024g), TRICE(Qiao et al., 2024), Re-Invoke (Chen et al., 2024a)\\nIn-Context\\nRetrieval (§4.2)\\nPrior Experience (§4.2.1)\\ne.g. RAP (Kagaya et al., 2024), JARVIS-1 (Wang et al., 2024f), EM-LLM (Fountas et al., 2024)\\nExample or Training Data (§4.2.2)\\ne.g. MoD (Wang et al., 2024c), RE4 (Li et al., 2024c), UPRISE (Cheng et al., 2023)\\nSynergized RAG-\\nReasoning (§5)\\nReasoning Workflow\\n(§5.1)\\nChain-based (§5.1.1)\\ne.g. IRCoT (Trivedi et al., 2023), Rat (Wang et al., 2024g), CoV-RAG (He et al., 2024a), RAFT (Zhang et al., 2024a)\\nTree-based\\n(§5.1.2)\\nToT e.g. RATT (Zhang et al., 2025a), Tree of Clarifications (Kim et al., 2023), GROVE (Wen et al., 2023),\\nMCTS e.g. AirRAG (Feng et al., 2025), MCTS-RAG (Hu et al., 2025), SeRTS (Hu et al., 2024)\\nGraph-based\\n(§5.1.3)\\nWalk-on-Graph: e.g. QA-GNN (Yasunaga et al., 2021), LightRAG (Guo et al., 2024), StructRAG (Li et al., 2024h)\\nThink-on-Graph: e.g. ToG (Sun et al., 2024b), ToG-2.0 (Ma et al., 2024a), Graph-CoT (Jin et al., 2024),\\nAgent Orchestration\\n(§5.2)\\nSignle-Agent\\n(§ 5.2.1)\\nPrompting: e.g. ReAct (Yao et al., 2023b), Search-O1 (Li et al., 2025b); SFT: e.g. Toolformer (Schick et al., 2023),\\nINTERS (Zhu et al., 2024); RL: e.g. Search-R1 (Jin et al., 2025) R1-Searcher (Song et al., 2025)\\nMulti-Agent\\n(§ 5.2.2)\\nDecentralized: e.g. M-RAG (Wang et al., 2024e), MDocAgent (Han et al., 2025), Agentic reasoning (Wu et al., 2025c)\\nCentralized: e.g. HM-RAG (Liu et al., 2025), SurgRAW (Low et al., 2025), Chain of Agents (Zhang et al., 2024c)\\nFigure 2: Taxonomy of Recent Advances in RAG-Reasoning System.\\n3.1\\nRetrieval Optimization\\nRetrieval optimization leverages reasoning to im-\\nprove result relevance and quality. Existing meth-\\nods are broadly categorized (1) Reasoning-Aware\\nQuery Reformulation, (2) Retrieval Strategy and\\nPlanning, and (3) Retrieval Model Enhancement.\\n3.1.1\\nReasoning-Aware Query Reformulation\\nIt reformulates the original query to better retrieve\\nreasoning-relevant context. First, query decompo-\\nsition breaks down complex queries into simpler\\nsub-queries (Xu et al., 2025b). Second, query re-\\nformulation recasts ambiguous queries into more\\nclear ones.\\nTo align with reasoning needs of\\ngenerator, certain works train rewrites with RL\\nsignals (Anonymous, 2025; Wang et al., 2025c).\\nThird, query expansion enrich the semantic rich-\\nness of the query via CoT reasoning (Dhuliawala\\net al., 2024; Li et al., 2024e; Lee et al., 2024).\\n3.1.2\\nRetrieval Strategy and Planning\\nThis section covers global retrieval guidance. Ad-\\nvance planning uses a reasoning model to gener-\\nate a complete retrieval blueprint prior to execu-\\ntion. PAR-RAG (Zhang et al., 2025d) applies CoT\\nfor multi-step planning, mitigating local optima.\\nLPKG (Wang et al., 2024b) fine-tunes LLMs on\\nknowledge graphs to encode relational structure. In\\ncontrast, adaptive retrieval decision methods make\\na one-step prediction on whether and how to re-\\ntrieve. FIND (Jia et al., 2025) and adaptive RAG\\n§3.3. In contrast, if reasoning dynamically triggers new\\nretrieval, it is discussed in §5.\\n(Jeong et al., 2024) use classifiers to assess query\\ncomplexity and select retrieval strategies, reducing\\nunnecessary calls. Marina et al. (2025) further adds\\nfeatures like entity popularity and question type.\\n3.1.3\\nRetrieval Model Enhancement\\nA line of work enhances retrievers with reason-\\ning via two strategies.\\nThe first one leverages\\nstructured knowledge: GNN-RAG (Mavromatis\\nand Karypis, 2024) encodes knowledge graphs\\nwith GNNs for implicit multi-hop reasoning, while\\nRuleRAG (Chen et al., 2024c) appends symbolic\\nrules to guide retrieval toward logical consistency.\\nAnother strategy integrates explicit reasoning: Ji\\net al. (2024) combines CoT with the query to im-\\nprove intermediate knowledge recall in multi-hop\\nQA.\\n3.2\\nIntegration Enhancement\\nIntegration enhancement uses reasoning to assess\\nrelevance and merge heterogeneous evidence, pre-\\nventing irrelevant content from disrupting gener-\\nation. Methods fall into two categories: (1) rele-\\nvance assessment and (2) information synthesis.\\n3.2.1\\nRelevance Assessment & Filtering\\nThese methods assess the relevance of each re-\\ntrieved fragment to the user query through deeper\\nreasoning. SEER (Zhao et al., 2024c) employs\\nassessor experts to select faithful, helpful, and con-\\ncise evidence while discarding irrelevant content.\\nYoran et al. (2024) improves robustness by filtering\\nnon-entailing passages using an NLI model, then\\n3'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='fine-tuning the LLM on mixed relevant/irrelevant\\ncontexts to help it ignore residual noise.\\n3.2.2\\nInformation Synthesis & Fusion\\nOnce relevant snippets are identified, the challenge\\nis to fuse them into a coherent evidence set. Beam-\\nAggR (Chu et al., 2024) enumerates sub-question\\nanswer combinations and aggregates them via prob-\\nabilistic reasoning. DualRAG (Cheng et al., 2025)\\ncombines reasoning-augmented querying with pro-\\ngressive knowledge aggregation to filter and or-\\nganize retrieved information into an evolving out-\\nline. CRP-RAG (Xu et al., 2024) builds a rea-\\nsoning graph to retrieve, evaluate, and aggregate\\nknowledge at each node, dynamically selecting\\nknowledge-sufficiency paths before generation.\\n3.3\\nGeneration Enhancement\\nEven with retrieved context, traditional RAG may\\nstill generate unfaithful content without reasoning.\\nReasoning during generation addresses this issue\\nthrough two main approaches: (1) context-aware\\nsynthesis and (2) grounded generation control.\\n3.3.1\\nContext-Aware Synthesis Strategies\\nContext-aware generation ensures outputs remain\\nrelevance while reducing noise. Selective–context\\nutilization prunes or re-weights content based on\\ntask relevance. Open-RAG (Islam et al., 2024)\\nuses a sparse expert mixture to dynamically select\\nknowledge modules, while RARE (Wang et al.,\\n2025d) adds domain knowledge to prompts to pro-\\nmote reliance on external context over memoriza-\\ntion. Reasoning path generation builds explicit log-\\nical chains to enhance transparency, e.g., Ranaldi\\net al. (2024) generate contrasting explanations by\\ncomparing paragraph relevance step-by-step, guid-\\ning the model toward accurate conclusions. Self-\\nReasoning (Xia et al., 2025b) constructs structured\\nreasoning chains through sequential evidence se-\\nlection and verification.\\n3.3.2\\nGrounded Generation Control\\nGrounded generation control introduces verifica-\\ntion mechanisms to ensure outputs remain an-\\nchored to retrieved evidence through reasoning.\\nFact verification methods use reasoning to assess\\nfactual consistency between generated content and\\nretrieved evidence, e.g., Self-RAG (Asai et al.,\\n2023) introduces reflection markers during decod-\\ning to trigger critical review and correction. Cita-\\ntion generation links generated content to source\\nmaterials to enhance traceability and credibility, as\\nin RARR (Gao et al., 2023a), which inserts cita-\\ntions while preserving stylistic coherence. Faith-\\nful reasoning ensures that each reasoning step ad-\\nheres to retrieved evidence without introducing\\nunverified content. TRACE (Fang et al., 2024)\\nbuilds knowledge graphs to form coherent evidence\\nchains, while AlignRAG (Wei et al., 2025b) applies\\ncriticism alignment to refine reasoning paths.\\n4\\nRAG-Enhanced Reasoning\\nIntegrating external knowledge or in-context\\nknowledge during reasoning can help LLMs reduce\\nhallucinations and bridge logical gaps. External re-\\ntrieval leverages structured sources like databases\\nor web content, providing factual grounding, like\\nIAG (Zhang et al., 2023). In-context retrieval uti-\\nlizes internal contexts like prior interactions or\\ntraining examples, enhancing contextual coherence,\\nlike RA-DT (Schmied et al., 2024). Both strategies\\ncollectively improve factual accuracy, interpretabil-\\nity, and logical consistency of reasoning processes.\\n4.1\\nExternal Knowledge Retrieval\\nExternal knowledge retrieval incorporates web\\ncontent, database information, or external tools\\ninto reasoning, effectively filling knowledge gaps.\\nTargeted retrieval improves factual accuracy, en-\\nabling language models to reliably address complex\\nqueries by grounding reasoning steps in verified\\nexternal evidence.\\n4.1.1\\nKnowledge Base\\nKnowledge base (KB) typically stores arithmetic,\\ncommonsense, or logical knowledge in databases,\\nbooks, or documents, with retrieval approaches\\nvarying by task. For question answering (QA) rea-\\nsoning, AlignRAG (Wei et al., 2025b), MultiHop-\\nRAG (Tang and Yang, 2024), and CRP-RAG (Xu\\net al., 2025a) retrieve interconnected factual entries\\nfrom general KBs to enhance sequential reasoning.\\nIn specialized reasoning tasks, mathematical ap-\\nproaches like Premise-Retrieval (Tao et al., 2025)\\nand ReaRAG (Lee et al., 2025) utilize formal lem-\\nmas from theorem libraries for structured deduc-\\ntion; legal approaches like CASEGPT (Yang, 2024)\\nand CBR-RAG (Wiratunga et al., 2024) extract\\njudicial precedents for analogical reasoning. For\\ncode generation tasks, CodeRAG (Li et al., 2025a)\\nand Koziolek et al. (2024) access code snippets\\nfrom repositories, ensuring syntactic correctness.\\n4'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='4.1.2\\nWeb Retrieval\\nWeb retrieval accesses dynamic online content like\\nweb pages, news or social media. Specifically,\\nin fact-checking tasks, approaches such as Ver-\\naCT Scan (Niu et al., 2024), Ragar (Khaliq et al.,\\n2024), PACAR (Zhao et al., 2024b), and STEEL\\n(Li et al., 2024b) verify claims step-by-step using\\nevidence from news or social media, enhancing log-\\nical reasoning. Meanwhile, QA-based reasoning\\nlike RARE (Tran et al., 2024), RAG-Star (Jiang\\net al., 2024), MindSearch (Chen et al., 2024b),\\nand OPEN-RAG (Islam et al., 2024) iteratively\\nrefine reasoning with broad web content, align-\\ning with current trends in agentic search, which\\ninvolve synthesizing complex online materials to\\nenhance context-aware and robust reasoning. Con-\\nversely, in specialized areas like medical reasoning,\\napproaches such as FRVA (Fan et al., 2024b) and\\nALR2 (Li et al., 2024d), retrieve literature for accu-\\nrate diagnostics.\\n4.1.3\\nTool Using\\nTool-using approaches leverage external resources\\nlike calculators, libraries, or APIs to enhance rea-\\nsoning interactively. In QA-based reasoning, Re-\\nInvoke (Chen et al., 2024a), AVATAR (Wu et al.,\\n2024), ToolkenGPT (Hao et al., 2023), and Tool-\\nLLM (Qin et al., 2023) invoke calculators or APIs\\n(e.g., Yahoo Finance, Wikidata), improving numer-\\nical accuracy and factual precision. Within the con-\\ntext of scientific modeling, SCIAGENT (Ma et al.,\\n2024b) and TRICE (Qiao et al., 2024) integrate\\nsymbolic computation tools (e.g., WolframAlpha),\\nstrengthening computational robustness. Similarly,\\nin mathematical computation, llm-tool-use (Luo\\net al., 2025b) autonomously employs calculators\\nfor accurate numerical reasoning. Distinctively in\\ncode generation tasks, RAR (Dutta et al., 2024)\\nretrieves code documentation via OSCAT libraries,\\nensuring syntactic accuracy and executable logic.\\n4.2\\nIn-context Retrieval\\nIn-context retrieval leverages a model’s internal\\nexperiences or retrieved examples from demonstra-\\ntions and training data to guide reasoning. This\\nretrieval provides relevant exemplars, guiding mod-\\nels to emulate reasoning patterns and enhancing\\naccuracy and logical coherence in novel questions.\\n4.2.1\\nPrior Experience\\nPrior experience refers to past interactions or suc-\\ncessful strategies stored in a model’s internal mem-\\nory, with retrieval varying by task. In tasks in-\\nvolving planning and decision-making tasks such\\nas robot path finding, RAHL (Sun et al., 2024a)\\nand RA-DT (Schmied et al., 2024) leverage past\\ndecisions and reinforcement signals for sequential\\nreasoning. For interactive reasoning tasks, JARVIS-\\n1 (Wang et al., 2024f), RAP (Kagaya et al., 2024),\\nand EM-LLM (Fountas et al., 2024) dynamically\\nrecall multimodal interactions and conversational\\nhistories, facilitating adaptive reasoning for person-\\nalized interactions. In the domain for logical rea-\\nsoning, CoPS (Yang et al., 2024a) retrieves struc-\\ntured prior cases like medical and legal cases for\\nrobust logical reasoning in medical and legal sce-\\nnarios.\\n4.2.2\\nExample or Training Data\\nUnlike approaches relying on prior experiences,\\nexample-based reasoning retrieves external exam-\\nples from demonstrations or training data. For ex-\\nample, In complex text-understanding, RE4 (Li\\net al., 2024c) and Fei et al. (2024) utilize anno-\\ntated sentence pairs to enhance relation recogni-\\ntion. Addressing QA-based reasoning, OpenRAG\\n(Zhou and Chen, 2025), UPRISE (Cheng et al.,\\n2023), MoD (Wang et al., 2024c), and Dr.ICL (Luo\\net al., 2023) select demonstrations closely match-\\ning queries, improving generalization. Addition-\\nally, in code generation tasks, PERC (Yoo et al.,\\n2025) retrieves pseudocode by semantic or struc-\\ntural similarity from datasets like HumanEval, en-\\nsuring alignment with target code.\\n5\\nSynergized RAG-Reasoning\\nMany real-world problems, such as open-domain\\nquestion answering (Yang et al., 2015; Chen and\\nYih, 2020) and scientific discovery (Lu et al., 2024;\\nWang et al., 2023; Baek et al., 2024; Schmidgall\\net al., 2025), require an iterative approach where\\nnew evidence continuously informs better reason-\\ning and vice versa. A single retrieval step may not\\nprovide sufficient information, and a single round\\nof reasoning may overlook key insights (Trivedi\\net al., 2023). By tightly integrating retrieval and\\nreasoning in a multi-step, interactive manner, these\\nsystems can progressively refine both the search rel-\\nevance of retrieved information and the reasoning-\\nbased understanding of the original query. We\\nfocus on two complementary perspectives within\\nexisting approaches: reasoning workflows, which\\nemphasize structured, often pre-defined inference\\nformats for multi-step reasoning; and agent orches-\\n5'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='tration, which focus on how agents interact with\\nenvironment and coordinate with each others.\\n5.1\\nReasoning Workflow\\nBroadly, the reasoning workflows can be catego-\\nrized as chain-based, tree-based, or graph-based,\\nreflecting an evolution from linear reasoning chains\\nto branching and expressive reasoning structures.\\n5.1.1\\nChain-based\\nChain-of-Thought (CoT) (Wei et al., 2022) struc-\\ntures the reasoning process as a linear sequence\\nof intermediate steps. However, relying solely on\\nthe parametric knowledge of LLMs can lead to\\nerror propagation. To solve this, IRCoT (Trivedi\\net al., 2023) and Rat (Wang et al., 2024g) interleave\\nretrieval operations between reasoning steps. Sev-\\neral recent methods further improve the robustness\\nand rigor of this chain-based paradigm via verifi-\\ncation and filtering. CoV-RAG (He et al., 2024a)\\nintroduces a chain-of-verification that checks and\\ncorrects each reasoning step against retrieved ref-\\nerences. To combat noisy or irrelevant context, ap-\\nproaches like RAFT (Zhang et al., 2024a) fine-tune\\nLLMs to ignore distractor documents, while Chain-\\nof-Note (Yu et al., 2024) prompts the model to take\\nsequential “reading notes” on retrieved documents\\nto filter out unhelpful information.\\n5.1.2\\nTree-based\\nTree-based reasoning methods typically adopt ei-\\nther Tree-of-Thought (ToT) (Yao et al., 2023a)\\nor Monte Carlo Tree Search (MCTS) (Browne\\net al., 2012) approaches. ToT extends the CoT\\nto explicitly construct a deterministic reasoning\\ntree and branch multiple logical pathways. Exam-\\nples include RATT (Zhang et al., 2025a), which\\nconstruct retrieval-augmented thought trees to si-\\nmultaneously evaluate multiple reasoning trajec-\\ntories.\\nSuch ToT principles avoid LLM being\\ntrapped by an early mistaken assumption and have\\nbeen applied to address ambiguous questions (Kim\\net al., 2023), to cover different diagnostic possibili-\\nties (Yang and Huang, 2025), and to create complex\\nstories (Wen et al., 2023). Conversely, MCTS-\\nbased approaches like AirRAG (Feng et al., 2025),\\nMCTS-RAG (Hu et al., 2025), and SeRTS (Hu\\net al., 2024) employ probabilistic tree search, dy-\\nnamically prioritizing exploration based on heuris-\\ntic probabilities. To ensure retrieval and reason-\\ning quality, AirRAG (Feng et al., 2025) incorpo-\\nrates self-consistency checks, and MCTS-RAG (Hu\\net al., 2025) integrates adaptive MCTS retrieval to\\nrefine evidence and reduce hallucinations.\\n5.1.3\\nGraph-based\\nWalk-on-Graph methods mainly rely on graph\\nlearning techniques for the retrieval and rea-\\nsoning.\\nFor example, PullNet (Sun et al.,\\n2019), QA-GNN (Yasunaga et al., 2021), and\\nGreaseLM (Zhang et al., 2022b) directly integrate\\ngraph neural networks (GNNs) to iteratively aggre-\\ngate information from neighbor nodes, excelling\\nat modeling the intricate relationships inherent in\\ngraph-structured data. Methods such as SR (Zhang\\net al., 2022a), LightRAG (Guo et al., 2024), and\\nStructRAG (Li et al., 2024h) employ lightweight\\ngraph techniques such as vector indexing and\\nPageRank to efficiently retrieve and reason in multi-\\nhop context, providing the LLM with high-quality,\\nstructured content tailored for the queries. In con-\\ntrast, Think-on-Graph methods integrate graph\\nstructures directly into the LLM reasoning loop,\\nenabling dynamic and iterative retrieval and reason-\\ning processes guided by the LLMs themselves. In\\nthe Think-on-Graph (ToG) framework (Sun et al.,\\n2024b; Ma et al., 2024a), the LLM uses the KG as\\na “reasoning playground”: at each step, it decides\\nwhich connected entity or relation to explore next,\\ngradually building a path that leads to the answer.\\nWhile Graph-CoT (Jin et al., 2024) introduces a\\nthree-stage iterative loop (reasoning, graph inter-\\naction, and execution), KGP (Wang et al., 2024d)\\nprioritize first constructing a document-level KG,\\nboth enabling LLM-driven graph traversal agent to\\nnavigate passages in each step with globally coher-\\nent context. GraphReader (Li et al., 2024f) further\\nrefines this paradigm by coupling LLM reasoning\\nwith explicit subgraph retrieval and evidence an-\\nchoring at each step\\n5.2\\nAgent Orchestration\\nAccording to agent architectures (Luo et al., 2025a),\\nwe organize existing work into single-agent and\\nmulti-agent. Particularly, we have attached recent\\nadvances in agentic deep research and implementa-\\ntions in Appendix B.\\n5.2.1\\nSingle-Agent\\nSingle agentic system interweaves knowledge re-\\ntrieval (search) into an LLM’s reasoning loop, en-\\nabling dynamic information lookup at each step\\nof problem solving and incentivizing it to actively\\nseek out relevant evidence when needed.\\n6'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='The ReAct (Yao et al., 2023b) paradigm and its\\nderivatives (Li et al., 2025b; Alzubi et al., 2025)\\nhave pioneered this prompting strategy by guid-\\ning LLMs to explicitly alternate between reason-\\ning steps and external tool interactions, such as\\ndatabase searches. Different from ReAct that sepa-\\nrates reasoning and action, with explicit commands\\nlike “search” triggering external retrieval, meth-\\nods such as Self-Ask (Press et al., 2023) and IR-\\nCoT (Trivedi et al., 2023) prompt the model to\\nrecursively formulate and answer sub-questions,\\nenabling interleaved retrieval within the Chain-of-\\nThought (step-by-step retrieval and reasoning). In-\\nvolving self-reflection strategies, DeepRAG (Guan\\net al., 2025) and Self-RAG (Asai et al., 2024) em-\\npower LLMs to introspectively assess their knowl-\\nedge limitations and retrieve only when necessary.\\nRather than relying solely on prompting or static\\nretrievers, Toolformer (Schick et al., 2023) and IN-\\nTERS (Zhu et al., 2024) represent a complementary\\napproach via supervised fine-tuning (SFT) LLMs\\non instruction-based or synthetic datasets that inter-\\nleave search and reasoning. Synthetic data genera-\\ntion (Schick et al., 2023; Mao et al., 2024; Zhang\\net al., 2024a) aims to create large-scale, diverse,\\nand task-specific datasets for search without the\\nneed for extensive human annotation. In contrast,\\ninstruction-based data reformulation (Zhu et al.,\\n2024; Wang et al., 2024a; Lin et al., 2023; Nguyen\\net al., 2024) repurposes existing datasets into in-\\nstructional formats to fine-tune models for im-\\nproved generalization and alignment with human-\\nlike reasoning. INTERS (Zhu et al., 2024) exem-\\nplifies this approach by introducing a SFT dataset\\nencompassing 20 tasks, derived from 43 distinct\\ndatasets with manually written templates.\\nReinforcement learning (RL)-incentivized ap-\\nproaches provides a mechanism to optimize answer\\nquality via reward signals on incentivizing agents’\\nbehaviors – what to search, how to integrate re-\\ntrieved evidence, and when to stop, aiming at com-\\nplex knowledge-intensive tasks (or “deep research”\\nquestions). Notable efforts like WebGPT (Nakano\\net al., 2021) and RAG-RL (Huang et al., 2025a)\\nfocus on improving reasoning fidelity by rewarding\\noutputs based on factual correctness or human pref-\\nerence. More recent contributions operate directly\\nin dynamic environments (e.g., live web search, lo-\\ncal search tools), training agents to explore, reflect,\\nand self-correct in noisy real-world conditions. For\\nexample, Search-R1 (Jin et al., 2025) learns to gen-\\nerate <search> token during reasoning and con-\\ncurrently R1-Searcher (Song et al., 2025) builds\\non RL-driven search demonstrating strong gener-\\nalization across domains. Deep-Researche (Zheng\\net al., 2025) make step further by introducing the\\nfirst end-to-end RL-trained research agent that in-\\nteracts with the open web. These settings showcase\\nemergent capabilities, like decomposition, itera-\\ntive verification, and retrieval planning, that su-\\npervised methods often hard to instill. Moreover,\\nReSearch (Chen et al., 2025b) and ReARTeR (Sun\\net al., 2025c) tackle a deeper challenge: not just\\nproducing correct answers, but aligning reasoning\\nsteps with both factuality and interpretability.\\n5.2.2\\nMulti-Agent\\nThe exploration of multi-agent collaboration within\\nRAG and reasoning has led to diverse orchestra-\\ntions: centralized architectures (harness collective\\nintelligence from workers-manager paradigm) and\\ndecentralized architectures (leverage complemen-\\ntary capabilities from role-specialized agents).\\nDecentralized architectures deploy multiple\\nagents to collaboratively perform retrieval, reason-\\ning, and knowledge integration, aiming to broaden\\ncoverage of relevant information and fully exploit\\nthe heterogeneous strengths of specialized agents.\\nWang et al. (2024e) and Salve et al. (2024) in-\\ntroduce multi-agent systems where each agent re-\\ntrieves from a partitioned database or a specific data\\nsource (relational databases, NoSQL document\\nstores, etc.). Beyond retrieval, Collab-RAG (Xu\\net al., 2025b) and RAG-KG-IL (Yu and McQuade,\\n2025) integrate different model capacities and as-\\nsign them different roles in reasoning and knowl-\\nedge integration. This philosophy extends to multi-\\nmodal settings as in MDocAgent (Han et al., 2025),\\nwhich employs a team of text and image agents to\\nprocess and reason the document-based QA. A gen-\\neral formulation is seen in Agentic reasoning (Wu\\net al., 2025c), which unites tool-using agents for\\nsearch, computation, and structured reasoning, or-\\nchestrated to solve complex analytical tasks.\\nCentralized architectures structure agents in hi-\\nerarchical centralized patterns, supporting efficient\\ntask decomposition and progressive refinement.\\nHM-RAG (Liu et al., 2025) and SurgRAW (Low\\net al., 2025) both employ decomposer-retriever-\\ndecider architectures, where different agent roles\\nisolate subproblems such as multimodal processing\\nor surgical decision-making. Wu et al. (2025a) and\\nIannelli et al. (2024) emphasize dynamic routing\\nand system reconfiguration, respectively—enabling\\n7'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='intelligent agent selection based on task relevance\\nor resource constraints. Chain of Agents (Zhang\\net al., 2024c) and the cooperative multi-agent con-\\ntrol framework for on-ramp merging (Zhang et al.,\\n2025c) illustrate hierarchical agent designs where\\nlayered processing enables long-context summa-\\nrization or policy refinement. Collectively, these\\nworks demonstrate how centralized control and hi-\\nerarchical pipelining foster efficiency and adapt-\\nability in multi-agent RAG-reasoning systems.\\n6\\nBenchmarks and Datasets\\nBenchmarks and datasets for simultaneously evalu-\\nating knowledge (RAG) and reasoning capability\\ncover a wide range of complexities, from basic\\nfact retrieval to intricate multi-step reasoning in\\ngeneral or specific domains. We categorize no-\\ntable benchmarks in several tasks and list them in\\nTable 1 and highlight their details and properties.\\nThese representative tasks include Web browsing,\\nsuch as BrowseComp (Wei et al., 2025a), single-\\nhop QA, such as TriviaQA (Joshi et al., 2017),\\nmulti-hop QA, such as HotpotQA (Yang et al.,\\n2018), multiple-choice QA, such as MMLU-Pro\\n(Wang et al., 2025b), mathematics, such as MATH\\n(Hendrycks et al., 2021), and code-centric eval-\\nuations from LiveCodeBench (Jain et al., 2024).\\nMore tasks can refer to Appendix A and Table 2.\\n7\\nFuture Work\\nFuture research directions for Synergized RAG-\\nReasoning systems center around enhancing both\\nreasoning and retrieval capabilities to meet real-\\nworld demands for accuracy, efficiency, trust, and\\nuser alignment. We outline several key challenges\\nand opportunities below.\\n• Reasoning Efficiency. Despite their advantages\\nin complex reasoning, Synergized RAG-Reasoning\\nsystems can suffer significant latency due to itera-\\ntive retrieval and multi-step reasoning loops (Sui\\net al., 2025). For instance, executing a single deep\\nresearch query can take over 10 minutes in prac-\\ntical settings. This issue is especially pronounced\\nin chain-based workflows discussed in Section 5.\\nFuture research should explore reasoning efficiency\\nthrough latent reasoning approaches and strategic\\ncontrol over reasoning depth via thought distilla-\\ntion and length-penalty (Xia et al., 2025a; Zhang\\net al., 2025b). Beyond reasoning itself, emerging\\ndirections in models compression like quantization,\\npruning, and knowledge distillation is worth to ex-\\nplore for efficient small RAG-reasoning systems.\\n• Retrieval Efficiency. On the retrieval side, effi-\\nciency demands budget-aware query planning and\\nmemory-aware mechanisms that cache prior evi-\\ndence or belief states to reduce redundant access\\n(Zhao et al., 2024a). Additionally, adaptive re-\\ntrieval control, learning when and how much to\\nretrieve based on uncertainty signals can reduce\\nwasteful operations. These technical paths push\\nthe system beyond static RAG, toward dynamic\\nself-regulation of efficient retreival behaviors un-\\nder real-world constraints.\\n• Human-Agent Collaboration. Many applica-\\ntions of RAG-Reasoning, such as literature reviews\\nor interactive programming, are inherently person-\\nalized and cannot assume users know precisely\\nwhat to ask or how to process retrieved results (Sun\\net al., 2025b). Corresponding to Section 5.2, hu-\\nmans can act as advanced agents, providing nu-\\nanced feedback to steer reasoning processes. Fu-\\nture systems should develop methods for modeling\\nuser intent under uncertainty (Zhang et al., 2025e;\\nYang et al., 2025), building interactive interfaces\\nfor iterative clarification, and designing agents that\\nadapt reasoning strategies based on user exper-\\ntise and preferences (Zhang et al., 2025g). This\\nhuman-in-the-loop approach (Zou et al., 2025) is\\nessential for creating robust and user-aligned RAG-\\nReasoning systems in open-ended domains.\\n• Agentic Structures and Capabilities. A key fea-\\nture of Synergized RAG-Reasoning is its agentic ar-\\nchitecture, where the system autonomously decides\\nthe roles of different agents and which tools or re-\\ntrieval strategies to invoke during inference stages\\n(Luo et al., 2025a; Bei et al., 2025). To fully ex-\\nploit this potential, future research should focus on\\ndeveloping agent frameworks capable of dynamic\\ntool selection, retrieval planning, and adaptive or-\\nchestration across reasoning workflows. Such ca-\\npabilities enable flexible, context-aware problem\\nsolving and are critical for handling diverse, com-\\nplex tasks (Schneider, 2025).\\n• Multimodal Retrieval. As also shown in our\\nbenchmark analysis, most existing Synergized\\nRAG-Reasoning systems remain confined to text-\\nonly tasks. However, real-world applications in-\\ncreasingly require the ability to retrieve and in-\\ntegrate multimodal content (Liang et al., 2024).\\n8'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Task\\nDataset\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning\\nSize\\nInput\\nOutput\\nWeb Browsing\\nBrowseComp (Wei et al., 2025a)\\nGeneral\\nHuman, Internet\\nCommonsense, Logical\\nDeductive\\n1,266\\nQuestion/Text\\nNatural Language\\nGAIA (Mialon et al., 2023)\\nGeneral\\nInternet, TooL\\nCommonsense, Logical\\nDeductive\\n466\\nQuestion/Text,\\nImage/File/Code\\nNatural Language\\nWebWalkerQA (Wu et al., 2025b)\\nGeneral\\nHuman, LLM\\nCommonsense, Logical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nSingle-hop QA\\nTriviaQA (Joshi et al., 2017)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n650,000+\\nQuestion/Text\\nNatural Language\\nNQ (Kwiatkowski et al., 2019)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n307,373\\nQuestion/Text\\nNatural Language\\nMulti-hop QA\\n2WikiMultiHopQA (Ho et al., 2020)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al., 2018)\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al., 2022)\\nGeneral\\nPrevious Resource,\\nInternet\\nCommonsense, Logical\\nDeductive\\n25,000\\nQuestion/Text\\nNatural Language\\nMulti-choice QA QuALITY (Pang et al., 2022)\\nNarrative\\nBooks\\nCommonsense, Logical\\nDeductive,\\nAbductive\\n6,737\\nQuestion/Text,\\nOptions\\nOptions\\nMMLU-Pro (Wang et al., 2025b)\\nScience\\nPrevious Resource,\\nInternet\\nArithmetic, Commonsense,\\nLogical\\nDeductive,\\nInductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Langue,\\nNumber, Options\\nMath\\nMATH (Hendrycks et al., 2021)\\nMath\\nExam\\nArithmetic, Logic\\nDeductive\\n12,500\\nQuestion/Text,\\nFigure, Equation\\nNatural Langue,\\nNumber\\nAQuA (Ling et al., 2017)\\nMath\\nExam, Internet,\\nPrevious Resource\\nArithmetic, Logic\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equation\\nNatural Langue,\\nOptions\\nCode\\nRefactoring Oracle (Tsantalis et al.,\\n2020)\\nSoftware\\nInternet, Human\\nLogical\\nDeductive\\n7,226\\nCode, Instruction\\nCode\\nLiveCodeBench (Jain et al., 2024)\\nContest\\nInternet\\nLogical\\nDeductive,\\nAbductive\\n500+\\nQuestion/Text,\\nCode, Instruction\\nCode, Test Output\\nTable 1: Overview of representative knowledge and reasoning intensive benchmarks by task category.\\nFuture research should move beyond the tradi-\\ntional vision-text paradigm to achieve genuine mul-\\ntimodality. This advancement necessitates strength-\\nening foundational abilities of MLLMs, including\\ngrounding and cross-modal reasoning (Liang et al.,\\n2024). Additionally, enhancing the agentic capabil-\\nities of these models through hybrid-modal chain-\\nof-thought reasoning is crucial, enabling interac-\\ntion with the real world via multimodal search tools\\n(Wang et al., 2025a). Concurrently, developing uni-\\nfied multimodal retrievers that can jointly embed\\nimages, tables, text, and heterogeneous documents\\nis essential.\\n• Retrieval Trustworthiness. Synergized RAG-\\nReasoning systems remain vulnerable to adversar-\\nial attacks through poisoned or misleading external\\nknowledge sources. Ensuring the trustworthiness\\nof retrieved content is therefore crucial for main-\\ntaining fully reliable downstream reasoning (Huang\\net al., 2024). Techniques like watermarking and\\ndigital fingerprinting have been employed to en-\\nhance system traceability. However, there’s a press-\\ning need to develop more dynamic and adaptive\\nmethods that can keep pace with the evolving land-\\nscape of LLMs, emerging attack techniques, and\\nshifting model contexts (Liu et al., 2024). Existing\\nstudies have also individually explored uncertainty\\nquantification and robust generation to bolster sys-\\ntem reliability (Shorinwa et al., 2025). Future re-\\nsearch should aim to integrate these approaches,\\nas their combination can mutually reinforce sys-\\ntem robustness and trustworthiness. Moreover, fu-\\nture efforts should also focus on extending current\\nbenchmarks to encompass multi-dimensional trust\\nmetrics beyond mere accuracy.\\n8\\nConclusion\\nThis survey charts the rapid convergence of re-\\ntrieval and reasoning in LLMs.\\nWe reviewed\\nthree evolutionary stages: (1) Reasoning-Enhanced\\nRAG, which uses multi-step reasoning to refine\\neach stage of RAG; (2) RAG-Enhanced Reason-\\ning, which leverages retrieved knowledge to bridge\\nfactual gaps during long CoT; and (3) Synergized\\nRAG-Reasoning systems, where single- or multi-\\nagents iteratively refine both search and reason-\\ning, exemplified by recent “Deep Research” plat-\\nforms. Collectively, these lines demonstrate that\\ntight retrieval–reasoning coupling improves fac-\\ntual grounding, logical coherence, and adaptability\\nbeyond one-way enhancement. Looking forward,\\nwe identify research avenues toward synergized\\nRAG-Reasoning systems that are more effective,\\nmultimodally-adaptive, trustworthy, and human-\\ncentric.\\nLimitations\\nWhile this survey synthesizes over 200 research\\npapers across RAG and reasoning with large lan-\\nguage models, its scope favors breadth over depth.\\nIn striving to provide a unified and comprehen-\\nsive taxonomy, we may not delve deeply into the\\ntechnical nuances or implementation details of indi-\\nvidual methods-especially within specialized sub-\\nfields of either RAG (e.g., sparse vs. dense re-\\ntrieval, memory-augmented retrievers) or reason-\\ning (e.g., formal logic solvers, symbolic methods,\\nor long-context reasoning). Moreover, our cate-\\n9'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='gorization framework (reasoning-enhanced RAG,\\nRAG-enhanced reasoning, and synergized RAG\\nand reasoning) abstracts across diverse methodolo-\\ngies. While this facilitates a high-level understand-\\ning of design patterns, it may obscure the finer-\\ngrained trade-offs, assumptions, and limitations\\nunique to each class of approach.\\nReferences\\nVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-\\nman, Harm de Vries, and Siva Reddy. 2022. Topi-\\nocqa: Open-domain conversational question answer-\\ning with topic switching. Transactions of the Associ-\\nation for Computational Linguistics, 10:468–483.\\nFiroj Alam, Ferda Ofli, and Muhammad Imran. 2018.\\nCrisismmd: Multimodal twitter datasets from natural\\ndisasters. In Proceedings of the international AAAI\\nconference on web and social media, volume 12.\\nSalaheddin Alzubi, Creston Brooks, Purva Chiniya,\\nEdoardo Contente, Chiara von Gerlach, Lucas Irwin,\\nYihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong\\nOh, et al. 2025. Open deep search: Democratizing\\nsearch with open-source reasoning agents. arXiv\\npreprint arXiv:2503.20201.\\nAnonymous. 2025.\\nDynQR: Dynamic uncertainty-\\nguided query rewriting for effective retrieval-\\naugmented generation. In Submitted to ACL Rolling\\nReview - December 2024. Under review.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil,\\nand Hannaneh Hajishirzi. 2023.\\nSelf-RAG: Self-\\nreflective retrieval augmented generation.\\nIn\\nNeurIPS 2023 Workshop on Instruction Tuning and\\nInstruction Following.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2024. Self-RAG: Learning to\\nretrieve, generate, and critique through self-reflection.\\nIn The Twelfth International Conference on Learning\\nRepresentations.\\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,\\nand Sung Ju Hwang. 2024.\\nResearchagent: Iter-\\native research idea generation over scientific liter-\\nature with large language models. arXiv preprint\\narXiv:2404.07738.\\nYuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi\\nChen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu,\\nShirui Pan, Yizhou Yu, et al. 2025. Graphs meet ai\\nagents: Taxonomy, progress, and future opportunities.\\narXiv preprint arXiv:2506.18019.\\nCameron B Browne, Edward Powley, Daniel White-\\nhouse, Simon M Lucas, Peter I Cowling, Philipp\\nRohlfshagen, Stephen Tavener, Diego Perez, Spyri-\\ndon Samothrakis, and Simon Colton. 2012. A survey\\nof monte carlo tree search methods. IEEE Transac-\\ntions on Computational Intelligence and AI in games,\\n4(1):1–43.\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-\\nvey on evaluation of large language models. ACM\\ntransactions on intelligent systems and technology,\\n15(3):1–45.\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\\nquestion answering. In Proceedings of the 58th an-\\nnual meeting of the association for computational\\nlinguistics: tutorial abstracts, pages 34–37.\\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou,\\nChenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen\\nZhang, Huajun Chen, Fan Yang, et al. 2025a.\\nResearch:\\nLearning to reason with search for\\nllms via reinforcement learning.\\narXiv preprint\\narXiv:2503.19470.\\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou,\\nChenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng\\nChen, Haofen Wang, Jeff Z Pan, et al. 2025b. Learn-\\ning to reason with search for llms via reinforcement\\nlearning. arXiv preprint arXiv:2503.19470.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng,\\nJiannan Guan, Peng Wang, Mengkang Hu, Yuhang\\nZhou, Te Gao, and Wanxiang Che. 2025c. Towards\\nreasoning era: A survey of long chain-of-thought\\nfor reasoning large language models. arXiv preprint\\narXiv:2503.09567.\\nYanfei Chen, Jinsung Yoon, Devendra Sachan, Qingze\\nWang, Vincent Cohen-Addad, Mohammadhossein\\nBateni, Chen-Yu Lee, and Tomas Pfister. 2024a. Re-\\ninvoke: Tool invocation rewriting for zero-shot tool\\nretrieval. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 4705–4726.\\nZehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu,\\nWenwei Zhang, Kai Chen, and Feng Zhao. 2024b.\\nMindsearch: Mimicking human minds elicits deep ai\\nsearcher. arXiv preprint arXiv:2407.20183.\\nZhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen\\nHuang, Yong Dou, Xuhui Jiang, and Jian Guo. 2024c.\\nRulerag: Rule-guided retrieval-augmented genera-\\ntion with language models for question answering.\\narXiv preprint arXiv:2410.22353.\\nDaixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng\\nZhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu\\nWei, Weiwei Deng, and Qi Zhang. 2023. Uprise:\\nUniversal prompt retrieval for improving zero-shot\\nevaluation. In Proceedings of the 2023 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 12318–12337.\\nRong Cheng, Jinyi Liu, Yan Zheng, Fei Ni, Jiazhen Du,\\nHangyu Mao, Fuzheng Zhang, Bo Wang, and Jianye\\nHao. 2025. Dualrag: A dual-process approach to in-\\ntegrate reasoning and retrieval for multi-hop question\\nanswering. arXiv preprint arXiv:2504.18243.\\nZheng Chu, Jingchang Chen, Qianglong Chen, Haotian\\nWang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu,\\n10'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='and Bing Qin. 2024. Beamaggr: Beam aggregation\\nreasoning over multi-source knowledge for multi-\\nhop question answering. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 1229–\\n1248.\\nDebrup Das, Debopriyo Banerjee, Somak Aditya,\\nand Ashish Kulkarni. 2024. Mathsensei: A tool-\\naugmented large language model for mathematical\\nreasoning. In Proceedings of the 2024 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies (Volume 1: Long Papers), pages 942–966.\\nChao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-\\nZhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song,\\nBo Zheng, et al. 2024. Longdocurl: a comprehensive\\nmultimodal long document benchmark integrating un-\\nderstanding, reasoning, and locating. arXiv preprint\\narXiv:2412.18424.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. 2024. Chain-of-verification reduces\\nhallucination in large language models. In Findings\\nof the Association for Computational Linguistics ACL\\n2024, pages 3563–3578.\\nAvik Dutta, Mukul Singh, Gust Verbruggen, Sumit Gul-\\nwani, and Vu Le. 2024. Rar: Retrieval-augmented re-\\ntrieval for code generation in low resource languages.\\nIn Proceedings of the 2024 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n21506–21515.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang,\\nHengyun Li, Dawei Yin, Tat-Seng Chua, and Qing\\nLi. 2024a. A survey on rag meeting llms: Towards\\nretrieval-augmented large language models. In Pro-\\nceedings of the 30th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, pages 6491–\\n6501.\\nYue Fan, Hu Zhang, Ru Li, Yujie Wang, Hongye Tan,\\nand Jiye Liang. 2024b. Frva: Fact-retrieval and ver-\\nification augmented entailment tree generation for\\nexplainable question answering. In Findings of the\\nAssociation for Computational Linguistics ACL 2024,\\npages 9111–9128.\\nJinyuan Fang, Zaiqiao Meng, and Craig Macdonald.\\n2024. Trace the evidence: Constructing knowledge-\\ngrounded reasoning chains for retrieval-augmented\\ngeneration. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2024, pages 8472–\\n8494.\\nWeizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang,\\nBo Bai, Lei Deng, and Wei Han. 2024.\\nRe-\\ntrieval meets reasoning: Dynamic in-context edit-\\ning for long-text understanding.\\narXiv preprint\\narXiv:2406.12331.\\nWenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi\\nSong, and Hao Wang. 2025.\\nAirrag:\\nActivat-\\ning intrinsic reasoning for retrieval augmented gen-\\neration via tree-based search.\\narXiv preprint\\narXiv:2501.10053.\\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi,\\nTushar Khot, and Pradeep Dasigi. 2020.\\nIirc: A\\ndataset of incomplete information reading compre-\\nhension questions. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 1137–1147.\\nZafeirios Fountas, Martin A Benfeghoul, Adnan Oomer-\\njee, Fenia Christopoulou, Gerasimos Lampouras,\\nHaitham Bou-Ammar, and Jun Wang. 2024. Human-\\nlike episodic memory for infinite context llms. arXiv\\npreprint arXiv:2407.09450.\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\\nKelvin Guu. 2023a. RARR: Researching and revis-\\ning what language models say, using language mod-\\nels. In Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 16477–16508, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang\\nJia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun,\\nHaofen Wang, and Haofen Wang. 2023b. Retrieval-\\naugmented generation for large language models: A\\nsurvey. arXiv preprint arXiv:2312.10997, 2:1.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 9:346–\\n361.\\nXinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin,\\nYaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and\\nJie Zhou. 2025. Deeprag: Thinking to retrieval step\\nby step for large language models. arXiv preprint\\narXiv:2502.01142.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and\\nChao Huang. 2024.\\nLightrag: Simple and fast\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2410.05779.\\nSiwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li,\\nHongtu Zhu, and Huaxiu Yao. 2025. Mdocagent: A\\nmulti-modal multi-agent framework for document\\nunderstanding. arXiv preprint arXiv:2503.13964.\\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.\\n2023.\\nToolkengpt: Augmenting frozen language\\nmodels with massive tools via tool embeddings. In\\nAdvances in Neural Information Processing Systems,\\nvolume 36, pages 45870–45894.\\nBolei He, Nuo Chen, Xinran He, Lingyong Yan,\\nZhenkai Wei, Jinchang Luo, and Zhen-Hua Ling.\\n11'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='2024a.\\nRetrieving, rethinking and revising: The\\nchain-of-verification can improve retrieval aug-\\nmented generation. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2024, pages\\n10371–10393.\\nJie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Z\\nPan. 2024b. Mintqa: A multi-hop question answer-\\ning benchmark for evaluating llms on new and tail\\nknowledge. arXiv preprint arXiv:2412.17032.\\nXiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla,\\nThomas Laurent, Yann LeCun, Xavier Bresson,\\nand Bryan Hooi. 2024c.\\nG-retriever: Retrieval-\\naugmented generation for textual graph understand-\\ning and question answering. Advances in Neural\\nInformation Processing Systems, 37:132876–132907.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and\\nJacob Steinhardt. 2021. Measuring mathematical\\nproblem solving with the MATH dataset. In Thirty-\\nfifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing a multi-hop\\nqa dataset for comprehensive evaluation of reasoning\\nsteps. In Proceedings of the 28th International Con-\\nference on Computational Linguistics, pages 6609–\\n6625.\\nMinda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou,\\nJingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li,\\nand Irwin King. 2024. Serts: Self-rewarding tree\\nsearch for biomedical retrieval-augmented genera-\\ntion. arXiv preprint arXiv:2406.11258.\\nYunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan.\\n2025. Mcts-rag: Enhancing retrieval-augmented gen-\\neration with monte carlo tree search. arXiv preprint\\narXiv:2503.20757.\\nJerry Huang, Siddarth Madala, Risham Sidhu, Cheng\\nNiu, Julia Hockenmaier, and Tong Zhang. 2025a.\\nRag-rl: Advancing retrieval-augmented generation\\nvia rl and curriculum learning.\\narXiv preprint\\narXiv:2503.12759.\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\\nZhangyin Feng, Haotian Wang, Qianglong Chen,\\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al.\\n2025b. A survey on hallucination in large language\\nmodels: Principles, taxonomy, challenges, and open\\nquestions. ACM Transactions on Information Sys-\\ntems, 43(2):1–55.\\nXiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie\\nJin, Yi Dong, Changshun Wu, Saddek Bensalem,\\nRonghui Mu, Yi Qi, Xingyu Zhao, et al. 2024. A sur-\\nvey of safety and trustworthiness of large language\\nmodels through the lens of verification and validation.\\nArtificial Intelligence Review, 57(7):175.\\nYulong Hui, Yao Lu, and Huanchen Zhang. 2024. Uda:\\nA benchmark suite for retrieval augmented genera-\\ntion in real-world document analysis. In The Thirty-\\neight Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.\\nMichael Iannelli, Sneha Kuchipudi, and Vera Dvorak.\\n2024. Sla management in reconfigurable multi-agent\\nrag: A systems approach to question answering.\\narXiv preprint arXiv:2412.06832.\\nShayekh Islam, Md Asib Rahman, KSM Tozammel Hos-\\nsain, Enamul Hoque, Shafiq Joty, and Md Rizwan\\nParvez. 2024. Open-rag: Enhanced retrieval aug-\\nmented reasoning with open-source large language\\nmodels. In Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2024, pages 14231–\\n14244.\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia\\nYan, Tianjun Zhang, Sida Wang, Armando Solar-\\nLezama, Koushik Sen, and Ion Stoica. 2024. Live-\\ncodebench: Holistic and contamination free eval-\\nuation of large language models for code. arXiv\\npreprint arXiv:2403.07974.\\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\\ning to adapt retrieval-augmented large language mod-\\nels through question complexity. In Proceedings of\\nthe 2024 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Pa-\\npers), pages 7029–7043.\\nYixin Ji, Kaixin Wu, Juntao Li, Wei Chen, Mingjie\\nZhong, Xu Jia, and Min Zhang. 2024. Retrieval and\\nreasoning on kgs: Integrate knowledge graphs into\\nlarge language models for complex question answer-\\ning. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 7598–7610.\\nMingyi Jia, Junwen Duan, Yan Song, and Jianxin Wang.\\n2025. Find: Fine-grained information density guided\\nadaptive retrieval-augmented generation for disease\\ndiagnosis. arXiv preprint arXiv:2502.14614.\\nJinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie\\nWang, Wayne Xin Zhao, Yang Song, and Tao Zhang.\\n2024. Rag-star: Enhancing deliberative reasoning\\nwith retrieval augmented verification and refinement.\\narXiv preprint arXiv:2412.12881.\\nPengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu\\nTian, SeongKu Kang, Zifeng Wang, Jimeng Sun,\\nand Jiawei Han. 2025. Deepretrieval: Hacking real\\nsearch engines and retrievers with large language\\nmodels via reinforcement learning. arXiv preprint\\narXiv:2503.00223.\\nBowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar\\nRoy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang,\\nSuhang Wang, Yu Meng, et al. 2024. Graph chain-\\nof-thought: Augmenting large language models by\\nreasoning on graphs. In Findings of the Association\\nfor Computational Linguistics ACL 2024, pages 163–\\n184.\\n12'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang,\\nHamed Zamani, and Jiawei Han. 2025.\\nSearch-\\nr1: Training llms to reason and leverage search en-\\ngines with reinforcement learning. arXiv preprint\\narXiv:2503.09516.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 1601–1611.\\nTomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou,\\nJayashree Karlekar, Sugiri Pranata, Akira Kinose,\\nKoki Oguri, Felix Wick, and Yang You. 2024. Rap:\\nRetrieval-augmented planning with contextual mem-\\nory for multimodal llm agents.\\narXiv preprint\\narXiv:2402.03610.\\nMohammed Khaliq, Paul Chang, Mingyang Ma, Bern-\\nhard Pflugfelder, and Filip Mileti´c. 2024.\\nRagar,\\nyour falsehood radar: Rag-augmented reasoning for\\npolitical fact-checking using multimodal large lan-\\nguage models. In Proceedings of the Seventh Fact Ex-\\ntraction and VERification Workshop (FEVER), pages\\n280–296.\\nGangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joon-\\nsuk Park, and Jaewoo Kang. 2023. Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-\\naugmented large language models. In Proceedings\\nof the 2023 Conference on Empirical Methods in\\nNatural Language Processing, pages 996–1009.\\nNeema Kotonya and Francesca Toni. 2020. Explainable\\nautomated fact-checking for public health claims. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 7740–7754.\\nHeiko Koziolek, Sten Grüner, Rhaban Hark, Viren-\\ndra Ashiwal, Sofia Linsbauer, and Nafise Eskandani.\\n2024. Llm-based and retrieval-augmented control\\ncode generation. In Proceedings of the 1st Inter-\\nnational Workshop on Large Language Models for\\nCode, pages 22–29.\\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mo-\\nhananey, Steven Schwarcz, Adam Stambler, Shyam\\nUpadhyay, and Manaal Faruqui. 2024.\\nFact,\\nfetch,\\nand reason:\\nA unified evaluation of\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2409.12941.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, et al. 2019. Natural questions: a benchmark\\nfor question answering research. Transactions of the\\nAssociation for Computational Linguistics, 7:453–\\n466.\\nSung-Min Lee, Eunhwan Park, Donghyeon Jeon, Inho\\nKang, and Seung-Hoon Na. 2024. Radcot: Retrieval-\\naugmented distillation to specialization models for\\ngenerating chain-of-thoughts in query expansion. In\\nProceedings of the 2024 Joint International Con-\\nference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING 2024),\\npages 13514–13523.\\nZhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang,\\nWeichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi\\nLi. 2025. Rearag: Knowledge-guided reasoning en-\\nhances factuality of large reasoning models with iter-\\native retrieval augmented generation. arXiv preprint\\narXiv:2503.21729.\\nDawei Li, Shu Yang, Zhen Tan, Jae Baik, Sukwon Yun,\\nJoseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-\\nTran, Ying Ding, et al. 2024a. Dalk: Dynamic co-\\naugmentation of llms and kg to answer alzheimer’s\\ndisease questions with scientific literature. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2024, pages 2187–2205.\\nGuanghua Li, Wensheng Lu, Wei Zhang, Defu Lian,\\nKezhong Lu, Rui Mao, Kai Shu, and Hao Liao.\\n2024b. Re-search for the truth: Multi-round retrieval-\\naugmented large language models are strong fake\\nnews detectors. arXiv preprint arXiv:2403.09747.\\nGuozheng Li, Peng Wang, Wenjun Ke, Yikai Guo, Ke Ji,\\nZiyu Shang, Jiajun Liu, and Zijie Xu. 2024c. Recall,\\nretrieve and reason: towards better in-context relation\\nextraction. In Proceedings of the Thirty-Third Inter-\\nnational Joint Conference on Artificial Intelligence,\\npages 6368–6376.\\nHuayang Li, Pat Verga, Priyanka Sen, Bowen Yang,\\nVijay Viswanathan, Patrick Lewis, Taro Watanabe,\\nand Yixuan Su. 2024d.\\nAlr2:\\nA retrieve-then-\\nreason framework for long-context question answer-\\ning. arXiv preprint arXiv:2410.03227.\\nJia Li, Xianjie Shi, Kechi Zhang, Lei Li, Ge Li, Zheng-\\nwei Tao, Fang Liu, Chongyang Tao, and Zhi Jin.\\n2025a. Coderag: Supportive code retrieval on bi-\\ngraph for real-world code generation. arXiv preprint\\narXiv:2504.10046.\\nMinghan Li, Honglei Zhuang, Kai Hui, Zhen Qin,\\nJimmy Lin, Rolf Jagerman, Xuanhui Wang, and\\nMichael Bendersky. 2024e. Can query expansion im-\\nprove generalization of strong cross-encoder rankers?\\nIn Proceedings of the 47th International ACM SIGIR\\nConference on Research and Development in Infor-\\nmation Retrieval, pages 2321–2326.\\nShilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu,\\nGe Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yang-\\nguang Li, Wanli Ouyang, et al. 2024f. Graphreader:\\nBuilding graph-based agent to enhance long-context\\nabilities of large language models. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2024, pages 12758–12786.\\nXiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang,\\nYujia Zhou,\\nYutao Zhu,\\nPeitian Zhang,\\nand\\nZhicheng Dou. 2025b. Search-o1: Agentic search-\\nenhanced large reasoning models. arXiv preprint\\narXiv:2501.05366.\\n13'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yu-\\ntao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng\\nDou. 2025c. Webthinker: Empowering large rea-\\nsoning models with deep research capability. arXiv\\npreprint arXiv:2504.21776.\\nYangning Li, Yinghui Li, Xinyu Wang, Yong Jiang,\\nZhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao\\nZheng, Fei Huang, Jingren Zhou, and Philip S. Yu.\\n2025d.\\nBenchmarking multimodal retrieval aug-\\nmented generation with dynamic VQA dataset and\\nself-adaptive planning agent. In The Thirteenth Inter-\\nnational Conference on Learning Representations.\\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\\nCao, and Shuzi Niu. 2017. Dailydialog: A manually\\nlabelled multi-turn dialogue dataset. arXiv preprint\\narXiv:1710.03957.\\nZhi Li, Yicheng Li, Hequan Ye, and Yin Zhang. 2024g.\\nTowards autonomous tool utilization in language\\nmodels: A unified, efficient and scalable frame-\\nwork. In Proceedings of the 2024 Joint International\\nConference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING 2024),\\npages 16422–16432.\\nZhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Ji-\\naxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian\\nXu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al.\\n2025e. From system 1 to system 2: A survey of\\nreasoning large language models.\\narXiv preprint\\narXiv:2502.17419.\\nZhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu\\nLin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xian-\\npei Han, Le Sun, and Yongbin Li. 2024h. Struc-\\ntrag: Boosting knowledge intensive reasoning of llms\\nvia inference-time hybrid information structurization.\\narXiv preprint arXiv:2410.08815.\\nZijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang,\\nQi Wang, Qiang Fu, and Ke Liu. 2024. A survey of\\nmultimodel large language models. In Proceedings\\nof the 3rd International Conference on Computer,\\nArtificial Intelligence and Control Engineering, pages\\n405–409.\\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\\nMaria Lomeli, Richard James, Pedro Rodriguez, Ja-\\ncob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.\\nRa-dit: Retrieval-augmented dual instruction tuning.\\nIn The Twelfth International Conference on Learning\\nRepresentations.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. arXiv preprint arXiv:1705.04146.\\nAiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming\\nHu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong,\\nand Philip Yu. 2024. A survey of text watermarking\\nin the era of large language models. ACM Computing\\nSurveys, 57(2):1–36.\\nPei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan\\nMeng, Ding Wang, and Jun Ma. 2025. Hm-rag: Hier-\\narchical multi-agent multimodal retrieval augmented\\ngeneration. arXiv preprint arXiv:2504.12330.\\nChang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao\\nZeng, Zhu Zhuo, Evangelos B Mazomenos, and\\nYueming Jin. 2025. Surgraw: Multi-agent workflow\\nwith chain-of-thought reasoning for surgical intelli-\\ngence. arXiv preprint arXiv:2503.10265.\\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foer-\\nster, Jeff Clune, and David Ha. 2024. The ai scientist:\\nTowards fully automated open-ended scientific dis-\\ncovery. arXiv preprint arXiv:2408.06292.\\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:\\nMultimodal reasoning via thought chains for science\\nquestion answering. Advances in Neural Information\\nProcessing Systems, 35:2507–2521.\\nJunyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Jun-\\nwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue\\nQiao, Qingqing Long, et al. 2025a. Large language\\nmodel agent: A survey on methodology, applications\\nand challenges. arXiv preprint arXiv:2503.21460.\\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasu-\\npat, Mehran Kazemi, Chitta Baral, Vaiva Im-\\nbrasaite, and Vincent Y Zhao. 2023.\\nDr. icl:\\nDemonstration-retrieved in-context learning. arXiv\\npreprint arXiv:2305.14128.\\nNe Luo, Aryo Pradipta Gema, Xuanli He, Emile\\nvan Krieken, Pietro Lesci, and Pasquale Minervini.\\n2025b.\\nSelf-training large language models for\\ntool-use without demonstrations.\\narXiv preprint\\narXiv:2502.05867.\\nShengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li,\\nHuaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo.\\n2024a. Think-on-graph 2.0: Deep and faithful large\\nlanguage model reasoning with knowledge-guided\\nretrieval augmented generation.\\narXiv preprint\\narXiv:2407.10805.\\nYubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu,\\nShuohang Wang, Liangming Pan, Yujiu Yang, Yixin\\nCao, and Aixin Sun. 2024b.\\nSciagent:\\nTool-\\naugmented language models for scientific reasoning.\\nIn Proceedings of the 2024 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n15701–15736.\\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen,\\nYizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. 2025. Mmlongbench-doc:\\nBenchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information\\nProcessing Systems, 37:95963–96010.\\nKelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo,\\nChenlong Deng, and Zhicheng Dou. 2024.\\nRag-\\nstudio: Towards in-domain adaptation of retrieval\\n14'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='augmented generation through self-alignment. In\\nFindings of the Association for Computational Lin-\\nguistics: EMNLP 2024, pages 725–735.\\nMaria Marina, Nikolay Ivanov, Sergey Pletenev,\\nMikhail Salnikov,\\nDaria Galimzianova,\\nNikita\\nKrayko, Vasily Konovalov, Alexander Panchenko,\\nand Viktor Moskvoretskii. 2025. Llm-independent\\nadaptive rag: Let the question speak for itself. arXiv\\npreprint arXiv:2505.04253.\\nCostas Mavromatis and George Karypis. 2024. Gnn-\\nrag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139.\\nGrégoire Mialon, Clémentine Fourrier, Thomas Wolf,\\nYann LeCun, and Thomas Scialom. 2023. Gaia: a\\nbenchmark for general ai assistants. In The Twelfth\\nInternational Conference on Learning Representa-\\ntions.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021.\\nWebgpt: Browser-assisted question-\\nanswering with human feedback.\\narXiv preprint\\narXiv:2112.09332.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\n2018. Don’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for ex-\\ntreme summarization. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 1797–1807.\\nXuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam,\\nAustin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Sil-\\nvio Savarese, Caiming Xong, and Shafiq Joty. 2024.\\nSfr-rag: Towards contextually faithful llms. arXiv\\npreprint arXiv:2409.09916.\\nCheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Jun-\\ntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu,\\nShizhe Diao, and Tong Zhang. 2024. Veract scan:\\nRetrieval-augmented fake news detection with justifi-\\nable reasoning. In Proceedings of the 62nd Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 3: System Demonstrations), pages\\n266–277.\\nYasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and\\nGreg Durrett. 2021. Creak: A dataset for common-\\nsense reasoning over entity knowledge. OpenReview.\\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\\nPadmakumar, Johnny Ma, Jana Thompson, He He,\\net al. 2022. Quality: Question answering with long\\ninput texts, yes! In Proceedings of the 2022 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, pages 5336–5358.\\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li,\\nJosephina Hu, Hugh Zhang, Chen Bo Calvin Zhang,\\nMohamed Shaaban, John Ling, Sean Shi, et al.\\n2025.\\nHumanity’s last exam.\\narXiv preprint\\narXiv:2501.14249.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language\\nmodels. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2023, pages 5687–5711.\\nShuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai\\nJia, Huajun Chen, and Ningyu Zhang. 2024. Making\\nlanguage models better tool learners with execution\\nfeedback. In Proceedings of the 2024 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies (Volume 1: Long Papers), pages 3550–\\n3568.\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\\nBill Qian, et al. 2023. Toolllm: Facilitating large\\nlanguage models to master 16000+ real-world apis.\\narXiv preprint arXiv:2307.16789.\\nLeonardo Ranaldi, Marco Valentino, and Andrè Fre-\\nitas. 2024. Eliciting critical reasoning in retrieval-\\naugmented language models via contrastive explana-\\ntions. arXiv preprint arXiv:2410.22874.\\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jack-\\nson Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-\\nlian Michael, and Samuel R Bowman. 2024. Gpqa:\\nA graduate-level google-proof q&a benchmark. In\\nFirst Conference on Language Modeling.\\nAniruddha Salve, Saba Attar, Mahesh Deshmukh, Say-\\nali Shivpuje, and Arnab Mitra Utsab. 2024. A collab-\\norative multi-agent approach to retrieval-augmented\\ngeneration across diverse data.\\narXiv preprint\\narXiv:2412.05838.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. 2023.\\nToolformer: Language models can teach themselves\\nto use tools. Advances in Neural Information Pro-\\ncessing Systems, 36:68539–68551.\\nSamuel Schmidgall, Yusheng Su, Ze Wang, Ximeng\\nSun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng\\nLiu, and Emad Barsoum. 2025.\\nAgent labora-\\ntory: Using llm agents as research assistants. arXiv\\npreprint arXiv:2501.04227.\\nThomas Schmied, Fabian Paischer, Vihang Patil,\\nMarkus Hofmarcher, Razvan Pascanu, and Sepp\\nHochreiter. 2024.\\nRetrieval-augmented decision\\ntransformer:\\nExternal memory for in-context rl.\\narXiv preprint arXiv:2410.07071.\\nJohannes Schneider. 2025. Generative to agentic ai:\\nSurvey, conceptualization, and challenges. arXiv\\npreprint arXiv:2504.18875.\\n15'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Eva Sharma, Chen Li, and Lu Wang. 2019. Bigpatent:\\nA large-scale dataset for abstractive and coherent\\nsummarization. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, pages 2204–2213.\\nOla Shorinwa, Zhiting Mei, Justin Lidard, Allen Z Ren,\\nand Anirudha Majumdar. 2025. A survey on un-\\ncertainty quantification of large language models:\\nTaxonomy, open research challenges, and future di-\\nrections. ACM Computing Surveys.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote,\\nYonatan Bisk,\\nAdam Trischler,\\nand Matthew\\nHausknecht. Alfworld: Aligning text and embod-\\nied environments for interactive learning. In Interna-\\ntional Conference on Learning Representations.\\nHuatong Song, Jinhao Jiang, Yingqian Min, Jie Chen,\\nZhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-\\nRong Wen. 2025. R1-searcher: Incentivizing the\\nsearch capability in llms via reinforcement learning.\\narXiv preprint arXiv:2503.05592.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta,\\nAdrià Garriga-Alonso, et al. 2022.\\nBeyond the\\nimitation game: Quantifying and extrapolating the\\ncapabilities of language models.\\narXiv preprint\\narXiv:2206.04615.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu\\nZhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, An-\\ndrew Wen, Shaochen Zhong, Hanjie Chen, et al.\\n2025. Stop overthinking: A survey on efficient rea-\\nsoning for large language models. arXiv preprint\\narXiv:2503.16419.\\nChuanneng Sun, Songjun Huang, and Dario Pompili.\\n2024a. Retrieval-augmented hierarchical in-context\\nreinforcement learning and hindsight modular reflec-\\ntions for task planning with llms. arXiv preprint\\narXiv:2408.06520.\\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.\\n2019. Pullnet: Open domain question answering\\nwith iterative retrieval on knowledge bases and text.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP), pages 2380–2390.\\nHao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan\\nHou, Yong Jiang, Pengjun Xie, Fei Huang, and Yan\\nZhang. 2025a. Zerosearch: Incentivize the search\\ncapability of llms without searching. arXiv preprint\\narXiv:2505.04588.\\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\\nWang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-\\nYeung Shum, and Jian Guo. 2024b. Think-on-graph:\\nDeep and responsible reasoning of large language\\nmodel on knowledge graph. In The Twelfth Interna-\\ntional Conference on Learning Representations.\\nQiang Sun, Tingting Bi, Sirui Li, Eun-Jung Holden,\\nPaul Duuring, Kai Niu, and Wei Liu. 2025b. Sym-\\nbioticrag: Enhancing document intelligence through\\nhuman-llm symbiotic collaboration. arXiv preprint\\narXiv:2505.02418.\\nZhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue\\nZang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang,\\nand Han Li. 2025c. Rearter: Retrieval-augmented\\nreasoning with trustworthy process rewarding. arXiv\\npreprint arXiv:2501.07861.\\nAlon Talmor and Jonathan Berant. 2018. The web as\\na knowledge-base for answering complex questions.\\nIn Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 641–651.\\nYixuan Tang and Yi Yang. 2024. Multihop-rag: Bench-\\nmarking retrieval-augmented generation for multi-\\nhop queries. arXiv preprint arXiv:2401.15391.\\nYicheng Tao, Haotian Liu, Shanwen Wang, and\\nHongteng Xu. 2025. Assisting mathematical for-\\nmalization with a learning-based premise retriever.\\narXiv preprint arXiv:2501.13959.\\nJames\\nThorne,\\nAndreas\\nVlachos,\\nChristos\\nChristodoulopoulos,\\nand\\nArpit\\nMittal.\\n2018.\\nFever: a large-scale dataset for fact extraction and\\nverification. arXiv preprint arXiv:1803.05355.\\nSM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vip-\\nula Rawte, Aman Chadha, and Amitava Das. 2024.\\nA comprehensive survey of hallucination mitigation\\ntechniques in large language models. arXiv preprint\\narXiv:2401.01313.\\nHieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang,\\nZhichao Yang, and Hong Yu. 2024. Rare: Retrieval-\\naugmented reasoning enhancement for large lan-\\nguage models. arXiv preprint arXiv:2412.02830.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022.\\nmusique: Multi-\\nhop questions via single-hop question composition.\\nTransactions of the Association for Computational\\nLinguistics, 10:539–554.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2023. Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-\\nintensive multi-step questions. In Proceedings of the\\n61st Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n10014–10037.\\nNikolaos Tsantalis, Ameya Ketkar, and Danny Dig.\\n2020. Refactoringminer 2.0. IEEE Transactions\\non Software Engineering, 48(3):930–950.\\nBoxin Wang, Wei Ping, Lawrence Mcafee, Peng Xu,\\nBo Li, Mohammad Shoeybi, and Bryan Catanzaro.\\n2024a. Instructretro: Instruction tuning post retrieval-\\naugmented pretraining. In International Conference\\non Machine Learning, pages 51255–51272. PMLR.\\n16'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao\\nGao, Kexin Huang, Ziming Liu, Payal Chandak,\\nShengchao Liu, Peter Van Katwyk, Andreea Deac,\\net al. 2023. Scientific discovery in the age of artificial\\nintelligence. Nature, 620(7972):47–60.\\nJunjie Wang, Mingyang Chen, Binbin Hu, Dan Yang,\\nZiqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jin-\\njie Gu, Jun Zhou, et al. 2024b. Learning to plan\\nfor retrieval-augmented large language models from\\nknowledge graphs. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2024, pages\\n7813–7835.\\nSong Wang, Zihan Chen, Chengshuai Shi, Cong Shen,\\nand Jundong Li. 2024c. Mixture of demonstrations\\nfor in-context learning. Advances in Neural Informa-\\ntion Processing Systems, 37:88091–88116.\\nYaoting Wang, Shengqiong Wu, Yuecheng Zhang,\\nShuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao\\nFei. 2025a.\\nMultimodal chain-of-thought reason-\\ning:\\nA comprehensive survey.\\narXiv preprint\\narXiv:2503.12605.\\nYu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi\\nZhang, and Tyler Derr. 2024d. Knowledge graph\\nprompting for multi-document question answering.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 38, pages 19206–19214.\\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,\\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\\nAaran Arulraj, Xuan He, Ziyan Jiang, et al. 2025b.\\nMmlu-pro: A more robust and challenging multi-task\\nlanguage understanding benchmark. Advances in\\nNeural Information Processing Systems, 37:95266–\\n95290.\\nYujing Wang, Hainan Zhang, Liang Pang, Binghui\\nGuo, Hongwei Zheng, and Zhiming Zheng. 2025c.\\nMaferw: Query rewriting with multi-aspect feed-\\nbacks for retrieval-augmented large language models.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 39, pages 25434–25442.\\nZheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and\\nWei Shi. 2024e. M-rag: Reinforcing large language\\nmodel performance through retrieval-augmented gen-\\neration with multiple partitions.\\nIn Proceedings\\nof the 62nd Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 1966–1978.\\nZhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen,\\nYu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang,\\nLinpeng Tang, Wentao Zhang, et al. 2025d. Rare:\\nRetrieval-augmented reasoning modeling.\\narXiv\\npreprint arXiv:2503.23513.\\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\\nHe, Zilong Zheng, Yaodong Yang, et al. 2024f.\\nJarvis-1: Open-world multi-task agents with memory-\\naugmented multimodal language models.\\nIEEE\\nTransactions on Pattern Analysis and Machine In-\\ntelligence.\\nZihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xi-\\naojian Ma, and Yitao Liang. 2024g.\\nRat:\\nRe-\\ntrieval augmented thoughts elicit context-aware rea-\\nsoning in long-horizon generation. arXiv preprint\\narXiv:2403.05313.\\nJason Wei, Nguyen Karina, Hyung Won Chung,\\nYunxin Joy Jiao, Spencer Papay, Amelia Glaese, John\\nSchulman, and William Fedus. 2024. Measuring\\nshort-form factuality in large language models. arXiv\\npreprint arXiv:2411.04368.\\nJason Wei, Zhiqing Sun, Spencer Papay, Scott McK-\\ninney, Jeffrey Han, Isa Fulford, Hyung Won Chung,\\nAlex Tachard Passos, William Fedus, and Amelia\\nGlaese. 2025a. Browsecomp: A simple yet challeng-\\ning benchmark for browsing agents. arXiv preprint\\narXiv:2504.12516.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\\net al. 2022. Chain-of-thought prompting elicits rea-\\nsoning in large language models. Advances in neural\\ninformation processing systems, 35:24824–24837.\\nJiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie\\nQiu, Wei Wei, Jinzhe Li, Wanli Ouyang, and Siqi\\nSun. 2025b. Alignrag: An adaptable framework for\\nresolving misalignments in retrieval-aware reasoning\\nof rag. arXiv preprint arXiv:2504.14858.\\nZhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi\\nShi, Zhen Huang, and Dongsheng Li. 2023. Grove: A\\nretrieval-augmented complex story generation frame-\\nwork with a forest of evidence. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2023, pages 3980–3998.\\nNirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawar-\\ndena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-\\nOrji, Ruvan Weerasinghe, Anne Liret, and Bruno\\nFleisch. 2024. Cbr-rag: case-based reasoning for\\nretrieval augmented generation in llms for legal ques-\\ntion answering. In International Conference on Case-\\nBased Reasoning, pages 445–460. Springer.\\nFeijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding,\\nand Jing Gao. 2025a. Talk to right specialists: Rout-\\ning and planning in multi-agent system for question\\nanswering. arXiv preprint arXiv:2501.07813.\\nJialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang,\\nZekun Xi, Runnan Fang, Linhai Zhang, Yulan He,\\nDeyu Zhou, Pengjun Xie, et al. 2025b. Webwalker:\\nBenchmarking llms in web traversal. arXiv preprint\\narXiv:2501.07572.\\nJunde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025c. Agen-\\ntic reasoning: Reasoning llms with tools for the deep\\nresearch. arXiv preprint arXiv:2502.04644.\\nShirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang,\\nMichihiro Yasunaga, Kaidi Cao, Vassilis Ioannidis,\\nKarthik Subbian, Jure Leskovec, and James Y Zou.\\n2024. Avatar: Optimizing llm agents for tool us-\\nage via contrastive reasoning. Advances in Neural\\nInformation Processing Systems, 37:25981–26010.\\n17'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang,\\nand Wenjie Li. 2025a.\\nTokenskip: Controllable\\nchain-of-thought compression in llms. arXiv preprint\\narXiv:2502.12067.\\nYuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, and\\nHaifeng Huang. 2025b.\\nImproving retrieval aug-\\nmented language model with self-reasoning. In Pro-\\nceedings of the AAAI conference on artificial intelli-\\ngence, volume 39, pages 25534–25542.\\nGuangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang,\\nHaolin Liu, Yifan Yang, Fangyuan Chen, Zhix-\\ning Song, Dengyu Wang, Minjia Zhang, et al.\\n2025. Rag-gym: Optimizing reasoning and search\\nagents with process supervision.\\narXiv preprint\\narXiv:2502.13957.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2024. Crp-rag: A retrieval-\\naugmented generation framework for supporting\\ncomplex logical reasoning and knowledge planning.\\nElectronics, 14(1):47.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2025a. Crp-rag: A retrieval-\\naugmented generation framework for supporting\\ncomplex logical reasoning and knowledge planning.\\nElectronics (2079-9292), 14(1).\\nRan Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C\\nHo, Haoyu Wang, and Carl Yang. 2025b. Collab-rag:\\nBoosting retrieval-augmented generation for complex\\nquestion answering via white-box and black-box llm\\ncollaboration. arXiv preprint arXiv:2504.04915.\\nChen Yang, Chenyang Zhao, Quanquan Gu, and Don-\\ngruo Zhou. 2024a. Cops: Empowering llm agents\\nwith provable cross-task experience sharing. arXiv\\npreprint arXiv:2410.16670.\\nRui Yang. 2024. Casegpt: a case reasoning framework\\nbased on language models and retrieval-augmented\\ngeneration. arXiv preprint arXiv:2407.07913.\\nWooseong Yang, Weizhi Zhang, Yuqing Liu, Yuwei\\nHan, Yu Wang, Junhyun Lee, and Philip S Yu. 2025.\\nCold-start recommendation with knowledge-guided\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2505.20773.\\nXiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,\\nXiangsen Chen, Sajal Choudhary, Rongze Gui, Ziran\\nJiang, Ziyu Jiang, et al. 2024b. Crag-comprehensive\\nrag benchmark. Advances in Neural Information\\nProcessing Systems, 37:10470–10490.\\nYahe Yang and Chengyue Huang. 2025. Tree-based\\nrag-agent recommendation system: A case study in\\nmedical test data. arXiv preprint arXiv:2501.02727.\\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\\nWikiqa: A challenge dataset for open-domain ques-\\ntion answering.\\nIn Proceedings of the 2015 con-\\nference on empirical methods in natural language\\nprocessing, pages 2013–2018.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D Manning. 2018. Hotpotqa: A dataset for\\ndiverse, explainable multi-hop question answering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n2369–2380.\\nShunyu Yao, Howard Chen, John Yang, and Karthik\\nNarasimhan. 2022. Webshop: Towards scalable real-\\nworld web interaction with grounded language agents.\\nAdvances in Neural Information Processing Systems,\\n35:20744–20757.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\\n2023a. Tree of thoughts: Deliberate problem solving\\nwith large language models.\\nAdvances in neural\\ninformation processing systems, 36:11809–11822.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.\\nReact: Synergizing reasoning and acting in language\\nmodels. In International Conference on Learning\\nRepresentations (ICLR).\\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\\nPercy Liang, and Jure Leskovec. 2021. Qa-gnn: Rea-\\nsoning with language models and knowledge graphs\\nfor question answering. In North American Chap-\\nter of the Association for Computational Linguistics\\n(NAACL).\\nJaeseok Yoo, Hojae Han, Youngwon Lee, Jaejin Kim,\\nand Seung-won Hwang. 2025. Perc: Plan-as-query\\nexample retrieval for underrepresented code genera-\\ntion. In Proceedings of the 31st International Con-\\nference on Computational Linguistics, pages 7982–\\n7997.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\\nBerant. 2024. Making retrieval-augmented language\\nmodels robust to irrelevant context. In ICLR 2024\\nWorkshop on Large Language Model (LLM) Agents.\\nHong Qing Yu and Frank McQuade. 2025. Rag-kg-il:\\nA multi-agent hybrid framework for reducing halluci-\\nnations and enhancing llm reasoning through rag and\\nincremental knowledge graph learning integration.\\narXiv preprint arXiv:2503.13514.\\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Peixin\\nCao, Kaixin Ma, Jian Li, Hongwei Wang, and Dong\\nYu. 2024. Chain-of-note: Enhancing robustness in\\nretrieval-augmented language models. In Proceed-\\nings of the 2024 Conference on Empirical Methods in\\nNatural Language Processing, pages 14672–14685.\\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie\\nTang, Cuiping Li, and Hong Chen. 2022a. Subgraph\\nretrieval enhanced model for multi-hop knowledge\\nbase question answering. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 5773–\\n5784.\\n18'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang,\\nDongjie Wang, and Kunpeng Liu. 2025a. Ratt: A\\nthought structure for coherent and correct llm reason-\\ning. In Proceedings of the AAAI Conference on Arti-\\nficial Intelligence, volume 39, pages 26733–26741.\\nJintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo,\\nShuofei Qiao, Lun Du, Da Zheng, Huajun Chen,\\nand Ningyu Zhang. 2025b.\\nLightthinker: Think-\\ning step-by-step compression.\\narXiv preprint\\narXiv:2502.15589.\\nMiao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang,\\nShuai Lu, Junfeng Jiao, and Tianyu Shi. 2025c. A\\ncascading cooperative multi-agent framework for on-\\nramp merging control integrating large language mod-\\nels. arXiv preprint arXiv:2503.08199.\\nNingning Zhang, Chi Zhang, Zhizhong Tan, Xingxing\\nYang, Weiping Deng, and Wenyong Wang. 2025d.\\nCredible plan-driven rag method for multi-hop ques-\\ntion answering. arXiv preprint arXiv:2504.16787.\\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gon-\\nzalez. 2024a. Raft: Adapting language model to do-\\nmain specific rag. In First Conference on Language\\nModeling.\\nWeizhi\\nZhang,\\nYuanchen\\nBei,\\nLiangwei\\nYang,\\nHenry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui\\nLi, Hao Chen, Jianling Wang, Yu Wang, et al. 2025e.\\nCold-start recommendation towards the era of large\\nlanguage models (llms): A comprehensive survey\\nand roadmap. arXiv preprint arXiv:2501.01945.\\nWeizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo,\\nGuancheng Wan, Liangwei Yang, Chenxuan Xie,\\nYuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al.\\n2025f. From web search towards agentic deep re-\\nsearch: Incentivizing search with reasoning agents.\\narXiv preprint arXiv:2506.18959.\\nWeizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liang-\\nwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng\\nZou, Zijie Huang, Zhengyang Wang, Yifan Gao,\\net al. 2025g. Personaagent: When large language\\nmodel agents meet personalization at test time. arXiv\\npreprint arXiv:2506.06254.\\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\\nHongyu Ren, Percy Liang, Christopher D Manning,\\nand Jure Leskovec. 2022b. Greaselm: Graph rea-\\nsoning enhanced language models. In International\\nConference on Learning Representations.\\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang\\nXu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai,\\nShuo Wang, Zhiyuan Liu, et al. 2024b. ∞bench:\\nExtending long context evaluation beyond 100k to-\\nkens. In Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 15262–15277.\\nYusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister,\\nRui Zhang, and Sercan Arik. 2024c. Chain of agents:\\nLarge language models collaborating on long-context\\ntasks. Advances in Neural Information Processing\\nSystems, 37:132208–132237.\\nZhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang\\nShi, Meng Han, Yongkang Wu, Ruofei Lai, and Zhao\\nCao. 2023. Iag: Induction-augmented generation\\nframework for answering reasoning questions. arXiv\\npreprint arXiv:2311.18397.\\nSiyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He,\\nLuna K Qiu, and Lili Qiu. 2024a. Retrieval aug-\\nmented generation (rag) and beyond: A comprehen-\\nsive survey on how to make your llms use external\\ndata more wisely. arXiv preprint arXiv:2409.14924.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\\nsurvey of large language models.\\narXiv preprint\\narXiv:2303.18223, 1(2).\\nXiaoyan Zhao, Lingzhi Wang, Zhanghao Wang, Hong\\nCheng, Rui Zhang, and Kam-Fai Wong. 2024b.\\nPacar: Automated fact-checking with planning and\\ncustomized action reasoning using large language\\nmodels.\\nIn Proceedings of the 2024 Joint In-\\nternational Conference on Computational Linguis-\\ntics, Language Resources and Evaluation (LREC-\\nCOLING 2024), pages 12564–12573.\\nXinping Zhao, Dongfang Li, Yan Zhong, Boren Hu,\\nYibin Chen, Baotian Hu, and Min Zhang. 2024c.\\nSeer: Self-aligned evidence extraction for retrieval-\\naugmented generation. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 3027–3041.\\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu.\\n2021. Minif2f: a cross-system benchmark for for-\\nmal olympiad-level mathematics.\\narXiv preprint\\narXiv:2109.00110.\\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai,\\nLyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025.\\nDeepresearcher: Scaling deep research via reinforce-\\nment learning in real-world environments. arXiv\\npreprint arXiv:2504.03160.\\nJiawei Zhou and Lei Chen. 2025. Openrag: Optimiz-\\ning rag end-to-end via in-context retrieval learning.\\narXiv preprint arXiv:2503.08398.\\nPeilin Zhou, Bruce Leon, Xiang Ying, Can Zhang,\\nYifan Shao, Qichen Ye, Dading Chong, Zhiling\\nJin, Chenxuan Xie, Meng Cao, et al. 2025a.\\nBrowsecomp-zh: Benchmarking web browsing abil-\\nity of large language models in chinese.\\narXiv\\npreprint arXiv:2504.19314.\\nYifei Zhou, Song Jiang, Yuandong Tian, Jason Weston,\\nSergey Levine, Sainbayar Sukhbaatar, and Xian Li.\\n2025b.\\nSweet-rl: Training multi-turn llm agents\\non collaborative reasoning tasks.\\narXiv preprint\\narXiv:2503.15478.\\n19'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen,\\nBinyu Xie, Zheng Liu, Ji-Rong Wen, and Zhicheng\\nDou. 2024. Inters: Unlocking the power of large\\nlanguage models in search with instruction tuning.\\nIn Proceedings of the 62nd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), pages 2782–2809.\\nHenry Peng Zou, Wei-Chieh Huang, Yaozu Wu,\\nYankai Chen, Chunyu Miao, Hoang Nguyen, Yue\\nZhou, Weizhi Zhang, Liancheng Fang, Langzhou\\nHe, et al. 2025.\\nA survey on large language\\nmodel based human-agent systems. arXiv preprint\\narXiv:2505.00753.\\n20'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='A\\nFull Benchmark\\nSection 6 introduces representative benchmarks for\\ndifferent RAG-reasoning tasks. This appendix com-\\nplements that discussion with a comprehensive list\\nof benchmarks organized by task and domain. Ta-\\nble 2 details each benchmark’s attributes, including\\nthe publication venue, code repository, task cate-\\ngory, domain, primary knowledge sources, knowl-\\nedge type, and reasoning capabilities. By consoli-\\ndating these attributes into a single table, we facili-\\ntate the selection and comparison of benchmarks,\\nenabling researchers to identify the most suitable\\ndatasets for future studies on RAG-enhanced rea-\\nsoning.\\nOur benchmark compilation is primarily derived\\nfrom the methods surveyed in Sections 3 to 5 of\\nthis paper, with a particular focus on synergized\\napproaches discussed in Section 5. We deliber-\\nately targeted benchmarks that require both exter-\\nnal knowledge retrieval and internal deep reason-\\ning, as this dual requirement reflects real-world\\nscenarios where models must not only access rel-\\nevant information but also integrate and reason\\nover it effectively. For example, in the QA do-\\nmain, we include datasets that necessitate synthe-\\nsizing evidence across multiple documents to an-\\nswer questions that cannot be resolved through\\nsingle-sentence retrieval. HotpotQA (Yang et al.,\\n2018) exemplifies this challenge, requiring reason-\\ning across different Wikipedia articles. In coding\\ntasks, benchmarks such as LiveCodeBench (Jain\\net al., 2024) and Refactoring Oracle (Tsantalis et al.,\\n2020) extend beyond pure algorithmic problem-\\nsolving by demanding retrieval of external code\\nsnippets and documentation. Similarly, in mathe-\\nmatics, benchmarks like MATH (Hendrycks et al.,\\n2021) and AQUA-RAT (Das et al., 2024) assess\\nnot only computational proficiency but also the re-\\ntrieval of relevant theorems and formulas, testing\\nthe model’s ability to integrate external mathemati-\\ncal knowledge with internal reasoning processes.\\nIn addition to established benchmarks, we have\\nincorporated newer and more challenging datasets\\nthat better mirror real-world applications. These\\ndatasets often demand extensive retrieval processes\\ncombined with expert-level or domain-specific\\nreasoning, as seen in Humanity’s Last Exam\\n(HLE) (Phan et al., 2025) and web search evalua-\\ntion tasks like BrowseComp (Wei et al., 2025a).\\nOverall, our collection encompasses 46 bench-\\nmarks covering 13 distinct tasks across 12 domains,\\neach explicitly annotated with features such as\\nknowledge source, knowledge type, and reasoning\\ncapacity. This breadth ensures coverage of diverse\\ndomains and task types, forming a solid foundation\\nfor evaluating the interplay between retrieval and\\nreasoning in RAG systems.\\nWithin this benchmark set, single-hop QA\\ndatasets like TriviaQA (Joshi et al., 2017) focus on\\nprecise retrieval and fact recall, requiring models\\nto locate and synthesize a single piece of evidence.\\nIn contrast, multi-hop QA benchmarks such as Hot-\\npotQA (Yang et al., 2018) and MuSiQue (Trivedi\\net al., 2022) challenge models to chain information\\nfrom multiple documents and employ deductive\\nreasoning to bridge disparate facts into coherent\\nanswers. Structured knowledge benchmarks, such\\nas GraphQA (He et al., 2024c), require reasoning\\nover relational graph representations, integrating\\nnodes and edges to resolve complex queries be-\\nyond plain text retrieval. Complementing these\\nopen-ended tasks, multiple-choice evaluations like\\nMMLU-Pro (Wang et al., 2025b) test domain-\\nspecific knowledge in areas such as science, history,\\nor law, assessing the model’s ability to perform\\nvarious reasoning styles, including inductive and\\nabductive inference. Multimodal QA benchmarks,\\nlike WebShop (Yao et al., 2022), test a model’s\\ncapacity to align textual and visual information\\nto determine the correct answer. Long-form QA\\ndatasets such as ∞BENCH (Zhang et al., 2024b)\\nevaluate models’ ability to maintain logical consis-\\ntency and perform inductive reasoning over lengthy\\ncontexts. Collectively, these benchmarks establish\\na comprehensive evaluation chain for systemati-\\ncally assessing RAG-reasoning capabilities.\\nBeyond text-based QA, RAG-augmented bench-\\nmarks span diverse tasks involving long-form\\ngeneration, interactive reasoning, and domain-\\nspecific challenges in mathematics and pro-\\ngramming.\\nMathematics benchmarks such as\\nMATH (Hendrycks et al., 2021) draw from\\ncompetition-level problems to assess arithmetic\\nand symbolic reasoning.\\nSummarization tasks\\nlike XSum (Narayan et al., 2018) evaluate a\\nmodel’s ability to condense entire news articles\\ninto concise summaries while preserving factual\\ncorrectness.\\nFact-checking benchmarks, such\\nas FEVER (Thorne et al., 2018), test the ca-\\npacity for evidence retrieval and claim verifica-\\ntion. Code-focused evaluations, including Live-\\nCodeBench (Jain et al., 2024), examine deductive\\nand abductive reasoning in the context of algo-\\n21'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Dataset\\nVenue\\nResource\\nTask\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning Capability\\nSize\\nInput\\nOutput\\nCode\\nLiveCodeBench (Jain\\net al., 2024)\\nArxiv’24\\nLink\\nCode\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n1,055\\nQuestion/Text, Code,\\nInstruction\\nCode Instance, Test\\nOutput\\nRefactoring Oracle\\n(Tsantalis et al., 2020)\\nIEEE’22\\nLink\\nCode\\nSoftware\\nInternet, Human\\nLogical\\nDeductive\\n7,226\\nCode, Instruction\\nCode Instance\\nColBench (Zhou et al.,\\n2025b)\\nArxiv’25\\nLink\\nCode\\nSoftware\\nLLM, Human\\nLogical\\nAbductive, Inductive\\n10,000+\\nQuestion/Text,\\nLinks/Sources, Code\\nCode Instance\\nMath\\nMATH (Hendrycks\\net al., 2021)\\nNeurIPS’21\\nLink\\nDomain-specific\\nQA\\nMath\\nExam/Competition\\nLogical, Arithmetic\\nDeductive\\n12,500\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nMiniF2F (Zheng et al.,\\n2021)\\nICLR’22\\nLink\\nDomain-specific\\nQA\\nMath\\nExam/Competition,\\nBooks\\nLogical, Arithmetic\\nDeductive\\n488\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nAQuA (Ling et al.,\\n2017)\\nArxiv’17\\nLink\\nDomain-specific\\nQA\\nMath\\nPrevious Source,\\nExam/Competition,\\nInternet\\nArithmetic, Logical\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equations\\nNatural Language,\\nOptions/Labels\\nFact Checking\\nCRAG (Yang et al.,\\n2024b)\\nNeurIPS’24\\nLink\\nFact Checking\\nGeneral\\nInternet\\nCommonsense\\nDeductive, Abductive\\n4,409\\nQuestion/Text\\nNatural Language\\nCREAK (Onoe et al.,\\n2021)\\nNeurIPS’21\\nLink\\nFact Checking\\nGeneral\\nHuman\\nCommonsense\\nDeductive, Abductive,\\nAnalogical\\n13,000\\nQuestion/Text\\nOptions/Labels,\\nNatural Language\\nFever (Thorne et al.,\\n2018)\\nACL’18\\nLink\\nFact Checking\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n185,445\\nQuestion/Text,\\nLinks/Sources\\nNatural Language,\\nOptions/Labels\\nPubHealth (Kotonya\\nand Toni, 2020)\\nEMNLP’20\\nLink\\nFact Checking\\nHealth\\nInternet\\nCommonsense,\\nLogical\\nAbductive, Deductive\\n11,800\\nQuestion/Text\\nNatural Language,\\nOptions\\nGraph QA\\nGraphQA (He et al.,\\n2024c)\\nNeurIPS’24\\nLink\\nGraph QA\\nGeneral\\nPrevious Source\\nCommonsense,\\nMultimodal\\nDeductive, Abductive\\n107,503\\nQuestion/Text\\nNatural Language\\nGRBENCH (Jin et al.,\\n2024)\\nACL’24\\nLink\\nGraph QA\\nGeneral\\nLLM, Human\\nLogical\\nDeductive, Inductive\\n1,740\\nQuestion/Text\\nNatural Language\\nLong-form QA\\n∞BENCH (Zhang\\net al., 2024b)\\nArxiv’24\\nLink\\nLong-form QA\\nGeneral\\nInternet, Human\\nMultimodal, Logical Inductive, Abductive\\n3,946\\nQuestion/Text, Code,\\nEquations\\nNatural Language,\\nNumber, Code\\nInstance\\nMultimodal QA\\nCrisisMMD (Alam\\net al., 2018)\\nArxiv’18\\nLink\\nMultimodal QA Crisis Response\\nMedia, Internet\\nCommonsense,\\nMultimodal\\nAbductive\\n16,097\\nQuestion/Text,\\nFigure/Image\\nOptions, Natural\\nLanguage\\nALFWORLD (Shridhar\\net al.)\\nICLR’21\\nLink\\nMultimodal QA\\nGame\\nPrevious Source\\nMultimodal\\nDeductive, Abductive\\n3,827\\nQuestion/Text,\\nFigure/Image\\nNatural Language\\nMMLongBench-DOC\\n(Ma et al., 2025)\\nNeurIPS’24\\nLink\\nMultimodal QA\\nNarrative\\nPrevious Source,\\nInternet\\nMultimodal\\nDeductive, Abductive\\n1,082\\nFigure/Image,\\nQuestion/Text,\\nDocuments\\nNatural Language,\\nNumber\\nLongDocURL (Deng\\net al., 2024)\\nArxiv’24\\nLink\\nMultimodal QA\\nNarrative\\nInternet, Previous\\nSource, LLM\\nMultimodal\\nDeductive, Abductive\\n2,325\\nFigure/Image,\\nQuestion/Text,\\nDocuments\\nNatural Language,\\nNumber\\nUDA (Hui et al., 2024)\\nNIPS’24\\nLink\\nMultimodal QA\\nNarrative\\nInternet,\\nPaper/Report\\nMultimodal\\nDeductive\\n29,590\\nDocuments,\\nQuestion/Text\\nNatural Language,\\nNumber\\nSCIENCEQA (Lu et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nScience\\nHuman\\nLogical, Multimodal\\nDeductive\\n21,000\\nQuestion/Text,\\nOptions,\\nFigure/Image\\nOptions, Natural\\nLanguage, Number\\nWebShop (Yao et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nE-commerce\\nInternet\\nMultimodal\\nInductive, Abductive\\n12,087\\nInstruction,\\nQuestion/Text\\nNatural Language,\\nImage/Figure\\nSurgCoTBench (Low\\net al., 2025)\\nArxiv’25\\n—\\nMultimodal QA\\nHealth\\nHuman\\nMultimodal, Logical Abductive, Deductive\\n14,176\\nQuestion/Text,\\nFigure/Image,\\nOptions\\nOptions, Natural\\nLanguage, Number\\nTable 2: Full representative knowledge and reasoning intensive benchmarks across diverse task categories (Part 1).\\n22'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Dataset\\nVenue\\nResource\\nTask\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning Capability\\nSize\\nInput\\nOutput\\nMulti-choice QA\\nBamboogle (Press et al.,\\n2023)\\nEMNLP’23\\nLink\\nMulti-choice QA\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n125\\nQuestion/Text\\nNatural Language\\nBIG-Bench (Srivastava\\net al., 2022)\\nArxiv’22\\nLink\\nMulti-choice QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive, Abductive,\\nInductive, Analogical\\n204\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber,\\nOptions/Labels\\nADQA (Li et al.,\\n2024a)\\nEMNLP’24\\nLink\\nMulti-choice QA\\nHealth\\nPrevious Source\\nCommonsense,\\nLogical\\nDeductive, Abductive\\n446\\nQuestion/Text,\\nOptions\\nOptions\\nQuALITY (Pang et al.,\\n2022)\\nNAACL’22\\nLink\\nMulti-choice QA\\nNarrative\\nBooks\\nCommonsense,\\nLogical\\nDeductive, Abductive\\n6,737\\nQuestion/Text,\\nOptions\\nOptions\\nMMLU-Pro (Wang\\net al., 2025b)\\nNeurIPS’24\\nLink\\nMulti-choice QA\\nScience\\nPrevious Source,\\nInternet\\nArithmetic,\\nCommonsense,\\nLogical\\nDeductive, Inductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options\\nMulti-hop QA\\nFRAMES (Krishna\\net al., 2024)\\nArxiv’24\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical, Arithmetic\\nDeductive\\n824\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al.,\\n2018)\\nEMNLP’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nGPQA (Rein et al.,\\n2024)\\nArxiv’24\\nLink\\nMulti-hop QA\\nScience\\nHuman\\nLogical\\nDeductive, Abductive\\n448\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options\\nHLE (Phan et al., 2025)\\nArxiv’25\\nLink\\nMulti-hop QA\\nScience\\nHuman\\nLogical, Arithmetic,\\nMultimodal\\nDeductive, Abductive\\n2,500\\nQuestion/Text,\\nOptions,\\nFigure/Image\\nNatural Language,\\nNumber, Options\\nCWQ (Talmor and\\nBerant, 2018)\\nNAACL’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n34,689\\nQuestion/Text\\nNatural Language\\nIIRC (Ferguson et al.,\\n2020)\\nEMNLP’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n13,000+\\nQuestion/Text,\\nLinks/Sources\\nNumber, Natural\\nLanguage\\nMINTQA (He et al.,\\n2024b)\\nArxiv’24\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n10,479\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al.,\\n2022)\\nACL’22\\nLink\\nMulti-hop QA\\nGeneral\\nPrevious Source,\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n25,000\\nQuestion/Text\\nNatural Language\\nTopiOCQA (Adlakha\\net al., 2022)\\nTACL’22\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n54,494\\nQuestion/Text\\nNatural Language\\n2WikiMultiHopQA (Ho\\net al., 2020)\\nCOLING’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nMulti-step QA\\nStrategyQA (Geva\\net al., 2021)\\nTACL’21\\nLink\\nMulti-step QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n2,780\\nQuestion/Text\\nNatural Language\\nSingle-hop QA\\nSimpleQA (Wei et al.,\\n2024)\\nArxiv’24\\nLink\\nSingle-hop QA\\nGeneral\\nLLM, Human\\nCommonsense\\nDeductive\\n4,326\\nQuestion/Text\\nNatural Language\\nTriviaQA (Joshi et al.,\\n2017)\\nACL’17\\nLink\\nSingle-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n650,000+\\nQuestion/Text\\nNatural Language\\nNQ (Kwiatkowski et al.,\\n2019)\\nACL’19\\nLink\\nSingle-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n307,373\\nQuestion/Text\\nNatural Language\\nText Summarization\\nXSum (Narayan et al.,\\n2018)\\nEMNLP’18\\nLink\\nText\\nSummarization\\nNarrative\\nInternet, Media\\nLogical,\\nCommonsense\\nAbductive\\n226,711\\nQuestion/Text\\nNatural Language\\nBIGPATENT (Sharma\\net al., 2019)\\nACL’19\\nLink\\nText\\nSummarization\\nPatent\\nInternet\\nCommonsense,\\nLogical\\nAbductive\\n1.3 M\\nQuestion/Text\\nNatural Language\\nWeb Browsing\\nBrowseComp (Wei\\net al., 2025a)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, Internet\\nCommonsense,\\nLogical\\nDeductive\\n1,266\\nQuestion/Text\\nNatural Language\\nBrowseComp-ZH\\n(Zhou et al., 2025a)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, Internet\\nCommonsense,\\nLogical\\nDeductive\\n289\\nQuestion/Text\\nNatural Language\\nGAIA (Mialon et al.,\\n2023)\\nICLR’23\\nLink\\nWeb Browsing\\nGeneral\\nInternet, TooL\\nCommonsense,\\nLogical\\nDeductive\\n466\\nQuestion/Text,\\nImage/File/Code\\nNatural Language\\nWebWalkerQA (Wu\\net al., 2025b)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, LLM\\nCommonsense,\\nLogical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nDialog\\nDailyDialog (Li et al.,\\n2017)\\nArxiv’17\\nLink\\nDialog\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\n–\\n13,118\\nQuestion/Text\\nNatural Language\\nTable 3: Full epresentative knowledge and reasoning intensive benchmarks across diverse task categories (Part 2,\\ncontinued).\\n23'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Benchmark\\nDomain\\nPrimary Retrieval Challenge\\nPrimary Reasoning Challenge\\nTriviaQA, NQ\\nGeneral\\nScale & Noise: Retrieval from massive, noisy cor-\\npora.\\nAmbiguity: Handling real-world queries that are of-\\nten underspecified or ambiguous.\\nHotpotQA,\\n2WikiMultiHopQA,\\nMuSiQue, HLE\\nGeneral\\nMulti-document / High-dependency Synthesis: Re-\\nquires finding and connecting evidence scattered\\nacross multiple Wikipedia articles.\\nMulti-hop Deduction: Explicitly designed to test\\nthe ability to link two or more discrete facts into a\\ncoherent reasoning path.\\nMMLU-Pro, QUALITY\\nScience, Narrative\\nExpert-level Retrieval: Requires accessing deep spe-\\ncialized knowledge from academic or densely written\\nnarrative sources.\\nComplex & Long-form Reasoning: MMLU-Pro de-\\nmands expert-level problem-solving over rote memo-\\nrization. QUALITY uniquely requires comprehension\\nof very long texts (often >5,000 tokens).\\nMATH, AQUA-RAT\\nMath\\nFormal Knowledge Retrieval: Locating precise\\nmathematical theorems, lemmas, or formulas in for-\\nmal corpora.\\nSymbolic & Deductive Reasoning: Involves per-\\nforming precise, multi-step logical and algebraic oper-\\nations where each step must be correct. AQUA-RAT\\nis unique in providing natural language rationales,\\nthus testing the model’s ability to explain its formal\\nreasoning.\\nLiveCodeBench\\nCode\\nStructural & Modal Heterogeneity: Must retrieve\\nfrom diverse, heterogeneous sources such as code\\nrepositories, documentation, and community forums\\nlike Stack Overflow.\\nTool Use & Self-correction Reasoning: Requires ap-\\nplying retrieved code snippets/APIs, executing code,\\nand reasoning based on test outputs to debug and iter-\\natively improve solutions.\\nBrowseComp,\\nWebWalkerQA\\nGeneral (Web)\\nDynamism, Interactivity, and Long-tail Retrieval:\\nTests agentic planning and tool use in live, unstruc-\\ntured web environments. BrowseComp requires cre-\\native, persistent navigation to locate hard-to-find, in-\\ntertwined information, while WebWalkerQA focuses\\non systematic traversal of a website’s subpages.\\nAgentic & Strategic Reasoning: Requires planning\\nand executing multi-step strategies (e.g., searching,\\nclicking, extracting) in dynamic and unpredictable\\ncontexts to achieve a defined goal.\\nTable 4: The primary retrieval and reasoning challenges for different RAG-Reasoning benchmarks.\\nrithmic problem-solving. Web-based tasks, exem-\\nplified by BrowseComp (Wei et al., 2025a), emu-\\nlate real-world search behavior, requiring iterative\\nquery formulation and navigation across multiple\\nwebpages.\\nIn addition to cataloging datasets, Table 4 pro-\\nvides a synthesized overview of the primary re-\\ntrieval and reasoning challenges associated with\\neach benchmark discussed in this survey. This\\ncomparative analysis reveals critical gaps in cur-\\nrent benchmark coverage that future research must\\naddress. From a domain perspective, most bench-\\nmarks still focus on a limited set of general or\\nacademic scenarios, with few tackling real-world,\\nrealistic industrial or vertical-domain tasks where\\nretrieval sources might be personalized, proprietary\\nor highly specialized. Regarding retrieval capa-\\nbilities, existing benchmarks rarely test systems’\\nability to handle heterogeneous or multimodal con-\\ntent, nor do they systematically evaluate robust-\\nness against noisy, evolving, or conflicting infor-\\nmation within a unified framework for trustworthi-\\nness. In terms of reasoning capabilities, current\\nbenchmarks primarily assess deductive reasoning,\\nleaving underexplored more complex forms such\\nas deep causal reasoning, counterfactual thinking,\\ndecision-oriented reasoning, or analogical reason-\\ning in specialized domains. Moreover, there is a\\nlack of standardized benchmarks and metrics for\\nevaluating the entire reasoning-retrieval trajectory,\\nincluding the efficiency of retrieval steps, the qual-\\nity of intermediate queries, and the logical consis-\\ntency of multi-step reasoning chains.\\nB\\nDeep Research Implementations\\nIn this section, we extend the discussion of the\\nagentic paradigm introduced in Section 5.2, in\\nwhich RAG systems adopt the role of active re-\\nsearchers who plan multistep queries, interleave\\nretrieval with reasoning, and coordinate specialized\\ntools or agents. These characteristics collectively\\ndefine what we refer to as deep research, represent-\\ning the ability of a system to autonomously break\\ndown complex questions, iteratively gather diverse\\nevidence, and synthesize information through mul-\\ntiple reasoning steps. This paradigm seeks to en-\\nhance autonomy, reduce hallucinations, and im-\\nprove factual accuracy in open-domain tasks.\\nSuch deep research systems can be realized\\nthrough either single-agent or multi-agent archi-\\ntectures. Single-agent systems rely on a single\\nmodel to manage the entire process of question\\ndecomposition, retrieval, and synthesis, offering\\nsimplicity and shared context but facing limita-\\ntions in handling highly specialized or multi-modal\\ntasks. In contrast, multi-agent systems distribute\\nthese responsibilities among specialized agents, en-\\nabling modularity and potentially greater robust-\\nness. However, this collaborative design introduces\\n24'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Name\\nBase Model\\nOptimizationReward\\nRetriever\\nAgent\\nArchitecture\\nTrain Data\\nEvaluation Data\\nLink\\nAgentic Reasoning\\n(Wu et al., 2025c)\\nN/A\\nPrompting\\nN/A\\nWeb Search\\nCentralized\\nN/A\\nGPQA\\nLink\\ngpt-researcher\\nPrompting\\nN/A\\nWeb Search,\\nLocal Retrieval\\nCentralized\\nN/A\\nN/A\\nLink\\ndeep-searcher\\nDeepseek, , Claude,\\nGemini, Qwen\\nPrompting\\nN/A\\nWeb Search\\nHierarchical\\nN/A\\nN/A\\nLink\\nSearch-R1 (Jin\\net al., 2025)\\nQwen2.5-7B-Instruct,\\nQwen2.5-7B-Base,\\nQwen-2.5-3B-Instruct,\\nQwen-2.5-3B-Base\\nGRPO,\\nPPO\\nExact\\nMatch\\nWeb Search\\nSingle\\nNQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nZeroSearch (Sun\\net al., 2025a)\\nQwen2.5-3B-Base,\\nQwen2.5-7B-Base,\\nQwen2.5-7B-Instruct,\\nQwen2.5-3B-Instruct,\\nLLaMA3.2-3B-Instruct,\\nLLaMA3.2-3B-Base\\nGRPO,\\nPPO,\\nReinforce\\nExact\\nMatch\\nWeb Search\\nSingle\\nNQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nWebthinker (Li\\net al., 2025c)\\nGPT-o1, GPT-o3,\\nDeepseek-R1, QwQ-32B,\\nQwen2.5-32B-Instruct\\nDPO\\nPreference\\nPairs\\nWeb Search\\nSingle\\nSuperGPQA,\\nWebWalkerQA,\\nOpenThoughts,\\nNaturalReasoning,\\nNuminaMath\\nGPQA, GAIA, WebWalkerQA,\\nHumanity’s Last Exam\\nLink\\nnanoDeepResearch\\nOpenAI series, Claude\\nPrompting\\nN/A\\nWeb Search\\nCentralized\\nN/A\\nN/A\\nLink\\nDeerFlow\\nQwen,\\nPrompting\\nN/A\\nWeb Search\\nDecentralized\\nN/A\\nN/A\\nLink\\ndeep-research\\nDeepseek,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nopen-deep-research\\nOpenAI series, Deepseek,\\nClaude, Gemini\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nDeepResearcher\\n(Zheng et al., 2025)\\nQwen2.5-7B-Instruct\\nGRPO\\nFormat\\nWeb Search\\nDecentralized\\nNQ, TQ, HotpotQA,\\n2WikiMultiHopQA\\nMuSiQue, Bamboogle, PopQA, NQ,\\nTQ, HotpotQA, 2WikiMultiHopQA\\nLink\\nR1-Searcher (Song\\net al., 2025)\\nQwen2.5-7B-Base,\\nLlama3.1-8B-Instruct\\nGRPO, Re-\\ninforce++,\\nSFT\\nRetrieval,\\nFormat\\nWeb Search,\\nLocal Retrieval\\nSingle\\nHotpotQA,\\n2WikiMultiHopQA\\nHotpotQA, 2WikiMultiHopQA,\\nMuSiQue, Bamboogle\\nLink\\nReSearch (Chen\\net al., 2025a)\\nQwen2.5-7B-Instruct,\\nQwen2.5-32B-Instruct\\nGRPO\\nFormat,\\nAnswer\\nWeb Search\\nSingle\\nMuSiQue\\nHotpotQA, 2WikiMultiHopQA,\\nMuSiQue, Bamboogle\\nLink\\nSearch-o1 (Li et al.,\\n2025b)\\nQwQ-32B-Preview\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nGPQA, MATH500, AMC2023,\\nAIME2024, LiveCodeBench, Natural\\nQuestions, TriviaQA, HotpotQA,\\n2Wiki, MuSiQue, Bamboogle\\nLink\\nr1-reasoning-rag\\nDeepseek\\nPrompting\\nN/A\\nLocal Retrieval,\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nOpen Deep Search\\n(Alzubi et al., 2025)\\nLlama3.1-70B,\\nDeepseek-R1\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nSimpleQA, FRAME\\nLink\\nnode-DeepResearch\\nGemini,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\ndeep-research\\nGemini, OpenAI series,\\nDeepseek, Claude, Grok\\nPrompt\\nN/A\\nLocal Retrieval,\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nTable 5: Overview of deep research implementations.\\nadditional complexity in coordination and commu-\\nnication, as well as higher computational costs.\\nAlongside these developments in agent orches-\\ntration, the nature of retrievers used in deep re-\\nsearch has also evolved significantly. Early RAG\\nsystems relied on sparse keyword-based retrieval,\\nlater surpassed by dense retrievers employing bi-\\nencoder architectures for semantic matching. More\\nrecent deep research systems increasingly integrate\\nweb search-based retrievers, allowing real-time ac-\\ncess to open-domain information. Some retrievers\\nhave also been transformed into LLM-callable tools\\nfor flexible invocation. This evolution of retrievers\\nhas played a crucial role in enabling the sophisti-\\ncated information-gathering processes required for\\ndeep research.\\nC\\nComparison of Reasoning Workflows\\nand Agent Orchestration Strategies\\nTable 6 summarizes the diverse reasoning work-\\nflows and agent orchestration strategies employed\\nin Synergized RAG-Reasoning systems, highlight-\\ning their respective strengths, limitations, and suit-\\nable application scenarios. Reasoning workflows\\nvary from linear chain-based approaches, which\\nare efficient but vulnerable to error propagation, to\\nmore complex tree-based and graph-based methods\\nthat offer higher recall and transparency at the cost\\nof increased computational overhead. Similarly,\\nagent orchestration strategies range from single-\\nagent setups to multi-agent systems that distribute\\nspecialized roles among agents, enhancing robust-\\nness and scalability. However, these advanced de-\\nsigns often introduce additional communication\\noverhead and complexity in conflict resolution.\\nThis comparison illustrates the trade-offs inherent\\nin choosing particular workflows or orchestration\\narchitectures and underscores the need for adaptive\\nsystems that can dynamically balance efficiency,\\naccuracy, and resource constraints in real-world\\napplications.\\n25'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Category\\nSub-category\\nStrengths\\nLimitations\\nSuitable Scenarios\\nReasoning\\nWorkflow\\nChain-based\\nOne retrieval per reasoning step; low\\nlatency and token cost. Easy to cache\\nand monitor.\\nAn early wrong sub-query propagates;\\ncontext grows fast on long chains.\\nSingle-hop or short multi-hop QA\\nwhere each intermediate fact is easy to\\naccess.\\nTree-based (ToT)\\nHigh recall: explores multiple\\nbranches in parallel, hedges against\\nearly errors. Transparent what-if traces.\\nQuadratic cost; tree branches require\\nmany retrieval calls.\\nAmbiguous or “multiple plausible\\npaths” tasks (e.g., HotpotQA, legal\\nreasoning) where missing one clue kills\\naccuracy.\\nTree-based\\n(MCTS)\\nBudget-aware exploration: focuses\\ncalls on promising branches; graceful\\nanytime stopping.\\nTuning-heavy and may converge to a\\nsuboptimal subtree.\\nDeep-search problems under tight\\nAPI-call or token budgets (e.g.,\\nbiomedical QA).\\nGraph-based\\n(Walk-on-Graph)\\nEfficient in explicit KG/document\\ngraphs; short reasoning paths on KGs.\\nRequires high-quality KGs; fails if\\ngraphs lack explicit edges; less flexible\\nfor open-web contexts.\\nEnterprise or domain-specific QA\\nwhere a curated KG exists (e.g.,\\nproduct catalogs).\\nGraph-based\\n(Think-on-Graph)\\nAdaptive and verifiable; LLM updates\\na live evidence graph, allowing\\nnode-level citation checks and high\\nfactual accuracy.\\nHigher latency; many micro-tool calls;\\nsearch space can explode without\\npruning.\\nOpen-domain “deep research” or\\nfact-dense synthesis tasks (e.g.,\\nBrowseComp, systematic reviews).\\nAgent\\nOrchestration\\nSingle-agent\\n(Prompt-only)\\nSimple implementation via a ReAct\\nloop; low resource overhead.\\nConstrained by prompt engineering\\nand system design flexibility.\\nPrototyping demos and small-scale\\napplications where simplicity\\noutweighs performance.\\nSingle-agent\\n(SFT)\\nClear, well-defined RAG and\\nreasoning patterns; higher precision\\nthan prompt-only approaches.\\nRequires large synthetic data; may\\noverfit tool schemas, reducing\\nout-of-domain generalization.\\nProduction chatbots with stable APIs\\nand predictable query formats (e.g.,\\ninternal customer support).\\nSingle-agent (RL)\\nAdaptive RAG and reasoning yields\\nhigh recall and accuracy; learns when\\nto retrieve and reason.\\nChallenging to define suitable reward\\nsignals; computationally expensive to\\ntrain.\\nOpen-domain research or long-form\\nQA where call costs are high and\\noptimal stop conditions matter.\\nMulti-agent\\n(Decentralized)\\nHigh recall via parallel domain\\nexperts; robustness to noisy or diverse\\ncorpora.\\nHigh communication and consensus\\noverhead; conflicting answers require\\nresolution.\\nLarge-scale evidence aggregation\\nacross heterogeneous sources (e.g.,\\nmeta-analysis, news tracking).\\nMulti-agent\\n(Central-\\nized/Hierarchical)\\nBudget-efficient: manager avoids\\nduplicate searches and ensures a clear\\nprovenance chain. Scales horizontally\\nwithout exponential cost growth.\\nManager prompts or policies can\\nbecome a single-point bottleneck,\\nlimiting performance.\\nComplex tasks requiring coordinated\\nsubtasks under strict API-call budgets.\\nTable 6: Comparison of reasoning workflows and agent orchestration in Synergized RAG-Reasoning systems.\\n26'), Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 0}, page_content='1 \\nSample PDF \\n \\nCreated for testing PDFObject \\n \\nThis PDF is three pages long. Three long pages. Or three short pages if \\nyou’re optimistic. Is it the same as saying “three long minutes”, knowing \\nthat all minutes are the same duration, and one cannot possibly be longer \\nthan the other? If these pages are all the same size, can one possibly be \\nlonger than the other? \\n \\nI digress. Here’s some Latin. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec \\nodio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum \\nimperdiet. Duis sagittis ipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris \\nmassa. Vestibulum lacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per \\nconubia nostra, per inceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem \\nat dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. \\nMorbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia \\naliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh.  \\n \\nQuisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia \\nnostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque \\nadipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet. Donec lacus nunc, viverra nec, blandit \\nvel, egestas et, augue. Vestibulum tincidunt malesuada tellus. Ut ultrices ultrices enim. Curabitur \\nsit amet mauris.  \\n \\nMorbi in dui quis est pulvinar ullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras \\nmetus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique, \\ndignissim in, ultrices sit amet, augue. Proin sodales libero eget ante. Nulla quam. Aenean laoreet. \\nVestibulum nisi lectus, commodo ac, facilisis ac, ultricies eu, pede. Ut orci risus, accumsan \\nporttitor, cursus quis, aliquet eget, justo. Sed pretium blandit orci.  \\n \\nUt eu diam at pede suscipit sodales. Aenean lectus elit, fermentum non, convallis id, sagittis at, \\nneque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem'), Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 1}, page_content='2 \\nipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante \\ndapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. \\nPraesent mauris.  \\n \\nFusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class \\naptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur \\nsodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean \\nquam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem.  \\n \\nProin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, \\nmassa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper \\nvel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, \\nurna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla \\nfacilisi. Ut fringilla. Suspendisse potenti.  \\n \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet.  \\n \\nDonec lacus nunc, viverra nec, blandit vel, egestas et, augue. Vestibulum tincidunt malesuada \\ntellus. Ut ultrices ultrices enim. Curabitur sit amet mauris. Morbi in dui quis est pulvinar \\nullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras metus. Sed aliquet risus a \\ntortor. Integer id quam. Morbi mi.  \\n \\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed \\ncursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis \\nipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris massa. Vestibulum \\nlacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per \\ninceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem \\nat dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. \\nMorbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia \\naliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh.  \\n \\nQuisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia \\nnostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque \\nadipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue'), Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 2}, page_content='3 \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet. Donec lacus nunc, viverra nec, blandit \\nvel, egestas et, augue. Vestibulum tincidunt malesuada tellus. Ut ultrices ultrices enim. Curabitur \\nsit amet mauris.  \\n \\nMorbi in dui quis est pulvinar ullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras \\nmetus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique, \\ndignissim in, ultrices sit amet, augue. Proin sodales libero eget ante. Nulla quam. Aenean laoreet. \\nVestibulum nisi lectus, commodo ac, facilisis ac, ultricies eu, pede. Ut orci risus, accumsan \\nporttitor, cursus quis, aliquet eget, justo. Sed pretium blandit orci.  \\n \\nUt eu diam at pede suscipit sodales. Aenean lectus elit, fermentum non, convallis id, sagittis at, \\nneque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem \\nipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante \\ndapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. \\nPraesent mauris.  \\n \\nFusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class \\naptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur \\nsodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean \\nquam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem.  \\n \\nProin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, \\nmassa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper \\nvel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, \\nurna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla \\nfacilisi. Ut fringilla. Suspendisse potenti.  \\n \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet.'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nRAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nZirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang∗\\nThe University of Hong Kong\\nzrguo101@hku.hk\\nxubinrencs@gmail.com\\nchaohuang75@gmail.com\\nABSTRACT\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\\nfor expanding Large Language Models beyond their static training limitations.\\nHowever, a critical misalignment exists between current RAG capabilities and\\nreal-world information environments. Modern knowledge repositories are inher-\\nently multimodal, containing rich combinations of textual content, visual elements,\\nstructured tables, and mathematical expressions. Yet existing RAG frameworks are\\nlimited to textual content, creating fundamental gaps when processing multimodal\\ndocuments. We present RAG-Anything, a unified framework that enables compre-\\nhensive knowledge retrieval across all modalities. Our approach reconceptualizes\\nmultimodal content as interconnected knowledge entities rather than isolated data\\ntypes. The framework introduces dual-graph construction to capture both cross-\\nmodal relationships and textual semantics within a unified representation. We\\ndevelop cross-modal hybrid retrieval that combines structural knowledge naviga-\\ntion with semantic matching. This enables effective reasoning over heterogeneous\\ncontent where relevant evidence spans multiple modalities. RAG-Anything demon-\\nstrates superior performance on challenging multimodal benchmarks, achieving\\nsignificant improvements over state-of-the-art methods. Performance gains become\\nparticularly pronounced on long documents where traditional approaches fail. Our\\nframework establishes a new paradigm for multimodal knowledge access, eliminat-\\ning the architectural fragmentation that constrains current systems. Our framework\\nis open-sourced at: https://github.com/HKUDS/RAG-Anything.\\n1\\nINTRODUCTION\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding\\nthe knowledge boundaries of Large Language Models (LLM) beyond their static training limita-\\ntions Zhang et al. (2025). By enabling dynamic retrieval and incorporation of external knowledge\\nduring inference, RAG systems transform static language models into adaptive, knowledge-aware\\nsystems. This capability has proven essential for applications requiring up-to-date information,\\ndomain-specific knowledge, or factual grounding that extends beyond pre-training corpora.\\nHowever, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the\\nrich multimodal information present in real-world documents. This limitation fundamentally mis-\\naligns with how information exists in authentic environments. Real-world knowledge repositories are\\ninherently heterogeneous and multimodal Abootorabi et al. (2025). They contain rich combinations\\nof textual content, visual elements, structured tables, and mathematical expressions across diverse\\ndocument formats. This textual assumption forces existing RAG systems to either discard non-textual\\ninformation entirely or flatten complex multimodal content into inadequate textual approximations.\\nThe consequences of this limitation become particularly severe in document-intensive domains\\nwhere multimodal content carries essential meaning. Academic research, financial analysis, and\\ntechnical documentation represent prime examples of knowledge-rich environments. These domains\\nfundamentally depend on visual and structured information. Critical insights are often encoded\\nexclusively in non-textual formats. Such formats resist meaningful conversion to plain text.\\nThe consequences of this limitation become particularly severe in knowledge-intensive domains where\\nmultimodal content carries essential meaning. Three representative scenarios illustrate the critical\\n∗Corresponding Author: Chao Huang\\n1\\narXiv:2510.12323v1  [cs.AI]  14 Oct 2025'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nneed for multimodal RAG capabilities. In Scientific Research, experimental results are primarily\\ncommunicated through plots, diagrams, and statistical visualizations. These contain core discoveries\\nthat remain invisible to text-only systems. Financial Analysis relies heavily on market charts,\\ncorrelation matrices, and performance tables. Investment insights are encoded in visual patterns\\nrather than textual descriptions. Additionally, Medical Literature Analysis depends on radiological\\nimages, diagnostic charts, and clinical data tables. These contain life-critical information essential for\\naccurate diagnosis and treatment decisions. Current RAG frameworks systematically exclude these\\nvital knowledge sources across all three scenarios. This creates fundamental gaps that render them\\ninadequate for real-world applications requiring comprehensive information understanding. Therefore,\\nmultimodal RAG emerges as a critical advancement. It is necessary to bridge these knowledge gaps\\nand enable truly comprehensive intelligence across all modalities of human knowledge representation.\\nAddressing multimodal RAG presents three fundamental technical challenges that demand principled\\nsolutions. This makes it significantly more complex than traditional text-only approaches. The naive\\nsolution of converting all multimodal content to textual descriptions introduces severe information\\nloss. Visual elements such as charts, diagrams, and spatial layouts contain semantic richness that\\ncannot be adequately captured through text alone. These inherent limitations necessitate the design of\\neffective technical components. Such components must be specifically designed to handle multimodal\\ncomplexity and preserve the full spectrum of information contained within diverse content types.\\nTechnical Challenges. • First, the unified multimodal representation challenge requires seam-\\nlessly integrating diverse information types. The system must preserve their unique characteristics\\nand cross-modal relationships. This demands advanced multimodal encoders that can capture both\\nintra-modal and inter-modal dependencies without losing essential visual semantics. • Second, the\\nstructure-aware decomposition challenge demands intelligent parsing of complex layouts. The\\nsystem must maintain spatial and hierarchical relationships crucial for understanding. This requires\\nspecialized layout-aware parsing modules that can interpret document structure and preserve contex-\\ntual positioning of multimodal elements. • Third, the cross-modal retrieval challenge necessitates\\nsophisticated mechanisms that can navigate between different modalities. These mechanisms must\\nreason over their interconnections during retrieval. This calls for cross-modal alignment systems\\ncapable of understanding semantic correspondences across text, images, and structured data. These\\nchallenges are amplified in long-context scenarios. Relevant evidence is dispersed across multiple\\nmodalities and sections, requiring coordinated reasoning across heterogeneous information sources.\\nOur Contributions. To address these challenges, we introduce RAG-Anything, a unified framework\\nthat fundamentally reimagines multimodal knowledge representation and retrieval. Our approach\\nemploys a dual-graph construction strategy that elegantly bridges the gap between cross-modal\\nunderstanding and fine-grained textual semantics. Rather than forcing diverse modalities into text-\\ncentric pipelines, RAG-Anything constructs complementary knowledge graphs that preserve both\\nmultimodal contextual relationships and detailed textual knowledge. This design enables seamless\\nintegration of visual elements, structured data, and mathematical expressions within a unified retrieval\\nframework. The system maintains semantic integrity across modalities while ensuring efficient\\ncross-modal reasoning capabilities throughout the process.\\nOur cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav-\\nigation with semantic similarity matching. This architecture addresses the fundamental limita-\\ntion of existing approaches that rely solely on embedding-based retrieval or keyword matching.\\nRAG-Anything leverages explicit graph relationships to capture multi-hop reasoning patterns. It\\nsimultaneously employs dense vector representations to identify semantically relevant content that\\nlacks direct structural connections. The framework introduces modality-aware query processing\\nand cross-modal alignment systems. These enable textual queries to effectively access visual and\\nstructured information. This unified approach eliminates the architectural fragmentation that plagues\\ncurrent multimodal RAG systems. It delivers superior performance particularly on long-context\\ndocuments where relevant evidence spans multiple modalities and document sections.\\nExperimental Validation. To validate the effectiveness of our proposed approach, we conduct com-\\nprehensive experiments on two challenging multimodal benchmarks: DocBench and MMLongBench.\\nOur evaluation demonstrates that RAG-Anything achieves superior performance across diverse do-\\nmains. The framework represents substantial improvements over state-of-the-art baselines. Notably,\\nour performance gains become increasingly significant as content length increases. We observe\\nparticularly pronounced advantages on long-context materials. This validates our core hypothesis\\n2'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nthat dual-graph construction and cross-modal hybrid retrieval are essential for handling complex\\nmultimodal materials. Our ablation studies reveal that graph-based knowledge representation provides\\nthe primary performance gains. Traditional chunk-based approaches fail to capture the structural\\nrelationships critical for multimodal reasoning. Case studies further demonstrate that our framework\\nexcels at precise localization within complex layouts. The system effectively disambiguates similar\\nterminology and navigates multi-panel visualizations through structure-aware retrieval mechanisms.\\n2\\nTHE RAG-ANYTHING FRAMEWORK\\n2.1\\nPRELIMINARY\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for dynamically\\nexpanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning\\ncapabilities, their knowledge remains static and bounded by training data cutoffs. This creates an\\never-widening gap with the rapidly evolving information landscape. RAG systems address this critical\\nlimitation by enabling LLMs to retrieve and incorporate external knowledge sources during inference.\\nThis transforms them from static repositories into adaptive, knowledge-aware systems.\\nThe Multimodal Reality: Beyond Text-Only RAG. Current RAG systems face a critical limitation\\nthat severely restricts their real-world deployment. Existing frameworks operate under the restrictive\\nassumption that knowledge corpus consists exclusively of plain textual documents. This assump-\\ntion fundamentally misaligns with how information exists in authentic environments. Real-world\\nknowledge repositories are inherently heterogeneous and multimodal, containing rich combinations\\nof textual content, visual elements, structured data, and mathematical expressions. These diverse\\nknowledge sources span multiple document formats and presentation mediums, from research papers\\nand technical slides to web pages and interactive documents.\\n2.1.1\\nMOTIVATING RAG-ANYTHING\\nThis multimodal reality introduces fundamental technical challenges that expose the inadequacy of\\ncurrent text-only RAG approaches. Effective multimodal RAG requires unified indexing strategies\\nthat can handle disparate data types, cross-modal retrieval mechanisms that preserve semantic\\nrelationships across modalities, and sophisticated synthesis techniques that can coherently integrate\\ndiverse information sources. These challenges demand a fundamentally different architectural\\napproach rather than incremental improvements to existing systems.\\nThe RAG-Anything framework introduces a unified approach for retrieving and processing knowl-\\nedge from heterogeneous multimodal information sources. Our system addresses the fundamental\\nchallenge of handling diverse data modalities and document formats within a retrieval pipeline.\\nThe framework comprises three core components: universal indexing for multimodal knowledge,\\ncross-modal adaptive retrieval, and knowledge-enhanced response generation. This integrated design\\nenables effective knowledge utilization across modalities while maintaining computational efficiency.\\n2.2\\nUNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\\nA key requirement for universal knowledge access is the ability to represent heterogeneous multimodal\\ncontent in a unified, retrieval-oriented abstraction. Unlike existing pipelines that simply parse\\ndocuments into text segments, RAG-Anything introduces Multimodal Knowledge Unification. This\\nprocess decomposes raw inputs into atomic knowledge units while preserving their structural context\\nand semantic alignment. For instance, RAG-Anything ensures that figures remain grounded in their\\ncaptions, equations remain linked to surrounding definitions, and tables stay connected to explanatory\\nnarratives. This transforms heterogeneous files into a coherent substrate for cross-modal retrieval.\\nFormally, each knowledge source ki ∈K (e.g., a web page) is decomposed into atomic content units:\\nki\\nDecompose\\n−−−−−−−→{cj = (tj, xj)}ni\\nj=1,\\n(1)\\nwhere each unit cj consists of a modality type tj ∈text, image, table, equation, . . . and its corre-\\nsponding raw content xj. The content xj represents the extracted information from the original\\nknowledge source, processed in a modality-aware manner to preserve semantic integrity.\\n3'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nParallel\\nParser\\nMultimodal Knowledge Unification\\n...\\nImage Info\\n...\\nEquation Info\\nText Info\\n...\\nTable Info\\nHierarchical Text \\nExtraction\\nImage Caption & \\nMetadata Extraction\\nLaTeX Equation \\nRecognition\\nTable Structure & \\nContent Parsing\\nDual-Graph Construction for Multimodal Knowledge \\n(each document) \\nStructured Content List\\nText Encoder\\nMulti-modal \\nProcessors\\nEntity & Relation\\nExtraction\\nBeekeeper\\nBee\\nObserve\\nKnowledge Graph\\nMerged Node\\nChild Node\\nParent Node: Multi-\\nmodal Instance\\nVLM/LLM\\nMerged\\n...\\nTextual Multi-modal Info\\nCross-Modal Knowledge\\nGraph\\nText-Based Knowledge Graph\\nKG over All Documents\\nMerged\\nText VDB\\nMulti-modal VDB\\nVDB over All\\nDocuments\\nQuery\\nCould you share insights \\non the experimental \\nresults and data tables?\\nResponse\\nBased on the experimental \\ndata, the results revealed...\\nSemantic Similarity \\nMatching\\nStructural Knowledge \\nNavigation\\nQuery High-/Low-level Keys Extraction\\nHybrid Retrieved Info\\n...\\nMulti-modal Info Processing\\nText Info Processing\\nLLM\\nText Encoder\\nVector Database\\nFigure 1: Overview of our proposed universal RAG framework RAG-Anything.\\nTo ensure high-fidelity extraction, RAG-Anything leverages specialized parsers for different content\\ntypes. Text is segmented into coherent paragraphs or list items. Figures are extracted with associated\\nmetadata such as captions and cross-references. Tables are parsed into structured cells with headers\\nand values. Mathematical expressions are converted into symbolic representations. The resulting xj\\npreserves both content and structural context within the source. This provides a faithful, modality-\\nconsistent representation. The decomposition abstracts diverse file formats into atomic units while\\nmaintaining their hierarchical order and contextual relationships. This canonicalization enables\\nuniform processing, indexing, and retrieval of multimodal content within our framework.\\n2.2.1\\nDUAL-GRAPH CONSTRUCTION FOR MULTIMODAL KNOWLEDGE\\nWhile multimodal knowledge unification provides a uniform abstraction across modalities, directly\\nconstructing a single unified graph often risks overlooking modality-specific structural signals. The\\nproposed RAG-Anything addresses this challenge through a dual-graph construction strategy. The\\nsystem first builds a cross-modal knowledge graph that faithfully grounds non-textual modalities\\nwithin their contextual environment. It then constructs a text-based knowledge graph using es-\\ntablished text-centric extraction pipelines. These complementary graphs are merged through entity\\nalignment. This design ensures accurate cross-modal grounding and comprehensive coverage of\\ntextual semantics, enabling richer knowledge representation and robust retrieval.\\n• Cross-Modal Knowledge Graph: Non-textual content like images, tables, and equations contains\\nrich semantic information that traditional text-only approaches often overlook. To preserve this\\nknowledge, RAG-Anything constructs a multimodal knowledge graph where non-text atomic\\nunits are transformed into structured graph entities. RAG-Anything leverages multimodal large\\nlanguage models to derive two complementary textual representations from each atomic content\\nunit. The first is a detailed description dchunk\\nj\\noptimized for cross-modal retrieval. The second is\\nan entity summary eentity\\nj\\ncontaining key attributes such as entity name, type, and description for\\ngraph construction. The generation process is context-aware, processing each unit with its local\\nneighborhood Cj = {ck | |k −j| ≤δ}, where δ controls the contextual window size. This ensures\\nrepresentations accurately reflect each unit’s role within the broader document structure.\\nBuilding on these textual representations, RAG-Anything constructs the graph structure using non-\\ntext units as anchor points. For each non-text unit cj, the graph extraction routine R(·) processes\\nits description dchunk\\nj\\nto identify fine-grained entities and relations:\\n(Vj, Ej) = R(dchunk\\nj\\n),\\n(2)\\nwhere Vj and Ej denote the sets of intra-chunk entities and their relations, respectively. Each\\natomic non-text unit is associated with a multimodal entity node vmm\\nj\\nthat serves as an anchor for\\n4'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nits intra-chunk entities through explicit belongs_to edges:\\n˜V = {vmm\\nj\\n}j ∪\\n[\\nj\\nVj,\\n(3)\\n˜E =\\n[\\nj\\nEj ∪\\n[\\nj\\n{(u\\nbelongs_to\\n−−−−−−−→vmm\\nj\\n) : u ∈Vj}.\\n(4)\\nThis construction preserves modality-specific grounding while ensuring non-textual content is con-\\ntextualized by its textual neighborhood. This enables reliable cross-modal retrieval and reasoning.\\n• Text-based Knowledge Graph: For text modality chunks, we construct a traditional text-based\\nknowledge graph following established methodologies similar to LightRAG (Guo et al., 2024)\\nand GraphRAG (Edge et al., 2024). The extraction process operates directly on textual content xj\\nwhere tj = text, leveraging named entity recognition and relation extraction techniques to identify\\nentities and their semantic relationships. Given the rich semantic information inherent in textual\\ncontent, multimodal context integration is not required for this component. The resulting text-based\\nknowledge graph captures explicit knowledge and semantic connections present in textual portions\\nof documents, complementing the multimodal graph’s cross-modal grounding capabilities.\\n2.2.2\\nGRAPH FUSION AND INDEX CREATION\\nThe separate cross-modal and text-based knowledge graphs capture complementary aspects of\\ndocument semantics. Integrating them creates a unified representation leveraging visual-textual\\nassociations and fine-grained textual relationships for enhanced retrieval.\\n• (i) Entity Alignment and Graph Fusion. To create a unified knowledge representation, we\\nmerge the multimodal knowledge graph ( ˜V , ˜E) and text-based knowledge graph through entity align-\\nment. This process uses entity names as primary matching keys to identify semantically equivalent\\nentities across both graph structures. The integration consolidates their representations, creating\\na comprehensive knowledge graph G = (V, E). This graph captures both multimodal contextual\\nrelationships and text-based semantic connections. The merged graph provides a holistic view of the\\ndocument collection. This enables effective retrieval by leveraging visual-textual associations from\\nthe multimodal graph and fine-grained textual knowledge relationships from the text-based graph.\\n• (ii) Dense Representation Generation. To enable efficient similarity-based retrieval, we construct\\na comprehensive embedding table T that encompasses all components generated during the indexing\\nprocess. We encode dense representations for all graph entities, relationships, and atomic content\\nchunks across modalities using an appropriate encoder. This creates a unified embedding space where\\neach component s ∈entities, relations, chunks is mapped to its corresponding dense representation:\\nT = emb(s) : s ∈V ∪E ∪cjj,\\n(5)\\nwhere emb(·) denotes the embedding function tailored for each component type. Together, the\\nunified knowledge graph G and the embedding table T constitute the complete retrieval index\\nI = (G, T ). This provides both structural knowledge representation and dense vector space for\\nefficient cross-modal similarity search during the subsequent retrieval stage.\\n2.3\\nCROSS-MODAL HYBRID RETRIEVAL\\nThe retrieval stage operates on the index I = (G, T ) to identify relevant knowledge components for a\\ngiven user query. Traditional RAG methods face significant limitations when dealing with multimodal\\ndocuments. They typically rely on semantic similarity within single modalities and fail to capture the\\nrich interconnections between visual, mathematical, tabular, and textual elements. To address these\\nchallenges, our framework introduces a cross-modal hybrid retrieval mechanism. This mechanism\\nleverages structural knowledge and semantic representations across heterogeneous modalities.\\nModality-Aware Query Encoding. Given a user query q, we first perform modality-aware query\\nanalysis to extract lexical cues and potential modality preferences embedded within the query.\\nFor instance, queries containing terms such as \"figure,\" \"chart,\" \"table,\" or \"equation\" provide\\nexplicit signals about the expected modality of relevant information. We then compute a unified text\\nembedding eq using the same encoder employed during indexing, ensuring consistency between\\n5'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nquery and knowledge representations. This embedding-based approach enables cross-modal retrieval\\ncapabilities where textual queries can effectively access multimodal content through their shared\\nrepresentations, maintaining retrieval consistency while preserving cross-modal accessibility.\\nHybrid Knowledge Retrieval Architecture. Recognizing that knowledge relevance manifests\\nthrough both explicit structural connections and implicit semantic relationships, we design a hybrid\\nretrieval architecture that strategically combines two complementary mechanisms.\\n• (i) Structural Knowledge Navigation. This mechanism addresses the challenge of capturing\\nexplicit relationships and multi-hop reasoning patterns. Traditional keyword-based retrieval often\\nfails to identify knowledge connected through intermediate entities or cross-modal relationships. To\\novercome this limitation, we exploit the structural properties encoded within our unified knowledge\\ngraph G. We employ keyword matching and entity recognition to locate relevant graph components.\\nThe retrieval process begins with exact entity matching against query terms.\\nWe then perform strategic neighborhood expansion to include related entities and relationships within\\na specified hop distance. This structural approach proves particularly effective at uncovering high-\\nlevel semantic connections and entity-relation patterns that span multiple modalities. It capitalizes\\non the rich cross-modal linkages established in our multimodal knowledge graph. The structural\\nnavigation yields candidate set Cstru(q) containing relevant entities, relationships, and their associated\\ncontent chunks that provide comprehensive contextual information.\\n• (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying\\nsemantically relevant knowledge that lacks explicit structural connections. While structural navigation\\nexcels at following explicit relationships, it may miss relevant content that is semantically related but\\nnot directly connected in the graph topology. To bridge this gap, we conduct dense vector similarity\\nsearch between the query embedding eq and all components stored in embedding table T .\\nThis approach encompasses atomic content chunks across all modalities, graph entities, and relation-\\nship representations, enabling fine-grained semantic matching that can surface relevant knowledge\\neven when traditional lexical or structural signals are absent. The learned embedding space captures\\nnuanced semantic relationships and contextual similarities that complement the explicit structural\\nsignals from the navigation mechanism. This retrieval pathway returns the top-k most semantically\\nsimilar chunks Cseman(q) ranked by cosine similarity scores, ensuring comprehensive coverage of\\nboth structurally and semantically relevant knowledge.\\nCandidate Pool Unification. Both retrieval pathways may return overlapping candidates with\\ndiffering relevance signals. This necessitates a principled approach to unify and rank results. Retrieval\\ncandidates from both pathways are unified into a comprehensive candidate pool: C(q) = Cstru(q) ∪\\nCseman(q). Simply merging candidates would ignore distinct evidence each pathway provides. It\\nwould fail to account for redundancy between retrieved content.\\n• (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion\\nscoring mechanism integrating multiple complementary relevance signals. These include structural\\nimportance derived from graph topology, semantic similarity scores from embedding space, and query-\\ninferred modality preferences obtained through lexical analysis. This multi-faceted scoring approach\\nensures that final ranked candidates C⋆(q) effectively balance structural knowledge relationships with\\nsemantic relevance while appropriately weighting different modalities based on query characteristics.\\n• (ii) Hybrid Retrieval Integration. The resulting hybrid retrieval mechanism enables our framework\\nto leverage the complementary strengths of both knowledge graphs and dense representations. This\\nprovides comprehensive coverage of relevant multimodal knowledge for response generation.\\n2.4\\nFROM RETRIEVAL TO SYNTHESIS\\nEffective multimodal question answering requires preserving rich visual semantics while maintaining\\ncoherent grounding across heterogeneous knowledge sources. Simple text-only approaches lose\\ncrucial visual information, while naive multimodal methods struggle with coherent cross-modal\\nintegration. Our synthesis stage addresses these challenges by systematically combining retrieved\\nmultimodal knowledge into comprehensive, evidence-grounded responses.\\n• (i) Building Textual Context. Given the top-ranked retrieval candidates C⋆(q), we construct a\\nstructured textual context. We concatenate textual representations of all retrieved components, includ-\\n6'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nTable 1: Statistics of Experimental Datasets.\\nDataset\\n# Documents\\n# Avg. Pages\\n# Avg. Tokens\\n# Doc Types\\n# Questions\\nDocBench\\n229\\n66\\n46377\\n5\\n1102\\nMMLongBench\\n135\\n47.5\\n21214\\n7\\n1082\\ning entity summaries, relationship descriptions, and chunk contents. The concatenation incorporates\\nappropriate delimiters to indicate modality types and hierarchical origins. This approach ensures the\\nlanguage model can effectively parse and reason over heterogeneous knowledge components.\\n• (ii) Recovering Visual Content. For multimodal chunks corresponding to visual artifacts, we\\nperform dereferencing to recover original visual content, creating V⋆(q). This design maintains con-\\nsistency with our unified embedding strategy. Textual proxies enable efficient retrieval while authentic\\nvisual content provides rich semantics necessary for sophisticated reasoning during synthesis.\\nThe synthesis process jointly conditions on both the assembled comprehensive textual context and\\ndereferenced visual artifacts using a vision-language model:\\nResponse = VLM(q, P(q), V⋆(q)),\\n(6)\\nwhere the VLM integrates information from query, textual context, and visual content. This unified\\nconditioning enables sophisticated visual interpretation while maintaining grounding in retrieved\\nevidence. The resulting responses are both visually informed and factually grounded.\\n3\\nEVALUATION\\n3.1\\nEXPERIMENTAL SETTINGS\\nEvaluation Datasets. We conduct comprehensive evaluations on two challenging multimodal\\nDocument Question Answering (DQA) benchmarks that reflect real-world complexity and diversity.\\nDocBench (Zou et al., 2024) provides a rigorous testbed with 229 multimodal documents spanning\\nfive critical domains: Academia, Finance, Government, Laws, and News. The dataset includes 1,102\\nexpert-crafted question-answer pairs. These documents are notably extensive, averaging 66 pages and\\napproximately 46,377 tokens, which presents substantial challenges for long-context understanding.\\nMMLongBench (Ma et al., 2024) complements this evaluation by focusing specifically on long-\\ncontext multimodal document comprehension. It features 135 documents across 7 diverse document\\ntypes with 1,082 expert-annotated questions. Together, these benchmarks provide comprehensive\\ncoverage of the multimodal document understanding challenges that RAG-Anything aims to address.\\nThey ensure our evaluation captures both breadth across domains and depth in document complexity.\\nDetailed dataset statistics and characteristics are provided in Appendix A.1.\\nBaselines. We compare RAG-Anything against the following methods for performance evaluation:\\n• GPT-4o-mini: A powerful multimodal language model with native text and image understanding\\ncapabilities. Its 128K token context window enables direct processing of entire documents. We\\nevaluate this model as a strong baseline for long-context multimodal understanding.\\n• LightRAG (Guo et al., 2024): A graph-enhanced RAG system that integrates structured knowledge\\nrepresentation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation\\ninformation and broader semantic context, improving retrieval precision and response coherence.\\n• MMGraphRAG (Wan & Yu, 2025): A multimodal retrieval framework that constructs unified\\nknowledge graphs spanning textual and visual content. This method employs spectral clustering\\nfor multimodal entity analysis and retrieves context along reasoning paths to guide generation.\\nExperimental Settings. In our experiments, we implement all baselines using GPT-4o-mini as\\nthe backbone LLM. Documents are parsed using MinerU (Wang et al., 2024) to extract text, im-\\nages, tables, and equations for downstream RAG processing. For the retrieval pipeline, we em-\\nploy the text-embedding-3-large model with 3072-dimensional embeddings. We use the\\nbge-reranker-v2-m3 model for reranking. For graph-based RAG methods, we enforce a com-\\nbined entity-and-relation token limit of 20,000 tokens and a chunk token limit of 12,000 tokens.\\n7'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nTable 2: Accuracy (%) on DocBench Dataset. Performance results with best scores highlighted in\\ndark blue and second-best in light blue. Domain categories include Academia (Aca.), Finance\\n(Fin.), Government (Gov.), Legal Documents (Law), and News Articles (News). Document types are\\ncategorized as Text-only (Txt.), Multimodal (Mm.), and Unanswerable queries (Una.).\\nMethod\\nDomains\\nTypes\\nOverall\\nAca.\\nFin.\\nGov.\\nLaw.\\nNews\\nTxt.\\nMm.\\nUna.\\nGPT-4o-mini\\n40.3\\n46.9\\n60.3\\n59.2\\n61.0\\n61.0\\n43.8\\n49.6\\n51.2\\nLightRAG\\n53.8\\n56.2\\n59.5\\n61.8\\n65.7\\n85.0\\n59.7\\n46.8\\n58.4\\nMMGraphRAG\\n64.3\\n52.8\\n64.9\\n40.0\\n61.5\\n67.6\\n66.0\\n60.5\\n61.0\\nRAGAnything\\n61.4\\n67.0\\n61.5\\n60.2\\n66.3\\n85.0\\n76.3\\n46.0\\n63.4\\nTable 3: Accuracy (%) on MMLongBench across different domains and overall performance. Best re-\\nsults are highlighted in dark blue and second-best in light blue.. Domain categories include Research\\nReports/Introductions (Res.), Tutorials/Workshops (Tut.), Academic Papers (Acad.), Guidebooks\\n(Guid.), Brochures (Broch.), Administration/Industry Files (Admin.), and Financial Reports (Fin.).\\nMethod\\nDomains\\nOverall\\nRes.\\nTut.\\nAcad.\\nGuid.\\nBroch.\\nAdmin.\\nFin.\\nGPT-4o-mini\\n35.5\\n44.0\\n24.6\\n33.1\\n29.5\\n46.8\\n31.1\\n33.5\\nLightRAG\\n40.8\\n34.1\\n36.2\\n39.4\\n41.0\\n44.4\\n38.3\\n38.9\\nMMGraphRAG\\n40.8\\n36.5\\n35.7\\n35.8\\n28.2\\n46.9\\n38.5\\n37.7\\nRAGAnything\\n46.6\\n43.5\\n38.7\\n43.9\\n34.0\\n45.7\\n43.6\\n42.8\\nOutputs are constrained to a one-sentence format. For the baseline GPT-4o-mini in our QA scenario,\\ndocuments are concatenated into image form with a maximum of 50 pages per document, rendered at\\n144 dpi. Finally, all query results are evaluated for accuracy by GPT-4o-mini.\\n3.2\\nPERFORMANCE COMPARISON\\nSuperior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior\\noverall performance over baselines through its unified multimodal framework. Unlike LightRAG,\\nwhich is restricted to text-only content processing, RAG-Anything treats text, images, tables, and\\nequations as first-class entities. MMGraphRAG only adds basic image processing while treating\\ntables and equations as plain text, missing crucial structural information. RAG-Anything introduces\\na comprehensive dual-graph construction strategy that preserves structural relationships across all\\nmodalities. This unified approach enables superior performance across both evaluation benchmarks.\\nEnhanced Long-Context Performance. RAG-Anything demonstrates superior performance on\\nlong-context documents. The framework excels where relevant evidence is dispersed across multiple\\nmodalities and sections. It achieves the best results in information-dense domains such as Research\\nReports and Financial Reports on MMLongBench. These improvements stem from the structured\\ncontext injection mechanism. This mechanism integrates dual-graph construction for cross-page entity\\nalignment. It combines semantic retrieval with structural navigation. The framework also employs\\nmodality-aware processing for efficient context window utilization. Unlike baselines that cannot\\nuniformly process diverse modalities, RAG-Anything effectively captures scattered multimodal\\nevidence. Its cross-modal hybrid retrieval architecture combines structural knowledge navigation\\nwith semantic similarity matching. This enables the framework to leverage both explicit relationships\\nand implicit semantic connections across modalities.\\nTo systematically evaluate model performance across varying document lengths, we conducted\\ncomprehensive experiments on both datasets. As illustrated in Figure 2, RAG-Anything and MM-\\nGraphRAG exhibit comparable performance on shorter documents. However, RAG-Anything’s\\nadvantages become increasingly pronounced as document length grows. On DocBench, the perfor-\\nmance gap expands dramatically to over 13 points for documents exceeding 100 pages (68.2% vs.\\n8'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 2: Performance evaluation across documents of varying lengths.\\nTable 4: Ablation study results on DocBench. The “Chunk-only” variant bypasses dual-graph\\nconstruction and relies solely on traditional chunk-based retrieval, while “w/o Reranker” eliminates\\ncross-modal reranking but preserves the core graph-based architecture.\\nMethod\\nDomains\\nTypes\\nOverall\\nAca.\\nFin.\\nGov.\\nLaw.\\nNews\\nTxt.\\nMm.\\nUna.\\nChunk-only\\n55.8\\n61.5\\n60.1\\n60.7\\n64.0\\n81.6\\n66.2\\n43.5\\n60.0\\nw/o Reranker\\n60.9\\n63.5\\n58.8\\n60.2\\n68.6\\n81.7\\n74.7\\n45.4\\n62.4\\nRAGAnything\\n61.4\\n67.0\\n61.5\\n60.2\\n66.3\\n85.0\\n76.3\\n46.0\\n63.4\\n54.6% for 101–200 pages; 68.8% vs. 55.0% for 200+ pages). On MMLongBench, RAG-Anything\\ndemonstrates consistent improvements across all length categories, achieving accuracy gains of 3.4\\npoints for 11–50 pages, 9.3 points for 51–100 pages, and 7.9 points for 101–200 pages. These\\nfindings confirm that our dual-graph construction and cross-modal hybrid retrieval mechanism is\\nparticularly effective for long-document reasoning tasks.\\n3.3\\nARCHITECTURAL VALIDATION WITH ABLATION STUDIES\\nTo isolate and quantify the contributions of key architectural components in RAG-Anything, we\\nconducted systematic ablation studies examining two critical design choices. Given that our approach\\nfundamentally differs from existing methods through dual-graph construction and hybrid retrieval,\\nwe specifically evaluated: i) Chunk-only, which bypasses graph construction entirely and relies\\nsolely on traditional chunk-based retrieval, and ii) w/o Reranker, which eliminates the cross-modal\\nreranking component while preserving the core graph-based architecture.\\nAs demonstrated in Table 4, the results validate our architectural design through striking performance\\nvariations. • Graph Construction is Essential. The chunk-only variant achieves merely 60.0%\\naccuracy with substantial cross-domain drops. This demonstrates that traditional chunking fails to\\ncapture structural and cross-modal relationships essential for multimodal documents. • Reranking\\nProvides Marginal Gains. Removing the reranker yields only a modest decline to 62.4%, while the\\nfull model achieves 63.4% accuracy. This indicates that cross-modal reranking provides valuable\\nrefinement, but primary gains stem from our graph-based retrieval and cross-modal integration.\\n3.4\\nCASE STUDIES\\nMultimodal documents contain rich structural information within each modality. Understanding\\nthese intra-modal structures is crucial for accurate reasoning. We analyze two representative cases\\nfrom DocBench to demonstrate how RAG-Anything leverages these structures. These cases highlight\\na key limitation of existing methods. Baselines either rely on superficial textual cues or flatten\\ncomplex visual elements into plain text. In contrast, RAG-Anything builds modality-aware graphs\\nthat preserve essential relationships (e.g., table header↔cell↔unit edges; panel↔caption↔axis\\nedges). This enables precise reasoning over complex document layouts.\\n• Case 1: Multi-panel Figure Interpretation. This case examines a common scenario in academic\\nliterature. Researchers often need to compare results across different experimental conditions. These\\nresults are typically presented in multi-panel visualizations. Figure 3 shows a challenging t-SNE\\n9'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content=\"RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nMultimodel Document\\nEvidence figure in the document\\nQuestion: Which model's style space shows a clearer separation\\nbetween different styles according to Figure 2?\\nGPT-4o-mini🤔: \\nAccording to Figure 2, the VAE\\nmodel's style space shows a clearer\\nseparation between different styles.\\nMMGraphRAG🤔: \\nAccording to Figure 2, the model's style space\\nshows a clearer separation between different styles\\nin the Variational Autoencoder (VAE) compared to\\nthe Deterministic Autoencoder (DAE).\\nRAG-Anything(Correct😉):\\nThe DAE model's style space shows a clearer\\nseparation between different styles according to\\nFigure 2.\\nLightRAG🤔：\\nAccording to Figure 2, the Variational Autoencoder\\n(VAE) shows a clearer separation between different\\nstyles \\nin \\nits \\nstyle \\nspace \\ncompared \\nto \\nthe\\nDeterministic Autoencoder (DAE). \\n(DAE shows a clearer seperation than VAE in Style Space)\\nFigure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation\\npatterns from the style-space panel, while avoiding confusion from the adjacent content-space panel.\\nvisualization with multiple subpanels. The query requires distinguishing between two related but\\ndistinct panels. RAG-Anything constructs a visual-layout graph where panels, axis titles, legends,\\nand captions become nodes. Key edges encode semantic relationships. Panels contain specific plots.\\nCaptions provide contextual information. Subfigures relate hierarchically. This structure guides the\\nretriever to focus on the style-space panel for comparing cluster separation patterns. The system\\navoids confusion from the adjacent content space panel. This panel shows less clear distinctions.\\nMultimodel Document\\nEvidence table in the document\\nQuestion: What was Novo Nordisk's total amount spent on wages and salaries in 2020?\\nGPT-4o-mini🤔: \\nNovo Nordisk's total amount\\nspent on wages and salaries in\\n2020 was DKK 32,928 million.\\nMMGraphRAG🤔: \\nNovo Nordisk spent a total of\\n11,503 million DKK on wages\\nand salaries in 2020.\\nRAG-Anything(Correct😉):\\nNovo Nordisk's total amount spent on wages\\nand salaries in 2020 was DKK 26,778 million. \\nLightRAG🤔：\\nNovo Nordisk spent DKK 11,503 million\\non wages and salaries in 2020.\\n(Identifying the true evidence is the key)\\nFigure 4: Financial table navigation case. The query involves locating the specific intersection of\\n“Wages and salaries” row and “2020” column amid similar terminological entries.\\n• Case 2: Financial Table Navigation. This case addresses a common challenge in financial\\ndocument analysis. Analysts must extract specific metrics from tables with similar terminology\\nand multiple time periods. Figure 4 shows this scenario. The query involves resolving ambiguous\\nfinancial terms and selecting the correct column for a specified year.\\nRAG-Anything transforms the financial report table into a structured graph. Each row header, column\\nheader (year), data cell, and unit becomes a node. The edges capture key relationships: row-of,\\ncolumn-of, header-applies-to, and unit-of. This structure enables precise navigation. The retriever\\nfocuses on the row “Wages and salaries” and the column for “2020”. It directs attention to the\\ntarget cell (26,778 million). The system successfully disambiguates nearby entries like “Share-based\\npayments.” Competing methods treat tables as linear text. They often confuse numerical spans and\\nyears. This leads to significantly inaccurate answers. RAG-Anything explicitly models relationships\\nwithin the table. It achieves precise selection and numeric grounding. This ensures accurate responses.\\n• Key Insights. Both cases demonstrate how RAG-Anything’s structure-aware design delivers\\ntargeted advantages. Our approach transforms documents into explicit graph representations. These\\ngraphs capture intra-modal relationships that traditional methods miss. In figures, connections\\nbetween panels, captions, and axes enable panel-level comparisons. This goes beyond keyword\\nmatching. In tables, row–column–unit graphs ensure accurate identification through modeling.\\nThis structure-aware retrieval design reduces confusion from repeated terminology and complex\\nlayouts. Traditional RAG systems struggle with these scenarios due to lack of structural understanding.\\nEven MMGraphRAG fails here because it only considers image modality entities. It ignores other\\nmodality entities like table cells, row headers, and column headers. RAG-Anything’s comprehensive\\ngraph representation captures all modality-specific entities and their relationships. This enables\\nprecise, modality-specific grounding that leads to consistent improvements in document Q&A tasks\\nrequiring fine-grained localization. Additional cases are available in Appendix A.2.\\n4\\nRELATED WORK\\n• Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with\\nlong-context inputs and multi-hop queries, failing to precisely locate dispersed evidence (Zhang et al.,\\n10\"), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\n2025). Graph structures address this limitation by introducing explicit relational modeling, improving\\nboth retrieval efficiency and reasoning accuracy (Bei et al., 2025).\\nSince GraphRAG (Edge et al., 2024), research has evolved along two complementary directions.\\nFirst, graph construction approaches optimize structures for retrieval efficiency, ranging from Ligh-\\ntRAG’s (Guo et al., 2024) sparsified indices to neural models like GNN-RAG (Mavromatis & Karypis,\\n2024) and memory-augmented variants like HippoRAG (Jimenez Gutierrez et al., 2024). Second,\\nknowledge aggregation approaches integrate information for multi-level reasoning through hier-\\narchical methods like RAPTOR (Sarthi et al., 2024) and ArchRAG (Wang et al., 2025). Despite\\nthese advances, existing systems remain text-centric with homogeneous inputs. This limits their\\napplicability to multimodal documents and constrains robust reasoning over heterogeneous content.\\nRAG-Anything addresses this gap by extending GraphRAG to all modalities.\\n• Multimodal Retrieval-Augmented Generation. Multimodal RAG represents a natural evolution\\nfrom text-based RAG systems, addressing the need to integrate external knowledge from diverse\\ndata modalities for comprehensive response generation (Abootorabi et al., 2025). However, current\\napproaches are fundamentally constrained by their reliance on modality-specific architectures. Exist-\\ning methods demonstrate these constraints across domains: VideoRAG (Ren et al., 2025) employs\\ndual-channel architectures for video understanding while MM-VID (Lin et al., 2023) converts videos\\nto text, losing visual information; VisRAG (Yu et al., 2025) preserves document layouts as images\\nbut misses granular relationships; MMGraphRAG (Wan & Yu, 2025) links scene graphs with textual\\nrepresentations but suffers from structural blindness—treating tables and formulas as plain text\\nwithout proper entity extraction, losing structural information for reasoning.\\nThe fundamental problem underlying these limitations is architectural fragmentation. Current systems\\nrequire specialized processing pipelines for each modality. This creates poor generalizability as new\\nmodalities demand custom architectures and fusion mechanisms. Such fragmentation introduces\\ncross-modal alignment difficulties, modality biases, and information bottlenecks. These issues\\nsystematically compromise system performance and scalability. RAG-Anything addresses this\\nfragmentation through a unified graph-based framework. Our approach processes all modalities with\\nconsistent structured modeling. This eliminates architectural constraints while preserving multimodal\\ninformation integrity. The result is seamless cross-modal reasoning across heterogeneous content.\\n5\\nCONCLUSION\\nRAG-Anything introduces a paradigm shift in multimodal retrieval through its unified graph-based\\nframework. Our core technical innovation is the dual-graph construction strategy that seamlessly\\nintegrates cross-modal and text-based knowledge graphs. Rather than forcing diverse modalities into\\ntext-centric pipelines that lose critical structural information, our approach fundamentally reconcep-\\ntualizes multimodal content as interconnected knowledge entities with rich semantic relationships.\\nThe hybrid retrieval mechanism strategically combines structural navigation with semantic matching,\\nenabling precise reasoning over complex document layouts. Comprehensive evaluation demonstrates\\nsuperior performance on long-context documents, particularly those exceeding 100 pages where\\ntraditional methods fail. This work establishes a new foundation for multimodal RAG systems that\\ncan handle the heterogeneous nature of diverse information landscapes.\\nOur analysis in Appendix A.5 reveals critical challenges facing current multimodal RAG systems.\\nTwo fundamental issues emerge through systematic failure case examination. First, systems exhibit\\ntext-centric retrieval bias, preferentially accessing textual sources even when queries explicitly\\nrequire visual information. Second, rigid spatial processing patterns fail to adapt to non-standard\\ndocument layouts. These limitations manifest in cross-modal misalignment scenarios and structurally\\nambiguous tables. The findings highlight the need for adaptive spatial reasoning and layout-aware\\nparsing mechanisms to handle real-world multimodal document complexity.\\n11'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nREFERENCES\\nMohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammad-\\nkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin\\nAsgari. Ask in any modality: A comprehensive survey on multimodal retrieval-augmented genera-\\ntion. arXiv preprint arXiv:2502.08826, 2025.\\nYuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu,\\nShirui Pan, Yizhou Yu, et al. Graphs meet ai agents: Taxonomy, progress, and future opportunities.\\narXiv preprint arXiv:2506.18019, 2025.\\nDarren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,\\nDasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A\\ngraph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-\\naugmented generation. arXiv preprint arXiv:2410.05779, 2024.\\nBernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neuro-\\nbiologically inspired long-term memory for large language models. NeurIPS, 37:59532–59569,\\n2024.\\nKevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng\\nWang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang. Mm-vid: Advancing video\\nunderstanding with gpt-4v(ision). arXiv preprint arXiv:2310.19773, 2023.\\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information Processing Systems, 37:95963–96010, 2024.\\nCostas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139, 2024.\\nXubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, and Chao Huang.\\nVide-\\norag:\\nRetrieval-augmented generation with extreme long-context videos.\\narXiv preprint\\narXiv:2502.01549, 2025.\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.\\nRaptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International\\nConference on Learning Representations, 2024.\\nXueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal\\nknowledge graphs. arXiv preprint arXiv:2507.20804, 2025.\\nBin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu,\\nYuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content\\nextraction. arXiv preprint arXiv:2409.18839, 2024.\\nShu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. Archrag: Attributed community-\\nbased hierarchical retrieval-augmented generation. arXiv preprint arXiv:2502.09891, 2025.\\nShi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang,\\nXu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on\\nmulti-modality documents. arXiv preprint arXiv:2410.10594, 2025.\\nQinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen,\\nYilin Xiao, Chuang Zhou, Yi Chang, and Xiao Huang. A survey of graph retrieval-augmented\\ngeneration for customized large language models. arXiv preprint arXiv:2501.13958, 2025.\\nAnni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and\\nDong Yu. Docbench: A benchmark for evaluating llm-based document reading systems. arXiv\\npreprint arXiv:2407.10701, 2024.\\n12'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nA\\nAPPENDIX\\nThis appendix provides comprehensive supporting materials for our experimental evaluation and\\nimplementation details. Section A.1 presents detailed dataset statistics for the DocBench and\\nMMLongBench multi-modal benchmarks, including document type distributions and complexity\\nmetrics. Section A.2 showcases additional case studies that demonstrate RAG-Anything’s structure-\\naware capabilities across diverse multimodal content understanding tasks. Section A.3 documents the\\ncomplete set of multimodal analysis prompts for vision, table, and equation processing that enable\\ncontext-aware interpretation. Section A.4 provides the standardized accuracy evaluation prompt used\\nfor consistent response assessment across all experimental conditions.\\nA.1\\nDATASET CHARACTERISTICS AND STATISTICS\\nTable 5: Document type distribution and statistics for the DocBench benchmark.\\nType\\nAcad.\\nFin.\\nGov.\\nLaw.\\nNews\\n# Docs\\n49\\n40\\n44\\n46\\n50\\n# Questions\\n303\\n288\\n148\\n191\\n172\\nAvg. Pages\\n11\\n192\\n69\\n58\\n1\\nTable 6: Document type distribution and statistics for the MMLongBench benchmark.\\nType\\nRes.\\nTut.\\nAcad.\\nGuid.\\nBroch.\\nAdmin.\\nFin.\\n# Docs\\n34\\n17\\n26\\n22\\n15\\n10\\n11\\n# Questions\\n292\\n138\\n199\\n155\\n100\\n81\\n117\\nAvg. Pages\\n39\\n58\\n35\\n78\\n30\\n17\\n87\\nTables 5 and 6 present the distribution of document types across the DocBench and MMLong-\\nBench benchmarks. • DocBench encompasses medium- to long-length documents spanning various\\ndomains, including legal, governmental, and financial files. Financial reports represent the most\\nextensive category, averaging 192 pages per document, while the News category consists of concise\\nsingle-page newspapers. • MMLongBench demonstrates a broader spectrum of document types and\\nlengths. Research reports, tutorials, and academic papers maintain moderate lengths of 35–58 pages\\non average, while guidebooks extend to approximately 78 pages. Brochures and administrative files\\nremain relatively compact, whereas financial reports again emerge as the longest category.\\nCollectively, these two benchmarks provide comprehensive coverage ranging from brief news arti-\\ncles to extensive technical and financial documentation. This establishes diverse and challenging\\nevaluation contexts for multimodal document understanding tasks.\\nA.2\\nADDITIONAL CASE STUDIES\\nMultimodel Document\\nEvidence figure in the document\\nQuestion: Which GCAN sub-model component removal\\nresulted in the lowest accuracy for Twitter15?\\nGPT-4o-mini🤔: \\nThe removal of the dual co-\\nattention mechanism from the\\nGCAN sub-model resulted in the\\nlowest accuracy for Twitter15.\\nMMGraphRAG🤔: \\nThe removal of the source tweet from the GCAN\\nmodel resulted in the lowest accuracy for\\nTwitter15, as indicated by significant accuracy\\ndrops in the ablation analysis in Figure 4.\\nRAG-Anything(Correct😉):\\nThe removal of the source tweet embeddings\\nand dual co-attention, indicated as model\\nconfiguration \"-S-A,\" resulted in the lowest\\naccuracy for Twitter15.\\nLightRAG🤔：\\nRemoving the source tweet from the GCAN model\\nresulted in the lowest accuracy for Twitter15, as\\nindicated by a significant drop in performance when\\nusing the sub-model that excluded both source tweet\\nembeddings and dual co-attention. \\nFigure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy\\nconfiguration, while baselines misinterpret spatial relationships.\\n• Demonstrating Visual Reasoning Capabilities. Figure 5 illustrates how RAG-Anything handles\\ncomplex visual reasoning tasks involving chart interpretation. The query asks which GCAN sub-\\nmodel component removal yields the lowest accuracy on Twitter15. Traditional approaches struggle\\n13'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nwith spatial relationships between visual elements. RAG-Anything addresses this challenge by\\nconstructing a structured graph representation of the bar plot. Bars, axis labels, and legends become\\ninterconnected nodes. These are linked by semantic relations such as bar-of and label-applies-to.\\nThis graph-based approach enables precise alignment between visual and textual elements. The\\nsystem correctly identifies the bar labeled \"-S-A\" (removing source tweet embeddings and dual\\nco-attention) and its corresponding accuracy value as the lowest performer. Baseline methods that\\nflatten visual information often misinterpret spatial relationships. They frequently conflate nearby\\ncomponents. RAG-Anything’s structured representation preserves critical visual-textual associations.\\nThis leads to accurate query resolution and proper attribution of performance drops to \"-S-A\".\\nMultimodel Document\\nEvidence table in the document\\nQuestion: Which model combination for the Evidence Inference\\ndataset has the highest AUPRC value?\\nGPT-4o-mini🤔: \\nThe model combination for the Evidence Inference\\ndataset with the highest AUPRC value is the BERT-\\nto-BERT model, achieving an AUPRC of 0.455.\\nMMGraphRAG🤔: \\nThe model combination for the Evidence Inference\\ndataset with the highest AUPRC value is the \"Bert-To-\\nBert\" model, which achieved an AUPRC score of\\n0.708.\\nRAG-Anything(Correct😉):\\nThe model combination for the Evidence\\nInference dataset with the highest AUPRC\\nvalue is GloVe + LSTM - Attention, achieving\\na value of 0.506. \\nLightRAG🤔：\\nThe model combination \"BERT + LSTM -\\nAttention\" has the highest AUPRC value for the\\nEvidence Inference dataset at 0.429. \\n(Table with spatial ambiguity)\\nFigure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while\\nthe compared approaches struggle with structural ambiguity.\\n• Handling Complex Tabular Structures. Figure 6 showcases RAG-Anything’s ability to navigate\\nintricate tabular data where structural disambiguation is crucial. The query seeks the model combi-\\nnation achieving the highest AUPRC value for the Evidence Inference dataset—a task complicated\\nby repeated row labels across multiple datasets within the same table. This scenario highlights a\\nfundamental limitation of conventional approaches that struggle with structural ambiguity in data.\\nRAG-Anything overcomes this by parsing the table into a comprehensive relational graph where\\nheaders and data cells become nodes connected through explicit row-of and column-of relationships.\\nThis structured representation enables the system to correctly isolate the Evidence Inference dataset\\ncontext and identify \"GloVe + LSTM – Attention\" with a score of 0.506 as the optimal configuration.\\nBy explicitly preserving hierarchical table constraints that other methods often collapse or misinterpret,\\nRAG-Anything ensures reliable reasoning across complex multi-dataset tabular structures.\\nA.3\\nCONTEXT-AWARE MULTIMODAL PROMPTING\\nThese three prompts orchestrate structured, context-aware multimodal analysis with JSON-formatted\\noutputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular,\\nand mathematical content while maintaining explicit alignment with surrounding information.\\nVision Analysis Prompt. Figure 7 orchestrates comprehensive image-context integration. The\\nprompt directs the model to systematically capture compositional elements, object relationships,\\nvisual attributes, stylistic features, dynamic actions, and technical components (e.g., charts), while es-\\ntablishing explicit connections to accompanying text. This approach transcends superficial description,\\nenabling contextually-grounded interpretations that enhance knowledge retrieval and substantiation.\\nTable Analysis Prompt. Figure 8 structures systematic tabular content decomposition across multiple\\nanalytical dimensions: structural organization, column semantics, critical values, statistical patterns,\\nand contextual relevance. Through precise terminology and numerical accuracy requirements, the\\nprompt eliminates ambiguous generalizations and ensures faithful preservation of key indicators\\nwhile maintaining coherent alignment with surrounding discourse.\\nEquation Analysis Prompt. Figure 9 prioritizes semantic interpretation over syntactic restatement\\nof mathematical expressions. The prompt instructs comprehensive analysis of variable definitions,\\noperational logic, theoretical foundations, inter-formula relationships, and practical applications. This\\nmethodology ensures mathematical content becomes integral to broader argumentative frameworks,\\nsupporting enhanced retrieval accuracy, analytical traceability, and reasoning coherence.\\n14'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 7: Vision analysis prompt for context-aware image interpretation and knowledge extraction.\\nFigure 8: Table analysis prompt for structured content decomposition and semantic understanding.\\n15'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 9: Equation analysis prompt for mathematical expression interpretation and integration.\\nFigure 10: Accuracy evaluation prompt for consistent factual assessment across question types.\\nA.4\\nACCURACY EVALUATION PROMPT DESIGN\\nFigure 10 presents the standardized prompt specifically designed for systematic factual accuracy as-\\nsessment of generated responses across multiple domains. The prompt establishes explicit evaluation\\ncriteria that prioritize content correctness over stylistic considerations, producing binary accuracy\\n16'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nclassifications accompanied by concise analytical justifications. All accuracy evaluations throughout\\nour comprehensive experimental framework were conducted using GPT-4o-mini, ensuring consistent\\nand reliable assessment standards across diverse question categories and specialized domains.\\nA.5\\nCHALLENGES AND FUTURE DIRECTIONS FOR MULTI-MODAL RAG\\nWhile current multimodal RAG systems demonstrate promising capabilities, their limitations emerge\\nmost clearly through systematic analysis of failure cases. Understanding where and why these systems\\nbreak down is crucial for advancing the field beyond current performance plateaus. Examining failure\\npatterns helps identify fundamental architectural bottlenecks and design principles for more robust\\nmultimodal systems. Our investigation reveals two critical failure patterns exposing deeper systemic\\nissues in multimodal RAG architectures. These patterns are not merely edge cases but reflect\\nfundamental challenges in cross-modal information integration and structural reasoning:\\n• Text-Centric Retrieval Bias: Systems exhibit strong preference for textual sources, even when\\nqueries explicitly demand visual information. This reveals inadequate cross-modal attention.\\n• Document Structure Processing Challenges: Systems struggle with complex layouts and non-\\nlinear information flows. This exposes limitations in spatial reasoning and contextual understanding.\\nThese failure modes illuminate key insights about current multimodal AI. They provide concrete\\ndirections for architectural innovations that could substantially improve system robustness.\\nQuestion: According to Figure 3, what is the sequence followed to generate the\\nfinal labels for Named Entity Recognition?\\nExpected answer: The sequence is Characters -> Char Embedding -> Char\\nLSTM-B/F -> Word Features -> Word Representation -> Word LSTM-F/B ->\\nWord CRF -> Label.\\nMultimodel Document\\nEvidence figure in the document\\nGPT-4o-mini🤔: \\n...involves inputting words and their representations\\ninto a Word CRF layer, followed by Word LSTM-B,\\nWord LSTM-F, and character-level representations,\\nculminating in the final prediction of entity labels.\\nMMGraphRAG🤔: \\n...involves using a combined representation of word\\nembeddings and character-level convolution, which is\\nthen processed through a Bi-LSTM layer, followed by\\na softmax or CRF layer to produce the final labels.\\nRAG-Anything🤔:\\n...involves processing input characters through\\nbi-directional LSTM layers (Char LSTM-B\\nand Char LSTM-F) followed by a softmax or\\nCRF layer that produces the final labels.\\nLightRAG🤔：\\n...a sequence where word representations are first\\nprocessed through a convolution layer, followed by a\\nBi-LSTM layer and then a softmax or CRF layer to\\nproduce the final labels. \\n(The order in the figure is from bottom to top)\\nFigure 11: Cross-modal noise case. All methods fail to retrieve the correct answer from the specified\\nimage, instead retrieving noisy textual evidence that misaligns with the structured visual content.\\nMultimodel Document\\nEvidence table in the document\\nQuestion: Which model\\'s style space shows a clearer separation\\nbetween different styles according to Figure 2?\\nGPT-4o-mini🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved by 6.26% when using the GEM fine-tuning\\nstrategy compared to training from scratch, achieving\\n65.35% versus 59.09%.\\nMMGraphRAG🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved from 44.24% when training from scratch to\\n59.83% when using the GEM fine-tuning strategy,\\nresulting in an improvement of 15.59%.\\nRAG-Anything🤔:\\nThe Joint Goal Accuracy in the \"Train\" domain\\nimproved from 44.24% with training from\\nscratch to 58.10% using the GEM fine-tuning\\nstrategy, resulting in an improvement of\\n13.86%. \\nLightRAG🤔：\\nThe Joint goal accuracy in the \"Train\" domain\\nimproved from 44.24% using training from scratch to\\n50.51% with GEM fine-tuning, indicating an\\nimprovement of 6.27%. \\n(An irregular table)\\nFigure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table\\nlayout with merged cells and unclear column boundaries, leading to incorrect data extraction.\\nCase 1: Cross-Modal Misalignment. Figure 11 presents a particularly revealing failure scenario\\nwhere all evaluated methods consistently produce incorrect answers despite having access to the\\nnecessary information. This universal failure across different architectures suggests fundamental\\nlimitations in how current systems handle noisy, heterogeneous multimodal data—a critical challenge\\nas real-world applications inevitably involve imperfect, inconsistent information sources. The failure\\nexposes two interconnected systemic issues that compound each other:\\nIssue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward\\ntextual passages. This occurs particularly when visual content lacks exact keyword matches. The\\nbias persists even when queries contain explicit instructions to prioritize visual sources. This reveals\\na fundamental weakness in cross-modal attention mechanisms.\\nThe retrieved textual information, while topically related, often operates at a different granularity level\\nthan visual content. Images may contain precise, structured data such as specific numerical values,\\n17'), Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\ndetailed diagrams, or exact spatial relationships. Corresponding text typically provides general,\\nconceptual descriptions. This semantic misalignment introduces noise that actively misleads the\\nreasoning process. The system attempts to reconcile incompatible levels of detail and specificity.\\nIssue 2: Rigid Spatial Processing Patterns. Current visual processing models exhibit fundamental\\nrigidity in spatial interpretation. Most systems default to sequential scanning patterns—top-to-\\nbottom and left-to-right—that mirror natural reading conventions. While effective for simple text\\ndocuments, this approach creates systematic failures with structurally complex real-world content.\\nMany documents require non-conventional processing strategies. Tables demand column-wise\\ninterpretation, technical diagrams follow specific directional flows, and scientific figures embed\\ncritical information in unexpectedly positioned annotations. These structural variations are prevalent\\nin professional documents, making adaptive spatial reasoning essential.\\nIn the observed failure case, the correct answer required integrating visual elements in reverse order\\nfrom the model’s default processing sequence. The system’s inability to recognize and adapt to this\\nstructural requirement led to systematic misinterpretation. This represents a fundamental architectural\\nlimitation where spatial reasoning remains static regardless of document context or query intent.\\nWhen spatial processing patterns are misaligned with document structure, the extracted information\\nbecomes not merely incomplete but actively misleading. This structural noise compounds other\\nprocessing errors and can lead to confident but entirely incorrect conclusions.\\nCase 2: Structural Noise in Ambiguous Table Layouts. As shown in Figure 12, all methods failed\\nwhen confronted with a structurally ambiguous table. The primary failure stems from the table’s\\nconfusing design: the GEM row lacks dedicated cell boundaries, and the \"Joint\" and \"Slot\" columns\\nmerge without clear separation. These structural irregularities create parsing ambiguities that system-\\natically mislead extraction algorithms. This failure pattern reveals a critical vulnerability in current\\nRAG systems. When table structures deviate from standard formatting conventions—through merged\\ncells, unclear boundaries, or non-standard layouts—extraction methods consistently misinterpret cell\\nrelationships and conflate distinct data values. This exposes the brittleness of current approaches\\nwhen faced with real-world document variations that deviate from clean, structured formats.\\nThe case highlights two essential directions for enhancing robustness. RAG systems require layout-\\naware parsing mechanisms that can recognize and adapt to structural irregularities rather than\\nimposing rigid formatting assumptions. Additionally, integrating visual processing capabilities\\ncould significantly improve noise resilience, as visual models can leverage spatial relationships and\\ncontextual design cues that are lost in purely structural representations.\\n18')]\n",
      "Total pages loaded: 108\n",
      "First page preview:\n",
      "AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON\n",
      "AGENTIC RAG\n",
      "Aditi Singh\n",
      "Department of Computer Science\n",
      "Cleveland State University\n",
      "Cleveland, OH, USA\n",
      "a.singh22@csuohio.edu\n",
      "Abul Ehtesham\n",
      "The Davey Tree Expert Company\n",
      "Kent, OH, USA\n",
      "abul.ehtesham@davey.com\n",
      "Saket Kumar\n",
      "The MathWorks Inc\n",
      "Natick, MA, USA\n",
      "saketk@mathworks.com\n",
      "Tala Talaei Khoei\n",
      "Khoury College of Computer Science\n",
      "Roux Institute at Northeastern University\n",
      "Portland, ME, USA\n",
      "t.talaeikhoei@northeastern.edu\n",
      "ABSTRACT\n",
      "Large Language Models (\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "pdf_loader= DirectoryLoader(\n",
    "    \"../data/pdf\", \n",
    "    glob= \"**/*.pdf\", #Pattern to match\n",
    "    loader_cls= PyMuPDFLoader,\n",
    "    show_progress= False\n",
    ")\n",
    "pdf_doc= pdf_loader.load()\n",
    "print(pdf_doc)\n",
    "\n",
    "print(f\"Total pages loaded: {len(pdf_doc)}\")\n",
    "print(\"First page preview:\")\n",
    "print(pdf_doc[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ee5d253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c6cd30",
   "metadata": {},
   "source": [
    "Embedding and VectorSrore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "527b9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd05e084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model Loaded! Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x130902710>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handels embeddings usinf SentenceTransformer\"\"\"\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize embedding manager\n",
    "        model_name: available in HuggingFace\n",
    "        \"\"\"\n",
    "        self.model_name= model_name\n",
    "        self.model= None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Loads the model\"\"\"\n",
    "        try:\n",
    "            print(\"Loading model...\")\n",
    "            self.model= SentenceTransformer(self.model_name)\n",
    "            print(f\"Model Loaded! Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error loading the model\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str])-> np.ndarray:\n",
    "        \"\"\"Generate embeddings for the lsit of texts\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"MOdel not loaded\")\n",
    "        print(\"Generating embeddings...\")\n",
    "        embeddings= self.model.encode(texts, show_progress_bar= True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "    def get_sentence_embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the embedding dimension for the mdoel\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"MOdel not loaded\")\n",
    "        return seld.model.get_sentence_embedding_dimension()\n",
    "\n",
    "\n",
    "#Initialize the embedding manager\n",
    "embedding_manager= EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0ef14",
   "metadata": {},
   "source": [
    "Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbb3dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store initialized. Collection: pdf-documents\n",
      "Existing documents in collection: 565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1526efeb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in ChromaDB\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf-documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"Initialize vector store\n",
    "        collection_name: Name of ChromaDB collection\n",
    "        persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.initialize_store()\n",
    "\n",
    "    def initialize_store(self):\n",
    "        \"\"\"Initialize the ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            \n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\n",
    "                    \"description\": \"PDF document embeddings for RAG\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            print(f\"Vector Store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error initializing the store\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"Add documents and their embeddings to vector store\n",
    "        documents: List of LangChain Document objects\n",
    "        embeddings: Corresponding embeddings\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match the number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['context_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error storing to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5518056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for better RAG performance.\n",
    "    \n",
    "    Parameters:\n",
    "    - chunk_size: Maximum characters per chunk (adjust based on your LLM)\n",
    "    - chunk_overlap: Characters to overlap between chunks (preserves context)\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, # Each chunk: ~1000 characters\n",
    "        chunk_overlap=chunk_overlap, # 200 chars overlap for context\n",
    "        length_function=len, # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # Split hierarchy\n",
    "    )\n",
    "    # Actually split the documents\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show what a chunk looks like\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6549f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 108 documents into 506 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON\n",
      "AGENTIC RAG\n",
      "Aditi Singh\n",
      "Department of Computer Science\n",
      "Cleveland State University\n",
      "Cleveland, OH, USA\n",
      "a.singh22@csuohio.edu\n",
      "Abul Ehtesham\n",
      "The Davey T...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 0}, page_content='AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON\\nAGENTIC RAG\\nAditi Singh\\nDepartment of Computer Science\\nCleveland State University\\nCleveland, OH, USA\\na.singh22@csuohio.edu\\nAbul Ehtesham\\nThe Davey Tree Expert Company\\nKent, OH, USA\\nabul.ehtesham@davey.com\\nSaket Kumar\\nThe MathWorks Inc\\nNatick, MA, USA\\nsaketk@mathworks.com\\nTala Talaei Khoei\\nKhoury College of Computer Science\\nRoux Institute at Northeastern University\\nPortland, ME, USA\\nt.talaeikhoei@northeastern.edu\\nABSTRACT\\nLarge Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human-\\nlike text generation and natural language understanding. However, their reliance on static training\\ndata limits their ability to respond to dynamic, real-time queries, resulting in outdated or inaccurate\\noutputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\\nintegrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 0}, page_content='outputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\\nintegrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite\\nits promise, traditional RAG systems are constrained by static workflows and lack the adaptability\\nrequired for multi-step reasoning and complex task management.\\nAgentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding\\nautonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflec-\\ntion, planning, tool use, and multi-agent collaboration to dynamically manage retrieval strategies,\\niteratively refine contextual understanding, and adapt workflows through clearly defined operational\\nstructures ranging from sequential steps to adaptive collaboration. This integration enables Agentic\\nRAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\\napplications.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 0}, page_content='RAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\\napplications.\\nThis survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational\\nprinciples and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG archi-\\ntectures, highlights key applications in industries such as healthcare, finance, and education, and exam-\\nines practical implementation strategies. Additionally, it addresses challenges in scaling these systems,\\nensuring ethical decision-making, and optimizing performance for real-world applications, while\\nproviding detailed insights into frameworks and tools for implementing Agentic RAG 1. The GitHub\\nlink for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey.\\nKeywords Large Language Models (LLMs) · Artificial Intelligence (AI) · Natural Language Understanding ·'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 0}, page_content='link for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey.\\nKeywords Large Language Models (LLMs) · Artificial Intelligence (AI) · Natural Language Understanding ·\\nRetrieval-Augmented Generation (RAG) · Agentic RAG · Autonomous AI Agents · Reflection · Planning · Tool\\nUse · Multi-Agent Collaboration · Agentic Patterns · Contextual Understanding · Dynamic Adaptability · Scalability ·\\nReal-Time Data Retrieval · Taxonomy of Agentic RAG · Healthcare Applications · Finance Applications · Educational\\nApplications · Ethical AI Decision-Making · Performance Optimization · Multi-Step Reasoning\\n1GitHub link: https://github.com/asinghcsu/AgenticRAG-Survey\\narXiv:2501.09136v3  [cs.AI]  4 Feb 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 1}, page_content='1\\nIntroduction\\nLarge Language Models (LLMs) [1, 2] [3], such as OpenAI’s GPT-4, Google’s PaLM, and Meta’s LLaMA, have signifi-\\ncantly transformed artificial intelligence (AI) with their ability to generate human-like text and perform complex natural\\nlanguage processing tasks. These models have driven innovation across diverse domains, including conversational\\nagents [4], automated content creation, and real-time translation. Recent advancements have extended their capabilities\\nto multimodal tasks, such as text-to-image and text-to-video generation [5], enabling the creation and editing of videos\\nand images from detailed prompts [6], which broadens the potential applications of generative AI.\\nDespite these advancements, LLMs face significant limitations due to their reliance on static pre-training data. This\\nreliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 1}, page_content='reliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world\\nscenarios. These challenges emphasize the need for systems that can integrate real-time data and dynamically refine\\nresponses to maintain contextual relevance and accuracy.\\nRetrieval-Augmented Generation (RAG) [8, 9] emerged as a promising solution to these challenges. By combining\\nthe generative capabilities of LLMs with external retrieval mechanisms [10], RAG systems enhance the relevance and\\ntimeliness of responses. These systems retrieve real-time information from sources such as knowledge bases [11], APIs,\\nor the web, effectively bridging the gap between static training data and the demands of dynamic applications. However,\\ntraditional RAG workflows remain limited by their linear and static design, which restricts their ability to perform\\ncomplex multi-step reasoning, integrate deep contextual understanding, and iteratively refine responses.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 1}, page_content='complex multi-step reasoning, integrate deep contextual understanding, and iteratively refine responses.\\nThe evolution of agents [12] has significantly enhanced the capabilities of AI systems. Modern agents, including\\nLLM-powered and mobile agents [13], are intelligent entities capable of perceiving, reasoning, and autonomously\\nexecuting tasks. These agents leverage agentic patterns, such as reflection [14], planning [15], tool use, and multi-agent\\ncollaboration [16], to enhance decision-making and adaptability.\\nFurthermore, these agents employ agentic workflow patterns [12, 13], such as prompt chaining, routing, parallelization,\\norchestrator-worker models, and evaluator-optimizer , to structure and optimize task execution. By integrating these\\npatterns, Agentic RAG systems can efficiently manage dynamic workflows and address complex problem-solving\\nscenarios. The convergence of RAG and agentic intelligence has given rise to Agentic Retrieval-Augmented Generation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 1}, page_content='scenarios. The convergence of RAG and agentic intelligence has given rise to Agentic Retrieval-Augmented Generation\\n(Agentic RAG) [14], a paradigm that integrates agents into the RAG pipeline. Agentic RAG enables dynamic retrieval\\nstrategies, contextual understanding, and iterative refinement [15], allowing for adaptive and efficient information\\nprocessing. Unlike traditional RAG, Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant\\ninformation, and refine responses, excelling in scenarios requiring precision and adaptability. The overview of Agentic\\nRAG is in figure 1.\\nThis survey explores the foundational principles, taxonomy, and applications of Agentic RAG. It provides a comprehen-\\nsive overview of RAG paradigms, such as Naïve RAG, Modular RAG, and Graph RAG [16], alongside their evolution\\ninto Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 1}, page_content='into Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications\\nacross domains such as healthcare [17, 18], finance, and education [19], and insights into implementation strategies,\\nbenchmarks, and ethical considerations.\\nThe structure of this paper is as follows: Section 2 introduces RAG and its evolution, highlighting the limitations of\\ntraditional approaches. Section 3 elaborates on the principles of agentic intelligence and agentic patterns. Section 4\\nelaborates agentic workflow patterns. Section 5 provides a taxonomy of Agentic RAG systems, including single-agent,\\nmulti-agent, and graph-based frameworks. Section 6 provides comparative analysis of Agentic RAG frameworks.\\nSection 7 examines applications of Agentic RAG, while Section 8 discusses implementation tools and frameworks.\\nSection 9 focuses on benchmarks and dataset, and Section 10 concludes with future directions for Agentic RAG systems.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 1}, page_content='Section 9 focuses on benchmarks and dataset, and Section 10 concludes with future directions for Agentic RAG systems.\\n2\\nFoundations of Retrieval-Augmented Generation\\n2.1\\nOverview of Retrieval-Augmented Generation (RAG)\\nRetrieval-Augmented Generation (RAG) represents a significant advancement in the field of artificial intelligence,\\ncombining the generative capabilities of Large Language Models (LLMs) with real-time data retrieval. While LLMs\\nhave demonstrated remarkable capabilities in natural language processing, their reliance on static pre-trained data\\noften results in outdated or incomplete responses. RAG addresses this limitation by dynamically retrieving relevant\\ninformation from external sources and incorporating it into the generative process, enabling contextually accurate and\\nup-to-date outputs.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 2}, page_content='Figure 1: An Overview of Agentic RAG\\n2.2\\nCore Components of RAG\\nThe architecture of RAG systems integrates three primary components (Figure2):\\n• Retrieveal: Responsible for querying external data sources such as knowledge bases, APIs, or vector databases.\\nAdvanced retrievers leverage dense vector search and transformer-based models to improve retrieval precision\\nand semantic relevance.\\n• Augmentation: Processes retrieved data, extracting and summarizing the most relevant information to align\\nwith the query context.\\n• Generation: Combines retrieved information with the LLM’s pre-trained knowledge to generate coherent,\\ncontextually appropriate responses.\\n2.3\\nEvolution of RAG Paradigms\\nThe field of Retrieval-Augmented Generation (RAG) has evolved significantly to address the increasing complexity of\\nreal-world applications, where contextual accuracy, scalability, and multi-step reasoning are critical. What began as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 2}, page_content='real-world applications, where contextual accuracy, scalability, and multi-step reasoning are critical. What began as\\nsimple keyword-based retrieval has transitioned into sophisticated, modular, and adaptive systems capable of integrating\\ndiverse data sources and autonomous decision-making processes. This evolution underscores the growing need for\\nRAG systems to handle complex queries efficiently and effectively.\\nThis section examines the progression of RAG paradigms, presenting key stages of development—Naïve RAG,\\nAdvanced RAG, Modular RAG, Graph RAG, and Agentic RAG alongside their defining characteristics, strengths, and\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 3}, page_content='Figure 2: Core Components of RAG\\nlimitations. By understanding the evolution of these paradigms, readers can appreciate the advancements made in\\nretrieval and generative capabilities and their application in various domains\\n2.3.1\\nNaïve RAG\\nNaïve RAG [20] represents the foundational implementation of retrieval-augmented generation. Figure 3 illustrates the\\nsimple retrieve-read workflow of Naive RAG, focusing on keyword-based retrieval and static datasets.. These systems\\nrely on simple keyword-based retrieval techniques, such as TF-IDF and BM25, to fetch documents from static datasets.\\nThe retrieved documents are then used to augment the language model’s generative capabilities.\\nFigure 3: An Overview of Naive RAG.\\nNaïve RAG is characterized by its simplicity and ease of implementation, making it suitable for tasks involving\\nfact-based queries with minimal contextual complexity. However, it suffers from several limitations:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 3}, page_content='fact-based queries with minimal contextual complexity. However, it suffers from several limitations:\\n• Lack of Contextual Awareness: Retrieved documents often fail to capture the semantic nuances of the query\\ndue to reliance on lexical matching rather than semantic understanding.\\n• Fragmented Outputs: The absence of advanced preprocessing or contextual integration often leads to\\ndisjointed or overly generic responses.\\n• Scalability Issues: Keyword-based retrieval techniques struggle with large datasets, often failing to identify\\nthe most relevant information.\\nDespite these limitations, Naïve RAG systems provided a critical proof-of-concept for integrating retrieval with\\ngeneration, laying the foundation for more sophisticated paradigms.\\n2.3.2\\nAdvanced RAG\\nAdvanced RAG [20] systems build upon the limitations of Naïve RAG by incorporating semantic understanding and\\nenhanced retrieval techniques. Figure 4 highlights the semantic enhancements in retrieval and the iterative, context-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 3}, page_content='enhanced retrieval techniques. Figure 4 highlights the semantic enhancements in retrieval and the iterative, context-\\naware pipeline of Advanced RAG. These systems leverage dense retrieval models, such as Dense Passage Retrieval\\n(DPR), and neural ranking algorithms to improve retrieval precision.\\nKey features of Advanced RAG include:\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 4}, page_content='Figure 4: Overview of Advanced RAG\\n• Dense Vector Search: Queries and documents are represented in high-dimensional vector spaces, enabling\\nbetter semantic alignment between the user query and retrieved documents.\\n• Contextual Re-Ranking: Neural models re-rank retrieved documents to prioritize the most contextually\\nrelevant information.\\n• Iterative Retrieval: Advanced RAG introduces multi-hop retrieval mechanisms, enabling reasoning across\\nmultiple documents for complex queries.\\nThese advancements make Advanced RAG suitable for applications requiring high precision and nuanced understanding,\\nsuch as research synthesis and personalized recommendations. However, challenges such as computational overhead\\nand limited scalability persist, particularly when dealing with large datasets or multi-step queries.\\n2.3.3\\nModular RAG\\nModular RAG [20] represents the latest evolution in RAG paradigms, emphasizing flexibility and customization.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 4}, page_content='2.3.3\\nModular RAG\\nModular RAG [20] represents the latest evolution in RAG paradigms, emphasizing flexibility and customization.\\nThese systems decompose the retrieval and generation pipeline into independent, reusable components, enabling\\ndomain-specific optimization and task adaptability. Figure 5 demonstrates the modular architecture, showcasing hybrid\\nretrieval strategies, composable pipelines, and external tool integration.\\nKey innovations in Modular RAG include:\\n• Hybrid Retrieval Strategies: Combining sparse retrieval methods (e.g., a sparse encoder-BM25) with dense\\nretrieval techniques [21] (e.g., DPR - Dense Passage Retrieval ) to maximize accuracy across diverse query\\ntypes.\\n• Tool Integration: Incorporating external APIs, databases, or computational tools to handle specialized tasks,\\nsuch as real-time data analysis or domain-specific computations.\\n• Composable Pipelines: Modular RAG enables retrievers, generators, and other components to be replaced,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 4}, page_content='such as real-time data analysis or domain-specific computations.\\n• Composable Pipelines: Modular RAG enables retrievers, generators, and other components to be replaced,\\nenhanced, or reconfigured independently, allowing high adaptability to specific use cases.\\nFor instance, a Modular RAG system designed for financial analytics might retrieve live stock prices via APIs, analyze\\nhistorical trends using dense retrieval, and generate actionable investment insights through a tailored language model.\\nThis modularity and customization make Modular RAG ideal for complex, multi-domain tasks, offering both scalability\\nand precision.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 5}, page_content='Figure 5: Overview of Modular RAG\\n2.3.4\\nGraph RAG\\nGraph RAG [16] extends traditional Retrieval-Augmented Generation systems by integrating graph-based data structures\\nas illustrated in Figure 6. These systems leverage the relationships and hierarchies within graph data to enhance multi-\\nhop reasoning and contextual enrichment. By incorporating graph-based retrieval, Graph RAG enables richer and more\\naccurate generative outputs, particularly for tasks requiring relational understanding.\\nGraph RAG is characterized by its ability to:\\n• Node Connectivity: Captures and reasons over relationships between entities.\\n• Hierarchical Knowledge Management: Handles structured and unstructured data through graph-based\\nhierarchies.\\n• Context Enrichment: Adds relational understanding by leveraging graph-based pathways.\\nHowever, Graph RAG has some limitations:\\n• Limited Scalability: The reliance on graph structures can restrict scalability, especially with extensive data\\nsources.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 5}, page_content='However, Graph RAG has some limitations:\\n• Limited Scalability: The reliance on graph structures can restrict scalability, especially with extensive data\\nsources.\\n• Data Dependency: High-quality graph data is essential for meaningful outputs, limiting its applicability in\\nunstructured or poorly annotated datasets.\\n• Complexity of Integration: Integrating graph data with unstructured retrieval systems increases design and\\nimplementation complexity.\\nGraph RAG is well-suited for applications such as healthcare diagnostics, legal research, and other domains where\\nreasoning over structured relationships is crucial.\\n2.3.5\\nAgentic RAG\\nAgentic RAG represents a paradigm shift by introducing autonomous agents capable of dynamic decision-making\\nand workflow optimization. Unlike static systems, Agentic RAG employs iterative refinement and adaptive retrieval\\nstrategies to address complex, real-time, and multi-domain queries. This paradigm leverages the modularity of retrieval'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 5}, page_content='strategies to address complex, real-time, and multi-domain queries. This paradigm leverages the modularity of retrieval\\nand generation processes while introducing agent-based autonomy.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 6}, page_content='Figure 6: Overview of Graph RAG\\nKey characteristics of Agentic RAG include:\\n• Autonomous Decision-Making: Agents independently evaluate and manage retrieval strategies based on\\nquery complexity.\\n• Iterative Refinement: Incorporates feedback loops to improve retrieval accuracy and response relevance.\\n• Workflow Optimization: Dynamically orchestrates tasks, enabling efficiency in real-time applications.\\nDespite its advancements, Agentic RAG faces some challenges:\\n• Coordination Complexity: Managing interactions between agents requires sophisticated orchestration\\nmechanisms.\\n• Computational Overhead: The use of multiple agents increases resource requirements for complex work-\\nflows.\\n• Scalability Limitations: While scalable, the dynamic nature of the system can strain computational resources\\nfor high query volumes.\\nAgentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\\ndynamic adaptability and contextual precision are paramount.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 6}, page_content='for high query volumes.\\nAgentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\\ndynamic adaptability and contextual precision are paramount.\\n2.4\\nChallenges and Limitations of Traditional RAG Systems\\nTraditional Retrieval-Augmented Generation (RAG) systems have significantly expanded the capabilities of Large\\nLanguage Models (LLMs) by integrating real-time data retrieval. However, these systems still face critical challenges\\nthat hinder their effectiveness in complex, real-world applications. The most notable limitations revolve around\\ncontextual integration, multi-step reasoning, and scalability and latency issues.\\n2.4.1\\nContextual Integration\\nEven when RAG systems successfully retrieve relevant information, they often struggle to seamlessly incorporate it\\ninto generated responses. The static nature of retrieval pipelines and limited contextual awareness lead to fragmented,\\ninconsistent, or overly generic outputs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 6}, page_content='into generated responses. The static nature of retrieval pipelines and limited contextual awareness lead to fragmented,\\ninconsistent, or overly generic outputs.\\nExample: A query such as, \"What are the latest advancements in Alzheimer’s research and their implications for\\nearly-stage treatment?\" might yield relevant research papers and medical guidelines. However, traditional RAG\\nsystems often fail to synthesize these findings into a coherent explanation that connects the new treatments to specific\\npatient scenarios. Similarly, for a query like, \"What are the best sustainable practices for small-scale agriculture in\\narid regions?\", traditional systems might retrieve documents on general agricultural methods but overlook critical\\nsustainability practices tailored to arid environments.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 7}, page_content='Table 1: Comparative Analysis of RAG Paradigms\\nParadigm\\nKey Features\\nStrengths\\nNaïve RAG\\n• Keyword-based retrieval (e.g.,\\nTF-IDF, BM25)\\n• Simple and easy to implement\\n• Suitable for fact-based queries\\nAdvanced RAG\\n• Dense retrieval models (e.g.,\\nDPR)\\n• Neural ranking and re-ranking\\n• Multi-hop retrieval\\n• High precision retrieval\\n• Improved contextual relevance\\nModular RAG\\n• Hybrid retrieval (sparse and\\ndense)\\n• Tool and API integration\\n• Composable, domain-specific\\npipelines\\n• High flexibility and customization\\n• Suitable for diverse applications\\n• Scalable\\nGraph RAG\\n• Integration\\nof\\ngraph-based\\nstructures\\n• Multi-hop reasoning\\n• Contextual\\nenrichment\\nvia\\nnodes\\n• Relational reasoning capabilities\\n• Mitigates hallucinations\\n• Ideal for structured data tasks\\nAgentic RAG\\n• Autonomous agents\\n• Dynamic decision-making\\n• Iterative refinement and work-\\nflow optimization\\n• Adaptable to real-time changes\\n• Scalable for multi-domain tasks\\n• High accuracy\\n2.4.2\\nMulti-Step Reasoning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 7}, page_content='• Dynamic decision-making\\n• Iterative refinement and work-\\nflow optimization\\n• Adaptable to real-time changes\\n• Scalable for multi-domain tasks\\n• High accuracy\\n2.4.2\\nMulti-Step Reasoning\\nMany real-world queries require iterative or multi-hop reasoning—retrieving and synthesizing information across\\nmultiple steps. Traditional RAG systems are often ill-equipped to refine retrieval based on intermediate insights or user\\nfeedback, resulting in incomplete or disjointed responses.\\nExample: A complex query like, \"What lessons from renewable energy policies in Europe can be applied to developing\\nnations, and what are the potential economic impacts?\" demands the orchestration of multiple types of information,\\nincluding policy data, contextualization for developing regions, and economic analysis. Traditional RAG systems\\ntypically fail to connect these disparate elements into a cohesive response.\\n2.4.3\\nScalability and Latency Issues'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 7}, page_content='typically fail to connect these disparate elements into a cohesive response.\\n2.4.3\\nScalability and Latency Issues\\nAs the volume of external data sources grows, querying and ranking large datasets becomes increasingly computationally\\nintensive. This results in significant latency, which undermines the system’s ability to provide timely responses in\\nreal-time applications.\\nExample: In time-sensitive settings such as financial analytics or live customer support, delays caused by querying\\nmultiple databases or processing large document sets can hinder the system’s overall utility. For example, a delay in\\nretrieving market trends during high-frequency trading could result in missed opportunities.\\n2.5\\nAgentic RAG: A Paradigm Shift\\nTraditional RAG systems, with their static workflows and limited adaptability, often struggle to handle dynamic, multi-\\nstep reasoning and complex real-world tasks. These limitations have spurred the integration of agentic intelligence,\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 8}, page_content='resulting in Agentic RAG. By incorporating autonomous agents capable of dynamic decision-making, iterative reasoning,\\nand adaptive retrieval strategies, Agentic RAG builds on the modularity of earlier paradigms while overcoming their\\ninherent constraints. This evolution enables more complex, multi-domain tasks to be addressed with enhanced precision\\nand contextual understanding, positioning Agentic RAG as a cornerstone for next-generation AI applications. In\\nparticular, Agentic RAG systems reduce latency through optimized workflows and refine outputs iteratively, tackling\\nthe very challenges that have historically hindered traditional RAG’s scalability and effectiveness.\\n3\\nCore Principles and Background of Agentic Intelligence\\nAgentic Intelligence forms the foundation of Agentic Retrieval-Augmented Generation (RAG) systems, enabling them\\nto transcend the static and reactive nature of traditional RAG. By integrating autonomous agents capable of dynamic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 8}, page_content='to transcend the static and reactive nature of traditional RAG. By integrating autonomous agents capable of dynamic\\ndecision-making, iterative reasoning, and collaborative workflows, Agentic RAG systems exhibit enhanced adaptability\\nand precision. This section explores the core principles underpinning agentic intelligence.\\nComponents of an AI Agent.\\nIn essence, an AI agent comprises (Figure. 7):\\n• LLM (with defined Role and Task): Serves as the agent’s primary reasoning engine and dialogue interface.\\nIt interprets user queries, generates responses, and maintains coherence.\\n• Memory (Short-Term and Long-Term): Captures context and relevant data across interactions. Short-term\\nmemory [22] tracks immediate conversation state, while long-term memory [22]stores accumulated knowledge\\nand agent experiences.\\n• Planning (Reflection & Self-Critique): Guides the agent’s iterative reasoning process through reflection,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 8}, page_content='and agent experiences.\\n• Planning (Reflection & Self-Critique): Guides the agent’s iterative reasoning process through reflection,\\nquery routing, or self-critique[23], ensuring that complex tasks are broken down effectively [24].\\n• Tools Vector Search, Web Search, APIs, etc.): Expands the agent’s capabilities beyond text generation,\\nenabling access to external resources, real-time data, or specialized computations.\\nFigure 7: An Overview of AI Agents\\nAgentic Patterns [25, 26] provide structured methodologies that guide the behavior of agents in Agentic Retrieval-\\nAugmented Generation (RAG) systems. These patterns enable agents to dynamically adapt, plan, and collaborate,\\nensuring that the system can handle complex, real-world tasks with precision and scalability. Four key patterns underpin\\nagentic workflows:\\n3.1\\nReflection\\nReflection is a foundational design pattern in agentic workflows, enabling agents to iteratively evaluate and refine their'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 8}, page_content='agentic workflows:\\n3.1\\nReflection\\nReflection is a foundational design pattern in agentic workflows, enabling agents to iteratively evaluate and refine their\\noutputs. By incorporating self-feedback mechanisms, agents can identify and address errors, inconsistencies, and areas\\nfor improvement, enhancing performance across tasks like code generation, text production, and question answering (\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 9}, page_content='as shown in Figure 8). In practical use, Reflection involves prompting an agent to critique its outputs for correctness,\\nstyle, and efficiency, then incorporating this feedback into subsequent iterations. External tools, such as unit tests or\\nweb searches, can further enhance this process by validating results and highlighting gaps.\\nIn multi-agent systems, Reflection can involve distinct roles, such as one agent generating outputs while another\\ncritiques them, fostering collaborative improvement. For instance, in legal research, agents can iteratively refine\\nresponses by re-evaluating retrieved case law, ensuring accuracy and comprehensiveness. Reflection has demonstrated\\nsignificant performance improvements in studies like Self-Refine [27], Reflexion [28], and CRITIC [23].\\nFigure 8: An Overview of Agentic Self- Reflection\\n3.2\\nPlanning\\nPlanning [24] is a key design pattern in agentic workflows that enables agents to autonomously decompose complex tasks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 9}, page_content='Figure 8: An Overview of Agentic Self- Reflection\\n3.2\\nPlanning\\nPlanning [24] is a key design pattern in agentic workflows that enables agents to autonomously decompose complex tasks\\ninto smaller, manageable subtasks. This capability is essential for multi-hop reasoning and iterative problem-solving in\\ndynamic and uncertain scenarios as shown in Figure 9a.\\nBy leveraging planning, agents can dynamically determine the sequence of steps needed to accomplish a larger objective.\\nThis adaptability allows agents to handle tasks that cannot be predefined, ensuring flexibility in decision-making.\\nWhile powerful, Planning can produce less predictable outcomes compared to deterministic workflows like Reflection.\\nPlanning is particularly suited for tasks that require dynamic adaptation, where predefined workflows are insufficient.\\nAs the technology matures, its potential to drive innovative applications across domains will continue to grow.\\n3.3\\nTool Use'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 9}, page_content='As the technology matures, its potential to drive innovative applications across domains will continue to grow.\\n3.3\\nTool Use\\nTool Use enables agents to extend their capabilities by interacting with external tools, APIs, or computational resources\\nas illustrated in 9b. This pattern allows agents to gather information, perform computations, and manipulate data beyond\\ntheir pre-trained knowledge. By dynamically integrating tools into workflows, agents can adapt to complex tasks and\\nprovide more accurate and contextually relevant outputs.\\nModern agentic workflows incorporate tool use for a variety of applications, including information retrieval, computa-\\ntional reasoning, and interfacing with external systems. The implementation of this pattern has evolved significantly\\nwith advancements like GPT-4’s function calling capabilities and systems capable of managing access to numerous'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 9}, page_content='with advancements like GPT-4’s function calling capabilities and systems capable of managing access to numerous\\ntools. These developments facilitate sophisticated workflows where agents autonomously select and execute the most\\nrelevant tools for a given task.\\nWhile tool use significantly enhances agentic workflows, challenges remain in optimizing the selection of tools,\\nparticularly in contexts with a large number of available options. Techniques inspired by retrieval-augmented generation\\n(RAG), such as heuristic-based selection, have been proposed to address this issue.\\n3.4\\nMulti-Agent\\nMulti-agent collaboration [29] is a key design pattern in agentic workflows that enables task specialization and parallel\\nprocessing. Agents communicate and share intermediate results, ensuring the overall workflow remains efficient and\\ncoherent. By distributing subtasks among specialized agents, this pattern improves the scalability and adaptability\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 10}, page_content='(a) An Overview of Agentic Planning\\n(b) An Overview of Tool Use\\nFigure 9: Overview of Agentic Planning and Tool Use\\nof complex workflows. Multi-agent systems allow developers to decompose intricate tasks into smaller, manageable\\nsubtasks assigned to different agents. This approach not only enhances task performance but also provides a robust\\nframework for managing complex interactions. Each agent operates with its own memory and workflow, which can\\ninclude the use of tools, reflection, or planning, enabling dynamic and collaborative problem-solving (see Figure 10).\\nWhile multi-agent collaboration offers significant potential, it is a less predictable design pattern compared to more\\nmature workflows like Reflection and Tool Use. Nevertheless, emerging frameworks such as AutoGen, Crew AI, and\\nLangGraph are providing new avenues for implementing effective multi-agent solutions.\\nFigure 10: An Overview of MultiAgent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 10}, page_content='LangGraph are providing new avenues for implementing effective multi-agent solutions.\\nFigure 10: An Overview of MultiAgent\\nThese design patterns form the foundation for the success of Agentic RAG systems. By structuring workflows—from\\nsimple, sequential steps to more adaptive, collaborative processes—these patterns enable systems to dynamically\\nadapt their retrieval and generative strategies to the diverse and ever-changing demands of real-world environments.\\nLeveraging these patterns, agents are capable of handling iterative, context-aware tasks that significantly exceed the\\ncapabilities of traditional RAG systems.\\n4\\nAgentic Workflow Patterns: Adaptive Strategies for Dynamic Collaboration\\nAgentic workflow patterns, [12, 13] structure LLM-based applications to optimize performance, accuracy, and efficiency.\\nDifferent approaches are suitable depending on task complexity and processing requirements.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 11}, page_content='4.1\\nPrompt Chaining: Enhancing Accuracy Through Sequential Processing\\nPrompt chaining [12, 13] decomposes a complex task into multiple steps, where each step builds upon the previous\\none. This structured approach improves accuracy by simplifying each subtask before moving forward. However, it may\\nincrease latency due to sequential processing.\\nFigure 11: Illustration of Prompt Chaining Workflow\\nWhen to Use: This workflow is most effective when a task can be broken down into fixed subtasks, each contributing\\nto the final output. It is particularly useful in scenarios where step-by-step reasoning enhances accuracy.\\nExample Applications:\\n• Generating marketing content in one language and then translating it into another while preserving nuances.\\n• Structuring document creation by first generating an outline, verifying its completeness, and then developing\\nthe full text.\\n4.2\\nRouting:Directing Inputs to Specialized Processes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 11}, page_content='• Structuring document creation by first generating an outline, verifying its completeness, and then developing\\nthe full text.\\n4.2\\nRouting:Directing Inputs to Specialized Processes\\nRouting [12, 13] involves classifying an input and directing it to an appropriate specialized prompt or process. This\\nmethod ensures distinct queries or tasks are handled separately, improving efficiency and response quality.\\nFigure 12: Illustration Routing Workflow\\nWhen to Use: Ideal for scenarios where different types of input require distinct handling strategies, ensuring optimized\\nperformance for each category.\\nExample Applications:\\n• Directing customer service queries into categories such as technical support, refund requests, or general\\ninquiries.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 12}, page_content='• Assigning simple queries to smaller models for cost efficiency, while complex requests go to advanced models.\\n4.3\\nParallelization: Speeding Up Processing Through Concurrent Execution\\nParallelization [12, 13] divides a task into independent processes that run simultaneously, reducing latency and\\nimproving throughput. It can be categorized into sectioning (independent subtasks) and voting (multiple outputs for\\naccuracy).\\nFigure 13: Illustration of Parallelization Workflow\\nWhen to Use: Useful when tasks can be executed independently to enhance speed or when multiple outputs improve\\nconfidence.\\nExample Applications:\\n• Sectioning: Splitting tasks like content moderation, where one model screens input while another generates a\\nresponse.\\n• Voting: Using multiple models to cross-check code for vulnerabilities or analyze content moderation decisions.\\n4.4\\nOrchestrator-Workers: Dynamic Task Delegation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 12}, page_content='response.\\n• Voting: Using multiple models to cross-check code for vulnerabilities or analyze content moderation decisions.\\n4.4\\nOrchestrator-Workers: Dynamic Task Delegation\\nThis workflow [12, 13] features a central orchestrator model that dynamically breaks tasks into subtasks, assigns them\\nto specialized worker models, and compiles the results. Unlike parallelization, it adapts to varying input complexity.\\nFigure 14: Illustration of Orchestrator-Workers Workflow\\nWhen to Use: Best suited for tasks requiring dynamic decomposition and real-time adaptation, where subtasks are not\\npredefined.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 13}, page_content='Example Applications:\\n• Automatically modifying multiple files in a codebase based on the nature of requested changes.\\n• Conducting real-time research by gathering and synthesizing relevant information from multiple sources.\\n4.5\\nEvaluator-Optimizer: Refining Output Through Iteration\\nThe evaluator-optimizer [12, 13] workflow iteratively improves content by generating an initial output and refining it\\nbased on feedback from an evaluation model.\\nFigure 15: Illustration of Evaluator-Optimizer Workflow\\nWhen to Use: Effective when iterative refinement significantly enhances response quality, especially when clear\\nevaluation criteria exist.\\nExample Applications:\\n• Improving literary translations through multiple evaluation and refinement cycles.\\n• Conducting multi-round research queries where additional iterations refine search results.\\n5\\nTaxonomy of Agentic RAG Systems\\nAgentic Retrieval-Augmented Generation (RAG) systems can be categorized into distinct architectural frameworks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 13}, page_content='5\\nTaxonomy of Agentic RAG Systems\\nAgentic Retrieval-Augmented Generation (RAG) systems can be categorized into distinct architectural frameworks\\nbased on their complexity and design principles. These include single-agent architectures, multi-agent systems, and hi-\\nerarchical agentic architectures. Each framework is tailored to address specific challenges and optimize performance for\\ndiverse applications. This section provides a detailed taxonomy of these architectures, highlighting their characteristics,\\nstrengths, and limitations.\\n5.1\\nSingle-Agent Agentic RAG: Router\\nA Single-Agent Agentic RAG: [30] serves as a centralized decision-making system where a single agent manages the\\nretrieval, routing, and integration of information (as shown in Figure. 16). This architecture simplifies the system by\\nconsolidating these tasks into one unified agent, making it particularly effective for setups with a limited number of\\ntools or data sources.\\nWorkflow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 13}, page_content='consolidating these tasks into one unified agent, making it particularly effective for setups with a limited number of\\ntools or data sources.\\nWorkflow\\n1. Query Submission and Evaluation: The process begins when a user submits a query. A coordinating\\nagent (or master retrieval agent) receives the query and analyzes it to determine the most suitable sources of\\ninformation.\\n2. Knowledge Source Selection: Based on the query’s type, the coordinating agent chooses from a variety of\\nretrieval options:\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 14}, page_content='• Structured Databases: For queries requiring tabular data access, the system may use a Text-to-SQL\\nengine that interacts with databases like PostgreSQL or MySQL.\\n• Semantic Search: When dealing with unstructured information, it retrieves relevant documents (e.g.,\\nPDFs, books, organizational records) using vector-based retrieval.\\n• Web Search: For real-time or broad contextual information, the system leverages a web search tool to\\naccess the latest online data.\\n• Recommendation Systems: For personalized or contextual queries, the system taps into recommendation\\nengines that provide tailored suggestions.\\n3. Data Integration and LLM Synthesis: Once the relevant data is retrieved from the chosen sources, it is\\npassed to a Large Language Model (LLM). The LLM synthesizes the gathered information, integrating insights\\nfrom multiple sources into a coherent and contextually relevant response.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 14}, page_content='passed to a Large Language Model (LLM). The LLM synthesizes the gathered information, integrating insights\\nfrom multiple sources into a coherent and contextually relevant response.\\n4. Output Generation: Finally, the system delivers a comprehensive, user-facing answer that addresses the\\noriginal query. This response is presented in an actionable, concise format and may optionally include\\nreferences or citations to the sources used.\\nFigure 16: An Overview of Single Agentic RAG\\nKey Features and Advantages.\\n• Centralized Simplicity: A single agent handles all retrieval and routing tasks, making the architecture\\nstraightforward to design, implement, and maintain.\\n• Efficiency & Resource Optimization: With fewer agents and simpler coordination, the system demands\\nfewer computational resources and can handle queries more quickly.\\n• Dynamic Routing: The agent evaluates each query in real-time, selecting the most appropriate knowledge\\nsource (e.g., structured DB, semantic search, web search).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 14}, page_content='• Dynamic Routing: The agent evaluates each query in real-time, selecting the most appropriate knowledge\\nsource (e.g., structured DB, semantic search, web search).\\n• Versatility Across Tools: Supports a variety of data sources and external APIs, enabling both structured and\\nunstructured workflows.\\n• Ideal for Simpler Systems: Suited for applications with well-defined tasks or limited integration requirements\\n(e.g., document retrieval, SQL-based workflows).\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 15}, page_content='Use Case: Customer Support\\nPrompt: Can you tell me the delivery status of my order?\\nSystem Process (Single-Agent Workflow):\\n1. Query Submission and Evaluation:\\n• The user submits the query, which is received by the coordinating agent.\\n• The coordinating agent analyzes the query and determines the most appropriate sources of\\ninformation.\\n2. Knowledge Source Selection:\\n• Retrieves tracking details from the order management database.\\n• Fetches real-time updates from the shipping provider’s API.\\n• Optionally conducts a web search to identify local conditions affecting delivery, such as weather\\nor logistical delays.\\n3. Data Integration and LLM Synthesis:\\n• The relevant data is passed to the LLM, which synthesizes the information into a coherent\\nresponse.\\n4. Output Generation:\\n• The system generates an actionable and concise response, providing live tracking updates and\\npotential alternatives.\\nResponse:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 15}, page_content='response.\\n4. Output Generation:\\n• The system generates an actionable and concise response, providing live tracking updates and\\npotential alternatives.\\nResponse:\\nIntegrated Response: “Your package is currently in transit and expected to arrive tomorrow evening. The live\\ntracking from UPS indicates it is at the regional distribution center.”\\n5.2\\nMulti-Agent Agentic RAG Systems:\\nMulti-Agent RAG [30] represents a modular and scalable evolution of single-agent architectures, designed to handle\\ncomplex workflows and diverse query types by leveraging multiple specialized agents (as shown in Figure 17). Instead\\nof relying on a single agent to manage all tasks—reasoning, retrieval, and response generation—this system distributes\\nresponsibilities across multiple agents, each optimized for a specific role or data source.\\nWorkflow\\n1. Query Submission: The process begins with a user query, which is received by a coordinator agent or master'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 15}, page_content='Workflow\\n1. Query Submission: The process begins with a user query, which is received by a coordinator agent or master\\nretrieval agent. This agent acts as the central orchestrator, delegating the query to specialized retrieval agents\\nbased on the query’s requirements.\\n2. Specialized Retrieval Agents: The query is distributed among multiple retrieval agents, each focusing on a\\nspecific type of data source or task. Examples include:\\n• Agent 1: Handles structured queries, such as interacting with SQL-based databases like PostgreSQL or\\nMySQL.\\n• Agent 2: Manages semantic searches for retrieving unstructured data from sources like PDFs, books, or\\ninternal records.\\n• Agent 3: Focuses on retrieving real-time public information from web searches or APIs.\\n• Agent 4: Specializes in recommendation systems, delivering context-aware suggestions based on user\\nbehavior or profiles.\\n3. Tool Access and Data Retrieval: Each agent routes the query to the appropriate tools or data sources within'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 15}, page_content='behavior or profiles.\\n3. Tool Access and Data Retrieval: Each agent routes the query to the appropriate tools or data sources within\\nits domain, such as:\\n• Vector Search: For semantic relevance.\\n• Text-to-SQL: For structured data.\\n• Web Search: For real-time public information.\\n• APIs: For accessing external services or proprietary systems.\\nThe retrieval process is executed in parallel, allowing for efficient processing of diverse query types.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 16}, page_content='Figure 17: An Overview of Multi-Agent Agentic RAG Systems\\n4. Data Integration and LLM Synthesis: Once retrieval is complete, the data from all agents is passed to a\\nLarge Language Model (LLM). The LLM synthesizes the retrieved information into a coherent and contextually\\nrelevant response, integrating insights from multiple sources seamlessly.\\n5. Output Generation: The system generates a comprehensive response, which is delivered back to the user in\\nan actionable and concise format.\\nKey Features and Advantages.\\n• Modularity: Each agent operates independently, allowing for seamless addition or removal of agents based on\\nsystem requirements.\\n• Scalability: Parallel processing by multiple agents enables the system to handle high query volumes efficiently.\\n• Task Specialization: Each agent is optimized for a specific type of query or data source, improving accuracy\\nand retrieval relevance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 16}, page_content='• Task Specialization: Each agent is optimized for a specific type of query or data source, improving accuracy\\nand retrieval relevance.\\n• Efficiency: By distributing tasks across specialized agents, the system minimizes bottlenecks and enhances\\nperformance for complex workflows.\\n• Versatility: Suitable for applications spanning multiple domains, including research, analytics, decision-\\nmaking, and customer support.\\nChallenges\\n• Coordination Complexity: Managing inter-agent communication and task delegation requires sophisticated\\norchestration mechanisms.\\n• Computational Overhead: Parallel processing of multiple agents can increase resource usage.\\n• Data Integration: Synthesizing outputs from diverse sources into a cohesive response is non-trivial and\\nrequires advanced LLM capabilities.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 17}, page_content='Use Case: Multi-Domain Research Assistant\\nPrompt: What are the economic and environmental impacts of renewable energy adoption in Europe?\\nSystem Process (Multi-Agent Workflow):\\n• Agent 1: Retrieves statistical data from economic databases using SQL-based queries.\\n• Agent 2: Searches for relevant academic papers using semantic search tools.\\n• Agent 3: Performs a web search for recent news and policy updates on renewable energy.\\n• Agent 4: Consults a recommendation system to suggest related content, such as reports or expert\\ncommentary.\\nResponse:\\nIntegrated Response: “Adopting renewable energy in Europe has led to a 20% reduction in greenhouse gas\\nemissions over the past decade, according to EU policy reports. Economically, renewable energy investments\\nhave generated approximately 1.2 million jobs, with significant growth in solar and wind sectors. Recent\\nacademic studies also highlight potential trade-offs in grid stability and energy storage costs.”\\n5.3\\nHierarchical Agentic RAG Systems'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 17}, page_content='academic studies also highlight potential trade-offs in grid stability and energy storage costs.”\\n5.3\\nHierarchical Agentic RAG Systems\\nHierarchical Agentic RAG: [14] systems employ a structured, multi-tiered approach to information retrieval and\\nprocessing, enhancing both efficiency and strategic decision-making as shown in Figure 18. Agents are organized in\\na hierarchy, with higher-level agents overseeing and directing lower-level agents. This structure enables multi-level\\ndecision-making, ensuring that queries are handled by the most appropriate resources.\\nFigure 18: An illustration of Hierarchical Agentic RAG\\nWorkflow\\n1. Query Reception: A user submits a query, received by a top-tier agent responsible for initial assessment and\\ndelegation.\\n2. Strategic Decision-Making: The top-tier agent evaluates the query’s complexity and decides which subor-\\ndinate agents or data sources to prioritize. Certain databases, APIs, or retrieval tools may be deemed more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 17}, page_content='dinate agents or data sources to prioritize. Certain databases, APIs, or retrieval tools may be deemed more\\nreliable or relevant based on the query’s domain.\\n3. Delegation to Subordinate Agents: The top-tier agent assigns tasks to lower-level agents specialized in\\nparticular retrieval methods (e.g., SQL databases, web search, or proprietary systems). These agents execute\\ntheir assigned tasks independently.\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 18}, page_content='4. Aggregation and Synthesis: The results from subordinate agents are collected and integrated by the higher-\\nlevel agent, which synthesizes the information into a coherent response.\\n5. Response Delivery: The final, synthesized answer is returned to the user, ensuring that the response is both\\ncomprehensive and contextually relevant.\\nKey Features and Advantages.\\n• Strategic Prioritization: Top-tier agents can prioritize data sources or tasks based on query complexity,\\nreliability, or context.\\n• Scalability: Distributing tasks across multiple agent tiers enables handling of highly complex or multi-faceted\\nqueries.\\n• Enhanced Decision-Making: Higher-level agents apply strategic oversight to improve overall accuracy and\\ncoherence of responses.\\nChallenges\\n• Coordination Complexity: Maintaining robust inter-agent communication across multiple levels can increase\\norchestration overhead.\\n• Resource Allocation: Efficiently distributing tasks among tiers to avoid bottlenecks is non-trivial.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 18}, page_content='orchestration overhead.\\n• Resource Allocation: Efficiently distributing tasks among tiers to avoid bottlenecks is non-trivial.\\nUse Case: Financial Analysis System\\nPrompt: What are the best investment options given the current market trends in renewable energy?\\nSystem Process (Hierarchical Agentic Workflow):\\n1. Top-Tier Agent: Assesses the query’s complexity and prioritizes reliable financial databases and\\neconomic indicators over less validated data sources.\\n2. Mid-Level Agent: Retrieves real-time market data (e.g., stock prices, sector performance) from\\nproprietary APIs and structured SQL databases.\\n3. Lower-Level Agent(s): Conducts web searches for recent policy announcements and consults recom-\\nmendation systems that track expert opinions and news analytics.\\n4. Aggregation and Synthesis: The top-tier agent compiles the results, integrating quantitative data with\\npolicy insights.\\nResponse:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 18}, page_content='mendation systems that track expert opinions and news analytics.\\n4. Aggregation and Synthesis: The top-tier agent compiles the results, integrating quantitative data with\\npolicy insights.\\nResponse:\\nIntegrated Response: “Based on current market data, renewable energy stocks have shown a 15% growth over\\nthe past quarter, driven by supportive government policies and heightened investor interest. Analysts suggest\\nthat wind and solar sectors, in particular, may experience continued momentum, while emerging technologies\\nlike green hydrogen present moderate risk but potentially high returns.”\\n5.4\\nAgentic Corrective RAG\\nCorrective RAG : introduces mechanisms to self-correct retrieval results, enhancing document utilization and improving\\nresponse generation quality as demonstrated in Figure 19. By embedding intelligent agents into the workflow, Corrective\\nRAG [31] [32] ensures iterative refinement of context documents and responses, minimizing errors and maximizing\\nrelevance.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 18}, page_content='RAG [31] [32] ensures iterative refinement of context documents and responses, minimizing errors and maximizing\\nrelevance.\\nKey Idea of Corrective RAG:\\nThe core principle of Corrective RAG lies in its ability to evaluate retrieved documents\\ndynamically, perform corrective actions, and refine queries to enhance the quality of generated responses. Corrective\\nRAG adjusts its approach as follows:\\n• Document Relevance Evaluation: Retrieved documents are assessed for relevance by the Relevance Evalua-\\ntion Agent. Documents below the relevance threshold trigger corrective steps.\\n• Query Refinement and Augmentation: Queries are refined by the Query Refinement Agent, which leverages\\nsemantic understanding to optimize retrieval for better results.\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 19}, page_content='Figure 19: Overview of Agentic Corrective RAG\\n• Dynamic Retrieval from External Sources: When context is insufficient, the External Knowledge Retrieval\\nAgent performs web searches or accesses alternative data sources to supplement the retrieved documents.\\n• Response Synthesis: All validated and refined information is passed to the Response Synthesis Agent for final\\nresponse generation.\\nWorkflow:\\nThe Corrective RAG system is built on five key agents:\\n1. Context Retrieval Agent: Responsible for retrieving initial context documents from a vector database.\\n2. Relevance Evaluation Agent: Assesses the retrieved documents for relevance and flags any irrelevant or\\nambiguous documents for corrective actions.\\n3. Query Refinement Agent: Rewrites queries to improve retrieval, leveraging semantic understanding to\\noptimize results.\\n4. External Knowledge Retrieval Agent: Performs web searches or accesses alternative data sources when the\\ncontext documents are insufficient.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 19}, page_content='optimize results.\\n4. External Knowledge Retrieval Agent: Performs web searches or accesses alternative data sources when the\\ncontext documents are insufficient.\\n5. Response Synthesis Agent: Synthesizes all validated information into a coherent and accurate response.\\nKey Features and Advantages:\\n• Iterative Correction: Ensures high response accuracy by dynamically identifying and correcting irrelevant or\\nambiguous retrieval results.\\n• Dynamic Adaptability: Incorporates real-time web searches and query refinement for enhanced retrieval\\nprecision.\\n• Agentic Modularity: Each agent performs specialized tasks, ensuring efficient and scalable operation.\\n• Factuality Assurance: By validating all retrieved and generated content, Corrective RAG minimizes the risk\\nof hallucination or misinformation.\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 20}, page_content='Use Case: Academic Research Assistant\\nPrompt: What are the latest findings in generative AI research?\\nSystem Process (Corrective RAG Workflow):\\n1. Query Submission: A user submits the query to the system.\\n2. Context Retrieval:\\n• The Context Retrieval Agent retrieves initial documents from a database of published papers on\\ngenerative AI.\\n• The retrieved documents are passed to the next step for evaluation.\\n3. Relevance Evaluation:\\n• The Relevance Evaluation Agent assesses the documents for alignment with the query.\\n• Documents are classified into relevant, ambiguous, or irrelevant categories. Irrelevant documents\\nare flagged for corrective actions.\\n4. Corrective Actions (if needed):\\n• The Query Refinement Agent rewrites the query to improve specificity and relevance.\\n• The External Knowledge Retrieval Agent performs web searches to fetch additional papers and\\nreports from external sources.\\n5. Response Synthesis:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 20}, page_content='• The External Knowledge Retrieval Agent performs web searches to fetch additional papers and\\nreports from external sources.\\n5. Response Synthesis:\\n• The Response Synthesis Agent integrates validated documents into a coherent and comprehensive\\nsummary.\\nResponse:\\nIntegrated Response: “Recent findings in generative AI highlight advancements in diffusion models, reinforce-\\nment learning for text-to-video tasks, and optimization techniques for large-scale model training. For more\\ndetails, refer to studies published in NeurIPS 2024 and AAAI 2025.”\\n5.5\\nAdaptive Agentic RAG\\nAdaptive Retrieval-Augmented Generation (Adaptive RAG) [33] enhances the flexibility and efficiency of large\\nlanguage models (LLMs) by dynamically adjusting query handling strategies based on the complexity of the incoming\\nquery. Unlike static retrieval workflows, Adaptive RAG [34] employs a classifier to assess query complexity and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 20}, page_content='query. Unlike static retrieval workflows, Adaptive RAG [34] employs a classifier to assess query complexity and\\ndetermine the most appropriate approach, ranging from single-step retrieval to multi-step reasoning, or even bypassing\\nretrieval altogether for straightforward queries as illustrated in Figure 20.\\nFigure 20: An Overview of Adaptive Agentic RAG\\nKey Idea of Adaptive RAG\\nThe core principle of Adaptive RAG lies in its ability to dynamically tailor retrieval\\nstrategies based on the complexity of the query. Adaptive RAG adjusts its approach as follows:\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 21}, page_content='• Straightforward Queries: For fact-based questions that require no additional retrieval (e.g., \"What is the\\nboiling point of water?\"), the system directly generates an answer using pre-existing knowledge.\\n• Simple Queries: For moderately complex tasks requiring minimal context (e.g., \"What is the status of my\\nlatest electricity bill?\"), the system performs a single-step retrieval to fetch the relevant details.\\n• Complex Queries: For multi-layered queries requiring iterative reasoning (e.g., \"How has the population of\\nCity X changed over the past decade, and what are the contributing factors?\"), the system employs multi-step\\nretrieval, progressively refining intermediate results to provide a comprehensive answer.\\nWorkflow:\\nThe Adaptive RAG system is built on three primary components:\\n1. Classifier Role:\\n• A smaller language model analyzes the query to predict its complexity.\\n• The classifier is trained using automatically labeled datasets, derived from past model outcomes and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 21}, page_content='1. Classifier Role:\\n• A smaller language model analyzes the query to predict its complexity.\\n• The classifier is trained using automatically labeled datasets, derived from past model outcomes and\\nquery patterns.\\n2. Dynamic Strategy Selection:\\n• For straightforward queries, the system avoids unnecessary retrieval, directly leveraging the LLM for\\nresponse generation.\\n• For simple queries, it employs a single-step retrieval process to fetch relevant context.\\n• For complex queries, it activates multi-step retrieval to ensure iterative refinement and enhanced reasoning.\\n3. LLM Integration:\\n• The LLM synthesizes retrieved information into a coherent response.\\n• Iterative interactions between the LLM and the classifier enable refinement for complex queries.\\nKey Features and Advantages\\n• Dynamic Adaptability: Adjusts retrieval strategies based on query complexity, optimizing both computational\\nefficiency and response accuracy.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 21}, page_content='Key Features and Advantages\\n• Dynamic Adaptability: Adjusts retrieval strategies based on query complexity, optimizing both computational\\nefficiency and response accuracy.\\n• Resource Efficiency: Minimizes unnecessary overhead for simple queries while ensuring thorough processing\\nfor complex ones.\\n• Enhanced Accuracy: Iterative refinement ensures that complex queries are resolved with high precision.\\n• Flexibility: Can be extended to incorporate additional pathways, such as domain-specific tools or external\\nAPIs.\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 22}, page_content='Use Case: Customer Support Assistant\\nPrompt: Why is my package delayed, and what alternatives do I have?\\nSystem Process (Adaptive RAG Workflow):\\n1. Query Classification:\\n• The classifier analyzes the query and determines it to be complex, requiring multi-step reasoning.\\n2. Dynamic Strategy Selection:\\n• The system activates a multi-step retrieval process based on the complexity classification.\\n3. Multi-Step Retrieval:\\n• Retrieves tracking details from the order database.\\n• Fetches real-time status updates from the shipping provider API.\\n• Conducts a web search for external factors such as weather conditions or local disruptions.\\n4. Response Synthesis:\\n• The LLM integrates all retrieved information, synthesizing a comprehensive and actionable\\nresponse.\\nResponse:\\nIntegrated Response: “Your package is delayed due to severe weather conditions in your region. It is currently\\nat the local distribution center and will be delivered in 2 days. Alternatively, you may opt for a local pickup'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 22}, page_content='at the local distribution center and will be delivered in 2 days. Alternatively, you may opt for a local pickup\\nfrom the facility.”\\n5.6\\nGraph-Based Agentic RAG\\n5.6.1\\nAgent-G: Agentic Framework for Graph RAG\\nAgent-G [8]: introduces a novel agentic architecture that integrates graph knowledge bases with unstructured document\\nretrieval. By combining structured and unstructured data sources, this framework enhances retrieval-augmented\\ngeneration (RAG) systems with improved reasoning and retrieval accuracy. It employs modular retriever banks,\\ndynamic agent interaction, and feedback loops to ensure high-quality outputs as shown in Figure 21.\\nFigure 21: An Overview of Agent-G: Agentic Framework for Graph RAG [8]\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 23}, page_content='Key Idea of Agent-G\\nThe core principle of Agent-G lies in its ability to dynamically assign retrieval tasks to\\nspecialized agents, leveraging both graph knowledge bases and textual documents. Agent-G adjusts its retrieval strategy\\nas follows:\\n• Graph Knowledge Bases: Structured data is used to extract relationships, hierarchies, and connections (e.g.,\\ndisease-to-symptom mappings in healthcare).\\n• Unstructured Documents: Traditional text retrieval systems provide contextual information to complement\\ngraph data.\\n• Critic Module: Evaluates the relevance and quality of retrieved information, ensuring alignment with the\\nquery.\\n• Feedback Loops: Refines retrieval and synthesis through iterative validation and re-querying.\\nWorkflow:\\nThe Agent-G system is built on four primary components:\\n1. Retriever Bank:\\n• A modular set of agents specializes in retrieving graph-based or unstructured data.\\n• Agents dynamically select relevant sources based on the query’s requirements.\\n2. Critic Module:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 23}, page_content='• A modular set of agents specializes in retrieving graph-based or unstructured data.\\n• Agents dynamically select relevant sources based on the query’s requirements.\\n2. Critic Module:\\n• Validates retrieved data for relevance and quality.\\n• Flags low-confidence results for re-retrieval or refinement.\\n3. Dynamic Agent Interaction:\\n• Task-specific agents collaborate to integrate diverse data types.\\n• Ensures cohesive retrieval and synthesis across graph and text sources.\\n4. LLM Integration:\\n• Synthesizes validated data into a coherent response.\\n• Iterative feedback from the critic ensures alignment with the query’s intent.\\nKey Features and Advantages\\n• Enhanced Reasoning: Combines structured relationships from graphs with contextual information from\\nunstructured documents.\\n• Dynamic Adaptability: Adjusts retrieval strategies dynamically based on query requirements.\\n• Improved Accuracy: Critic module reduces the risk of irrelevant or low-quality data in responses.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 23}, page_content='• Dynamic Adaptability: Adjusts retrieval strategies dynamically based on query requirements.\\n• Improved Accuracy: Critic module reduces the risk of irrelevant or low-quality data in responses.\\n• Scalable Modularity: Supports the addition of new agents for specialized tasks, enhancing scalability.\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 24}, page_content='Use Case: Healthcare Diagnostics\\nPrompt: What are the common symptoms of Type 2 Diabetes, and how are they related to heart disease?\\nSystem Process (Agent-G Workflow):\\n1. Query Reception and Assignment: The system receives the query and identifies the need for both\\ngraph-structured and unstructured data to answer the question comprehensively.\\n2. Graph Retriever:\\n• Extracts relationships between Type 2 Diabetes and heart disease from a medical knowledge\\ngraph.\\n• Identifies shared risk factors such as obesity and high blood pressure by exploring graph hierar-\\nchies and relationships.\\n3. Document Retriever:\\n• Retrieves descriptions of Type 2 Diabetes symptoms (e.g., increased thirst, frequent urination,\\nfatigue) from medical literature.\\n• Adds contextual information to complement the graph-based insights.\\n4. Critic Module:\\n• Evaluates the relevance and quality of the retrieved graph data and document data.\\n• Flags low-confidence results for refinement or re-querying.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 24}, page_content='4. Critic Module:\\n• Evaluates the relevance and quality of the retrieved graph data and document data.\\n• Flags low-confidence results for refinement or re-querying.\\n5. Response Synthesis: The LLM integrates validated data from the Graph Retriever and Document\\nRetriever into a coherent response, ensuring alignment with the query’s intent.\\nResponse:\\nIntegrated Response: “Type 2 Diabetes symptoms include increased thirst, frequent urination, and fatigue.\\nStudies show a 50% correlation between diabetes and heart disease, primarily through shared risk factors such\\nas obesity and high blood pressure.”\\n5.6.2\\nGeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation\\nGeAR [35]: introduces an agentic framework that enhances traditional Retrieval-Augmented Generation (RAG) systems\\nby incorporating graph-based retrieval mechanisms. By leveraging graph expansion techniques and an agent-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 24}, page_content='by incorporating graph-based retrieval mechanisms. By leveraging graph expansion techniques and an agent-based\\narchitecture, GeAR addresses challenges in multi-hop retrieval scenarios, improving the system’s ability to handle\\ncomplex queries as shown in Figure 22.\\nKey Idea of GeAR\\nGeAR advances RAG performance through two primary innovations:\\n• Graph Expansion: Enhances conventional base retrievers (e.g., BM25) by expanding the retrieval process to\\ninclude graph-structured data, enabling the system to capture complex relationships and dependencies between\\nentities.\\n• Agent Framework: Incorporates an agent-based architecture that utilizes graph expansion to manage retrieval\\ntasks more effectively, allowing for dynamic and autonomous decision-making in the retrieval process.\\nWorkflow:\\nThe GeAR system operates through the following components:\\n1. Graph Expansion Module:\\n• Integrates graph-based data into the retrieval process, allowing the system to consider relationships'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 24}, page_content='Workflow:\\nThe GeAR system operates through the following components:\\n1. Graph Expansion Module:\\n• Integrates graph-based data into the retrieval process, allowing the system to consider relationships\\nbetween entities during retrieval.\\n• Enhances the base retriever’s ability to handle multi-hop queries by expanding the search space to include\\nconnected entities.\\n2. Agent-Based Retrieval:\\n• Employs an agent framework to manage the retrieval process, enabling dynamic selection and combination\\nof retrieval strategies based on the query’s complexity.\\n• Agents can autonomously decide to utilize graph-expanded retrieval paths to improve the relevance and\\naccuracy of retrieved information.\\n25'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 25}, page_content='3. LLM Integration:\\n• Combines the retrieved information, enriched by graph expansion, with the capabilities of a Large\\nLanguage Model (LLM) to generate coherent and contextually relevant responses.\\n• The integration ensures that the generative process is informed by both unstructured documents and\\nstructured graph data.\\nFigure 22: An Overview of GeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation[35]\\nKey Features and Advantages\\n• Enhanced Multi-Hop Retrieval: GeAR’s graph expansion allows the system to handle complex queries that\\nrequire reasoning over multiple interconnected pieces of information.\\n• Agentic Decision-Making: The agent framework enables dynamic and autonomous selection of retrieval\\nstrategies, improving efficiency and relevance.\\n• Improved Accuracy: By incorporating structured graph data, GeAR enhances the precision of retrieved\\ninformation, leading to more accurate and contextually appropriate responses.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 25}, page_content='• Improved Accuracy: By incorporating structured graph data, GeAR enhances the precision of retrieved\\ninformation, leading to more accurate and contextually appropriate responses.\\n• Scalability: The modular nature of the agent framework allows for the integration of additional retrieval\\nstrategies and data sources as needed.\\n26'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 26}, page_content='Use Case: Multi-Hop Question Answering\\nPrompt: Which author influenced the mentor of J.K. Rowling?\\nSystem Process (GeAR Workflow):\\n1. Top-Tier Agent: Evaluates the query’s multi-hop nature and determines that a combination of graph\\nexpansion and document retrieval is necessary to answer the question.\\n2. Graph Expansion Module:\\n• Identifies that J.K. Rowling’s mentor is a key entity in the query.\\n• Traces the literary influences on that mentor by exploring graph-structured data on literary\\nrelationships.\\n3. Agent-Based Retrieval:\\n• An agent autonomously selects the graph-expanded retrieval path to gather relevant information\\nabout the mentor’s influences.\\n• Integrates additional context by querying textual data sources for unstructured details about the\\nmentor and their influences.\\n4. Response Synthesis: Combines insights from the graph and document retrieval processes using the\\nLLM to generate a response that accurately reflects the complex relationships in the query.\\nResponse:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 26}, page_content='LLM to generate a response that accurately reflects the complex relationships in the query.\\nResponse:\\nIntegrated Response: “J.K. Rowling’s mentor, [Mentor Name], was heavily influenced by [Author Name],\\nknown for their [notable works or genre]. This connection highlights the layered relationships in literary history,\\nwhere influential ideas often pass through multiple generations of authors.”\\n5.7\\nAgentic Document Workflows in Agentic RAG\\nAgentic Document Workflows (ADW) [36] extend traditional Retrieval-Augmented Generation (RAG) paradigms by\\nenabling end-to-end knowledge work automation. These workflows orchestrate complex document-centric processes,\\nintegrating document parsing, retrieval, reasoning, and structured outputs with intelligent agents (see Figure 23). ADW\\nsystems address limitations of Intelligent Document Processing (IDP) and RAG by maintaining state, coordinating\\nmulti-step workflows, and applying domain-specific logic to documents.\\nWorkflow'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 26}, page_content='systems address limitations of Intelligent Document Processing (IDP) and RAG by maintaining state, coordinating\\nmulti-step workflows, and applying domain-specific logic to documents.\\nWorkflow\\n1. Document Parsing and Information Structuring:\\n• Documents are parsed using enterprise-grade tools (e.g., LlamaParse) to extract relevant data fields such\\nas invoice numbers, dates, vendor information, line items, and payment terms.\\n• Structured data is organized for downstream processing.\\n2. State Maintenance Across Processes:\\n• The system maintains state about document context, ensuring consistency and relevance across multi-step\\nworkflows.\\n• Tracks the progression of the document through various processing stages.\\n3. Knowledge Retrieval:\\n• Relevant references are retrieved from external knowledge bases (e.g., LlamaCloud) or vector indexes.\\n• Retrieves real-time, domain-specific guidelines for enhanced decision-making.\\n4. Agentic Orchestration:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 26}, page_content='• Retrieves real-time, domain-specific guidelines for enhanced decision-making.\\n4. Agentic Orchestration:\\n• Intelligent agents apply business rules, perform multi-hop reasoning, and generate actionable recommen-\\ndations.\\n• Orchestrates components such as parsers, retrievers, and external APIs for seamless integration.\\n5. Actionable Output Generation:\\n• Outputs are presented in structured formats, tailored to specific use cases.\\n• Recommendations and extracted insights are synthesized into concise and actionable reports.\\n27'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 27}, page_content='Figure 23: An Overview of Agentic Document Workflows (ADW)\\n[36]\\nUse Case: Invoice Payments Workflow\\nPrompt: Generate a payment recommendation report based on the submitted invoice and associated vendor\\ncontract terms.\\nSystem Process (ADW Workflow):\\n1. Parse the invoice to extract key details such as invoice number, date, vendor information, line items,\\nand payment terms.\\n2. Retrieve the corresponding vendor contract to verify payment terms and identify any applicable\\ndiscounts or compliance requirements.\\n3. Generate a payment recommendation report that includes original amount due, potential early payment\\ndiscounts, budget impact analysis, and strategic payment actions.\\nResponse: Integrated Response: \"Invoice INV-2025-045 for $15,000.00 has been processed. An early payment\\ndiscount of 2% is available if paid by 2025-04-10, reducing the amount due to $14,700.00. A bulk order discount'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 27}, page_content='discount of 2% is available if paid by 2025-04-10, reducing the amount due to $14,700.00. A bulk order discount\\nof 5% was applied as the subtotal exceeded $10,000.00. It is recommended to approve early payment to save\\n2% and ensure timely fund allocation for upcoming project phases.\"\\nKey Features and Advantages\\n• State Maintenance: Tracks document context and workflow stage, ensuring consistency across processes.\\n• Multi-Step Orchestration: Handles complex workflows involving multiple components and external tools.\\n• Domain-Specific Intelligence: Applies tailored business rules and guidelines for precise recommendations.\\n• Scalability: Supports large-scale document processing with modular and dynamic agent integration.\\n• Enhanced Productivity: Automates repetitive tasks while augmenting human expertise in decision-making.\\n28'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 28}, page_content='6\\nComparative Analysis of Agentic RAG Frameworks\\nTable 2 provides a comprehensive comparative analysis of the three architectural frameworks: Traditional RAG, Agentic\\nRAG, and Agentic Document Workflows (ADW). This analysis highlights their respective strengths, weaknesses, and\\nbest-fit scenarios, offering valuable insights into their applicability across diverse use cases.\\nTable 2: Comparative Analysis: Traditional RAG vs Agentic RAG vs Agentic Document Workflows (ADW)\\nFeature\\nTraditional RAG\\nAgentic RAG\\nAgentic Document\\nWorkflows (ADW)\\nFocus\\nIsolated retrieval and\\ngeneration tasks\\nMulti-agent\\ncollaboration and\\nreasoning\\nDocument-centric\\nend-to-end workflows\\nContext Maintenance\\nLimited\\nEnabled through\\nmemory modules\\nMaintains state across\\nmulti-step workflows\\nDynamic Adaptability\\nMinimal\\nHigh\\nTailored to document\\nworkflows\\nWorkflow\\nOrchestration\\nAbsent\\nOrchestrates multi-agent\\ntasks\\nIntegrates multi-step\\ndocument processing\\nUse of External\\nTools/APIs\\nBasic integration (e.g.,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 28}, page_content='Minimal\\nHigh\\nTailored to document\\nworkflows\\nWorkflow\\nOrchestration\\nAbsent\\nOrchestrates multi-agent\\ntasks\\nIntegrates multi-step\\ndocument processing\\nUse of External\\nTools/APIs\\nBasic integration (e.g.,\\nretrieval tools)\\nExtends via tools like\\nAPIs and knowledge\\nbases\\nDeeply integrates business\\nrules and domain-specific\\ntools\\nScalability\\nLimited to small\\ndatasets or queries\\nScalable for multi-agent\\nsystems\\nScales for multi-domain\\nenterprise workflows\\nComplex Reasoning\\nBasic (e.g., simple\\nQ&A)\\nMulti-step reasoning\\nwith agents\\nStructured reasoning across\\ndocuments\\nPrimary Applications\\nQA systems, knowledge\\nretrieval\\nMulti-domain\\nknowledge and\\nreasoning\\nContract review, invoice\\nprocessing, claims analysis\\nStrengths\\nSimplicity, quick setup\\nHigh accuracy,\\ncollaborative reasoning\\nEnd-to-end automation,\\ndomain-specific intelligence\\nChallenges\\nPoor contextual\\nunderstanding\\nCoordination\\ncomplexity\\nResource overhead, domain\\nstandardization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 28}, page_content='High accuracy,\\ncollaborative reasoning\\nEnd-to-end automation,\\ndomain-specific intelligence\\nChallenges\\nPoor contextual\\nunderstanding\\nCoordination\\ncomplexity\\nResource overhead, domain\\nstandardization\\nThe comparative analysis underscores the evolutionary trajectory from Traditional RAG to Agentic RAG and further to\\nAgentic Document Workflows (ADW). While Traditional RAG offers simplicity and ease of deployment for basic tasks,\\nAgentic RAG introduces enhanced reasoning and scalability through multi-agent collaboration. ADW builds upon these\\nadvancements by providing robust, document-centric workflows that facilitate end-to-end automation and integration\\nwith domain-specific processes. Understanding the strengths and limitations of each framework is crucial for selecting\\nthe most appropriate architecture to meet specific application requirements and operational demands.\\n7\\nApplications of Agentic RAG'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 28}, page_content='the most appropriate architecture to meet specific application requirements and operational demands.\\n7\\nApplications of Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems have demonstrated transformative potential across a variety\\nof domains. By combining real-time data retrieval, generative capabilities, and autonomous decision-making, these\\nsystems address complex, dynamic, and multi-modal challenges. This section explores the key applications of Agentic\\nRAG, providing detailed insights into how these systems are shaping industries such as customer support, healthcare,\\nfinance, education, legal workflows, and creative industries.\\n7.1\\nCustomer Support and Virtual Assistants\\nAgentic RAG systems are revolutionizing customer support by enabling real-time, context-aware query resolution.\\nTraditional chatbots and virtual assistants often rely on static knowledge bases, leading to generic or outdated responses.\\n29'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 29}, page_content='By contrast, Agentic RAG systems dynamically retrieve the most relevant information, adapt to the user’s context, and\\ngenerate personalized responses.\\nUse Case: Twitch Ad Sales Enhancement [37]\\nFor instance, Twitch leveraged an agentic workflow with RAG on Amazon Bedrock to streamline ad sales. The system\\ndynamically retrieved advertiser data, historical campaign performance, and audience demographics to generate detailed\\nad proposals, significantly boosting operational efficiency.\\nKey Benefits:\\n• Improved Response Quality: Personalized and context-aware replies enhance user engagement.\\n• Operational Efficiency: Reduces the workload on human support agents by automating complex queries.\\n• Real-Time Adaptability: Dynamically integrates evolving data, such as live service outages or pricing\\nupdates.\\n7.2\\nHealthcare and Personalized Medicine\\nIn healthcare, the integration of patient-specific data with the latest medical research is critical for informed decision-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 29}, page_content='updates.\\n7.2\\nHealthcare and Personalized Medicine\\nIn healthcare, the integration of patient-specific data with the latest medical research is critical for informed decision-\\nmaking. Agentic RAG systems enable this by retrieving real-time clinical guidelines, medical literature, and patient\\nhistory to assist clinicians in diagnostics and treatment planning.\\nUse Case: Patient Case Summary [38]\\nAgentic RAG systems have been applied in generating patient case summaries. For example, by integrating electronic\\nhealth records (EHR) and up-to-date medical literature, the system generates comprehensive summaries for clinicians\\nto make faster and more informed decisions.\\nKey Benefits:\\n• Personalized Care: Tailors recommendations to individual patient needs.\\n• Time Efficiency: Streamlines the retrieval of relevant research, saving valuable time for healthcare providers.\\n• Accuracy: Ensures recommendations are based on the latest evidence and patient-specific parameters.\\n7.3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 29}, page_content='• Accuracy: Ensures recommendations are based on the latest evidence and patient-specific parameters.\\n7.3\\nLegal and Contract Analysis\\nAgentic RAG systems are redefining how legal workflows are conducted, offering tools for rapid document analysis and\\ndecision-making.\\nUse Case: Contract Review [39]\\nA legal agentic RAG system can analyze contracts, extract critical clauses, and identify potential risks. By combining\\nsemantic search capabilities with legal knowledge graphs, it automates the tedious process of contract review, ensuring\\ncompliance and mitigating risks.\\nKey Benefits:\\n• Risk Identification: Automatically flags clauses that deviate from standard terms.\\n• Efficiency: Reduces the time spent on contract review processes.\\n• Scalability: Handles large volumes of contracts simultaneously.\\n7.4\\nFinance and Risk Analysis\\nAgentic RAG systems are transforming the finance industry by providing real-time insights for investment decisions,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 29}, page_content='7.4\\nFinance and Risk Analysis\\nAgentic RAG systems are transforming the finance industry by providing real-time insights for investment decisions,\\nmarket analysis, and risk management. These systems integrate live data streams, historical trends, and predictive\\nmodeling to generate actionable outputs.\\nUse Case: Auto Insurance Claims Processing [40]\\nIn auto insurance, Agentic RAG can automate claim processing. For example, by retrieving policy details and combining\\nthem with accident data, it generates claim recommendations while ensuring compliance with regulatory requirements.\\nKey Benefits:\\n• Real-Time Analytics: Delivers insights based on live market data.\\n30'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 30}, page_content='• Risk Mitigation: Identifies potential risks using predictive analysis and multi-step reasoning.\\n• Enhanced Decision-Making: Combines historical and live data for comprehensive strategies.\\n7.5\\nEducation and Personalized Learning\\nEducation is another domain where Agentic RAG systems are making significant strides. These systems enable adaptive\\nlearning by generating explanations, study materials, and feedback tailored to the learner’s progress and preferences.\\nUse Case: Research Paper Generation [41]\\nIn higher education, Agentic RAG has been used to assist researchers by synthesizing key findings from multiple\\nsources. For instance, a researcher querying, “What are the latest advancements in quantum computing?” receives a\\nconcise summary enriched with references, enhancing the quality and efficiency of their work.\\nKey Benefits:\\n• Tailored Learning Paths: Adapts content to individual student needs and performance levels.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 30}, page_content='Key Benefits:\\n• Tailored Learning Paths: Adapts content to individual student needs and performance levels.\\n• Engaging Interactions: Provides interactive explanations and personalized feedback.\\n• Scalability: Supports large-scale deployments for diverse educational environments.\\n7.6\\nGraph-Enhanced Applications in Multimodal Workflows\\nGraph-Enhanced Agentic RAG (GEAR) combines graph structures with retrieval mechanisms, making it particularly\\neffective in multimodal workflows where interconnected data sources are essential.\\nUse Case: Market Survey Generation\\nGEAR enables the synthesis of text, images, and videos for marketing campaigns. For example, querying, “What\\nare the emerging trends in eco-friendly products?” generates a detailed report enriched with customer preferences,\\ncompetitor analysis, and multimedia content.\\nKey Benefits:\\n• Multi-Modal Capabilities: Integrates text, image, and video data for comprehensive outputs.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 30}, page_content='competitor analysis, and multimedia content.\\nKey Benefits:\\n• Multi-Modal Capabilities: Integrates text, image, and video data for comprehensive outputs.\\n• Enhanced Creativity: Generates innovative ideas and solutions for marketing and entertainment.\\n• Dynamic Adaptability: Adapts to evolving market trends and customer needs.\\nThe applications of Agentic RAG systems span a wide range of industries, showcasing their versatility and transformative\\npotential. From personalized customer support to adaptive education and graph-enhanced multimodal workflows, these\\nsystems address complex, dynamic, and knowledge-intensive challenges. By integrating retrieval, generation, and\\nagentic intelligence, Agentic RAG systems are paving the way for next-generation AI applications.\\n8\\nTools and Frameworks for Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant evolution in combining retrieval,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 30}, page_content='8\\nTools and Frameworks for Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant evolution in combining retrieval,\\ngeneration, and agentic intelligence. These systems extend the capabilities of traditional RAG by integrating decision-\\nmaking, query reformulation, and adaptive workflows. The following tools and frameworks provide robust support for\\ndeveloping Agentic RAG systems, addressing the complex requirements of real-world applications.\\nKey Tools and Frameworks:\\n• LangChain and LangGraph: LangChain [42] provides modular components for building RAG pipelines,\\nseamlessly integrating retrievers, generators, and external tools. LangGraph complements this by introducing\\ngraph-based workflows that support loops, state persistence, and human-in-the-loop interactions, enabling\\nsophisticated orchestration and self-correction mechanisms in agentic systems.\\n• LlamaIndex: LlamaIndex’s [43] Agentic Document Workflows (ADW) enable end-to-end automation of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 30}, page_content='sophisticated orchestration and self-correction mechanisms in agentic systems.\\n• LlamaIndex: LlamaIndex’s [43] Agentic Document Workflows (ADW) enable end-to-end automation of\\ndocument processing, retrieval, and structured reasoning. It introduces a meta-agent architecture where\\nsub-agents manage smaller document sets, coordinating through a top-level agent for tasks such as compliance\\nanalysis and contextual understanding.\\n• Hugging Face Transformers and Qdrant: Hugging Face [44] offers pre-trained models for embedding and\\ngeneration tasks, while Qdrant [45] enhances retrieval workflows with adaptive vector search capabilities,\\nallowing agents to optimize performance by dynamically switching between sparse and dense vector methods.\\n31'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 31}, page_content='• CrewAI and AutoGen: These frameworks emphasize multi-agent architectures. CrewAI [46] supports\\nhierarchical and sequential processes, robust memory systems, and tool integrations. AG2 [47] (formerly\\nknows as AutoGen [48, 49]) excels in multi-agent collaboration with advanced support for code generation,\\ntool execution, and decision-making.\\n• OpenAI Swarm Framework: An educational framework designed for ergonomic, lightweight multi-agent\\norchestration [50], emphasizing agent autonomy and structured collaboration.\\n• Agentic RAG with Vertex AI: Developed by Google, Vertex AI [51] integrates seamlessly with Agentic\\nRetrieval-Augmented Generation (RAG), providing a platform to build, deploy, and scale machine learning\\nmodels while leveraging advanced AI capabilities for robust, contextually aware retrieval and decision-making\\nworkflows.\\n• Semantic Kernel: Semantic Kernel [52, 53] is an open-source SDK by Microsoft that integrates large language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 31}, page_content='workflows.\\n• Semantic Kernel: Semantic Kernel [52, 53] is an open-source SDK by Microsoft that integrates large language\\nmodels (LLMs) into applications. It supports agentic patterns, enabling the creation of autonomous AI agents\\nfor natural language understanding, task automation, and decision-making. It has been used in scenarios like\\nServiceNow’s P1 incident management to facilitate real-time collaboration, automate task execution, and\\nretrieve contextual information seamlessly\\n• Amazon Bedrock for Agentic RAG: Amazon Bedrock [37] provides a robust platform for implementing\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n• IBM Watson and Agentic RAG: IBM’s watsonx.ai [54] supports building Agentic RAG systems, exemplified\\nby using the Granite-3-8B-Instruct model to answer complex queries by integrating external information and\\nenhancing response accuracy.\\n• Neo4j and Vector Databases: Neo4j, a prominent open-source graph database, excels in handling complex'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 31}, page_content='enhancing response accuracy.\\n• Neo4j and Vector Databases: Neo4j, a prominent open-source graph database, excels in handling complex\\nrelationships and semantic queries. Alongside Neo4j, vector databases like Weaviate, Pinecone, Milvus, and\\nQdrant provide efficient similarity search and retrieval capabilities, forming the backbone of high-performance\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n9\\nBenchmarks and Datasets\\nCurrent benchmarks and datasets provide valuable insights into evaluating Retrieval-Augmented Generation (RAG)\\nsystems, including those with agentic and graph-based enhancements. While some are explicitly designed for RAG,\\nothers are adapted to test retrieval, reasoning, and generation capabilities in diverse scenarios. Datasets are crucial for\\ntesting the retrieval, reasoning, and generation components of RAG systems. Table 3 discusses some key datasets based\\non the dowstream task for RAG Evaluation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 31}, page_content='testing the retrieval, reasoning, and generation components of RAG systems. Table 3 discusses some key datasets based\\non the dowstream task for RAG Evaluation.\\nBenchmarks play a critical role in standardizing the evaluation of RAG systems by providing structured tasks and\\nmetrics. The following benchmarks are particularly relevant:\\n• BEIR (Benchmarking Information Retrieval): A versatile benchmark designed for evaluating embedding\\nmodels on a variety of information retrieval tasks, encompassing 17 datasets across diverse domains like\\nbioinformatics, finance, and question answering [55].\\n• MS MARCO (Microsoft Machine Reading Comprehension): Focused on passage ranking and question\\nanswering, this benchmark is widely used for dense retrieval tasks in RAG systems [56].\\n• TREC (Text REtrieval Conference, Deep Learning Track): Provides datasets for passage and document\\nretrieval, emphasizing the quality of ranking models in retrieval pipelines [57].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 31}, page_content='• TREC (Text REtrieval Conference, Deep Learning Track): Provides datasets for passage and document\\nretrieval, emphasizing the quality of ranking models in retrieval pipelines [57].\\n• MuSiQue (Multihop Sequential Questioning): A benchmark for multihop reasoning across multiple\\ndocuments, emphasizing the importance of retrieving and synthesizing information from disconnected contexts\\n[58].\\n• 2WikiMultihopQA: A dataset designed for multihop QA tasks over two Wikipedia articles, focusing on the\\nability to connect knowledge across multiple sources [59].\\n• AgentG (Agentic RAG for Knowledge Fusion): Tailored for agentic RAG tasks, this benchmark assesses\\ndynamic information synthesis across multiple knowledge bases [8].\\n• HotpotQA: A multi-hop QA benchmark requiring retrieval and reasoning over interconnected contexts, ideal\\nfor evaluating complex RAG workflows[60].\\n• RAGBench: A large-scale, explainable benchmark featuring 100,000 examples across industry domains, with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 31}, page_content='for evaluating complex RAG workflows[60].\\n• RAGBench: A large-scale, explainable benchmark featuring 100,000 examples across industry domains, with\\na TRACe evaluation framework for actionable RAG metrics [61].\\n32'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 32}, page_content='• BERGEN (Benchmarking Retrieval-Augmented Generation): A library for systematically benchmarking\\nRAG systems with standardized experiments [62].\\n• FlashRAG Toolkit: Implements 12 RAG methods and includes 32 benchmark datasets to support efficient\\nand standardized RAG evaluation [63].\\n• GNN-RAG: This benchmark evaluates graph-based RAG systems on tasks like node-level and edge-level\\npredictions, focusing on retrieval quality and reasoning performance in Knowledge Graph Question Answering\\n(KGQA) [64].\\nTable 3: Downstream Tasks and Datasets for RAG Evaluation (Adapted from [20]\\nCategory\\nTask Type\\nDatasets and References\\nQA\\nSingle-hop QA\\nNatural Questions (NQ) [65], TriviaQA [66], SQuAD [67],\\nWeb Questions (WebQ) [68], PopQA [69], MS MARCO\\n[56]\\nMulti-hop QA\\nHotpotQA [60], 2WikiMultiHopQA [59], MuSiQue [58]\\nLong-form QA\\nELI5 [70], NarrativeQA (NQA) [71], ASQA [72], QM-\\nSum [73]\\nDomain-specific QA\\nQasper [74], COVID-QA [75], CMB/MMCU Medical\\n[76]\\nMulti-choice QA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 32}, page_content='Long-form QA\\nELI5 [70], NarrativeQA (NQA) [71], ASQA [72], QM-\\nSum [73]\\nDomain-specific QA\\nQasper [74], COVID-QA [75], CMB/MMCU Medical\\n[76]\\nMulti-choice QA\\nQuALITY [77], ARC (No reference available), Common-\\nsenseQA [78]\\nGraph-based QA\\nGraph QA\\nGraphQA [79]\\nEvent Argument Extraction\\nWikiEvent [80], RAMS [81]\\nDialog\\nOpen-domain Dialog\\nWizard of Wikipedia (WoW) [82]\\nPersonalized Dialog\\nKBP [83], DuleMon [84]\\nTask-oriented Dialog\\nCamRest [85]\\nRecommendation\\nPersonalized Content\\nAmazon Datasets (Toys, Sports, Beauty) [86]\\nReasoning\\nCommonsense Reasoning\\nHellaSwag [87], CommonsenseQA [78]\\nCoT Reasoning\\nCoT Reasoning [88]\\nComplex Reasoning\\nCSQA [89]\\nOthers\\nLanguage Understanding\\nMMLU (No reference available), WikiText-103 [65]\\nFact Checking/Verification\\nFEVER [90], PubHealth [91]\\nStrategy QA\\nStrategyQA [92]\\nSummarization\\nText Summarization\\nWikiASP [93], XSum [94]\\nLong-form Summarization\\nNarrativeQA (NQA) [71], QMSum [73]\\nText Generation\\nBiography\\nBiography Dataset (No reference available)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 32}, page_content='Summarization\\nText Summarization\\nWikiASP [93], XSum [94]\\nLong-form Summarization\\nNarrativeQA (NQA) [71], QMSum [73]\\nText Generation\\nBiography\\nBiography Dataset (No reference available)\\nText Classification\\nSentiment Analysis\\nSST-2 [95]\\nGeneral Classification\\nVioLens[96], TREC [57]\\nCode Search\\nProgramming Search\\nCodeSearchNet [97]\\nRobustness\\nRetrieval Robustness\\nNoMIRACL [98]\\nLanguage Modeling Robustness\\nWikiText-103 [99]\\nMath\\nMath Reasoning\\nGSM8K [100]\\nMachine Translation\\nTranslation Tasks\\nJRC-Acquis [101]\\n10\\nConclusion\\nAgentic Retrieval-Augmented Generation (RAG) represents a transformative advancement in artificial intelligence,\\naddressing the limitations of traditional RAG systems through the integration of autonomous agents. By leveraging\\n33'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 33}, page_content='agentic intelligence, these systems introduce capabilities such as dynamic decision-making, iterative reasoning, and\\ncollaborative workflows, enabling them to tackle complex, real-world tasks with enhanced precision and adaptability.\\nThis survey explored the evolution of RAG systems, from their initial implementations to advanced paradigms like\\nModular RAG, highlighting the contributions and limitations of each. The integration of agents into the RAG pipeline\\nhas emerged as a pivotal development, resulting in Agentic RAG systems that overcome static workflows and limited\\ncontextual adaptability. Applications across healthcare, finance, education, and creative industries demonstrate the\\ntransformative potential of these systems, showcasing their ability to deliver personalized, real-time, and context-aware\\nsolutions.\\nDespite their promise, Agentic RAG systems face challenges that require further research and innovation. Coordination'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 33}, page_content='solutions.\\nDespite their promise, Agentic RAG systems face challenges that require further research and innovation. Coordination\\ncomplexity in multi-agent architectures, scalability, and latency issues, as well as ethical considerations, must be\\naddressed to ensure robust and responsible deployment. Additionally, the lack of specialized benchmarks and datasets\\ntailored to evaluate agentic capabilities poses a significant hurdle. Developing evaluation methodologies that capture\\nthe unique aspects of Agentic RAG, such as multi-agent collaboration and dynamic adaptability, will be crucial for\\nadvancing the field.\\nLooking ahead, the convergence of retrieval-augmented generation and agentic intelligence has the potential to redefine\\nAI’s role in dynamic and complex environments. By addressing these challenges and exploring future directions,\\nresearchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for transformative'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 33}, page_content='researchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for transformative\\napplications across industries and domains. As AI systems continue to evolve, Agentic RAG stands as a cornerstone for\\ncreating adaptive, context-aware, and impactful solutions that meet the demands of a rapidly changing world.\\nReferences\\n[1] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and\\nJianfeng Gao. Large language models: A survey, 2024.\\n[2] Aditi Singh. Exploring language models: A comprehensive survey and analysis. In 2023 International Con-\\nference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication\\nEngineering (RMKMATE), pages 1–4, 2023.\\n[3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\\nJunjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 33}, page_content='Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,\\nYifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language\\nmodels, 2024.\\n[4] Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. A complete survey on llm-based ai\\nchatbots, 2024.\\n[5] Aditi Singh. A survey of ai text-to-image and ai text-to-video generators. In 2023 4th International Conference\\non Artificial Intelligence, Robotics and Control (AIRC), pages 32–36, 2023.\\n[6] Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and Tala Talaei Khoei.\\nExploring prompt engineering: A systematic review with swot analysis, 2024.\\n[7] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 33}, page_content='Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles,\\ntaxonomy, challenges, and open questions. ACM Transactions on Information Systems, November 2024.\\n[8] Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala,\\nand Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented generation, 2024.\\n[9] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao\\nZhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey, 2024.\\n[10] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan,\\nand Graham Neubig. Active retrieval augmented generation, 2023.\\n[11] Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n[12] Anthropic.\\nBuilding\\neffective\\nagents,\\n2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 33}, page_content='[11] Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n[12] Anthropic.\\nBuilding\\neffective\\nagents,\\n2024.\\nhttps://www.anthropic.com/research/\\nbuilding-effective-agents. Accessed: February 2, 2025.\\n[13] LangChain.\\nLanggraph workflows tutorial, 2025.\\nhttps://langchain-ai.github.io/langgraph/\\ntutorials/workflows/. Accessed: February 2, 2025.\\n34'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 34}, page_content='[14] Chidaksh Ravuru, Sagar Srinivas Sakhinana, and Venkataramana Runkana.\\nAgentic retrieval-augmented\\ngeneration for time series analysis, 2024.\\n[15] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey, 2023.\\n[16] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang.\\nGraph retrieval-augmented generation: A survey, 2024.\\n[17] Aditi Singh, Abul Ehtesham, Saifuddin Mahmud, and Jong-Hoon Kim. Revolutionizing mental health care\\nthrough langchain: A journey with a large language model. In 2024 IEEE 14th Annual Computing and\\nCommunication Workshop and Conference (CCWC), pages 0073–0078, 2024.\\n[18] Gaurav Kumar Gupta, Aditi Singh, Sijo Valayakkad Manikandan, and Abul Ehtesham. Digital diagnostics: The\\npotential of large language models in recognizing symptoms of common illnesses. AI, 6(1), 2025.\\n[19] Aditi Singh, Abul Ehtesham, Saket Kumar, Gaurav Kumar Gupta, and Tala Talaei Khoei. Encouraging responsible'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 34}, page_content='[19] Aditi Singh, Abul Ehtesham, Saket Kumar, Gaurav Kumar Gupta, and Tala Talaei Khoei. Encouraging responsible\\nuse of generative ai in education: A reward-based learning approach. In Tim Schlippe, Eric C. K. Cheng, and\\nTianchong Wang, editors, Artificial Intelligence in Education Technologies: New Development and Innovative\\nPractices, pages 404–413, Singapore, 2025. Springer Nature Singapore.\\n[20] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and\\nHaofen Wang. Retrieval-augmented generation for large language models: A survey, 2024.\\n[21] Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen\\ntau Yih. Dense passage retrieval for open-domain question answering, 2020.\\n[22] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong\\nWen. A survey on the memory mechanism of large language model based agents, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 34}, page_content='[22] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong\\nWen. A survey on the memory mechanism of large language model based agents, 2024.\\n[23] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large\\nlanguage models can self-correct with tool-interactive critiquing, 2024.\\n[24] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang,\\nand Enhong Chen. Understanding the planning of llm agents: A survey, 2024.\\n[25] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Enhancing ai systems with agentic workflows\\npatterns in large language model. In 2024 IEEE World AI IoT Congress (AIIoT), pages 527–532, 2024.\\n[26] DeepLearning.AI. How agents can improve llm performance. https://www.deeplearning.ai/the-batch/\\nhow-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io,\\n2024.\\nAc-\\ncessed: 2025-01-13.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 34}, page_content='how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io,\\n2024.\\nAc-\\ncessed: 2025-01-13.\\n[27] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,\\nSean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.\\n[28] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning, 2023.\\n[29] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and\\nXiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges, 2024.\\n[30] Weaviate Blog. What is agentic rag? https://weaviate.io/blog/what-is-agentic-rag#:~:text=is%\\n20Agentic%20RAG%3F-,%E2%80%8B,of%20the%20non%2Dagentic%20pipeline. Accessed: 2025-01-14.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 34}, page_content='[30] Weaviate Blog. What is agentic rag? https://weaviate.io/blog/what-is-agentic-rag#:~:text=is%\\n20Agentic%20RAG%3F-,%E2%80%8B,of%20the%20non%2Dagentic%20pipeline. Accessed: 2025-01-14.\\n[31] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation, 2024.\\n[32] LangGraph CRAG Tutorial. Langgraph crag: Contextualized retrieval-augmented generation tutorial. https:\\n//langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/. Accessed: 2025-01-14.\\n[33] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. Adaptive-rag: Learning to adapt\\nretrieval-augmented large language models through question complexity, 2024.\\n[34] LangGraph Adaptive RAG Tutorial. Langgraph adaptive rag: Adaptive retrieval-augmented generation tu-\\ntorial.\\nhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/.\\nAccessed: 2025-01-14.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 34}, page_content='torial.\\nhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/.\\nAccessed: 2025-01-14.\\n[35] Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Damien Graux, Dandan\\nTu, Zeren Jiang, Ruofei Lai, Yang Ren, and Jeff Z. Pan. Gear: Graph-enhanced agent for retrieval-augmented\\ngeneration, 2024.\\n[36] LlamaIndex.\\nIntroducing agentic\\ndocument\\nworkflows.\\nhttps://www.llamaindex.ai/blog/\\nintroducing-agentic-document-workflows, 2025. Accessed: 2025-01-13.\\n35'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 35}, page_content='[37] AWS\\nMachine\\nLearning\\nBlog.\\nHow\\ntwitch\\nused\\nagentic\\nworkflow\\nwith\\nrag\\non\\namazon\\nbedrock\\nto\\nsupercharge\\nad\\nsales.\\nhttps://aws.amazon.com/blogs/machine-learning/\\nhow-twitch-used-agentic-workflow-with-rag-on-amazon-bedrock-to-supercharge-ad-sales/,\\n2025. Accessed: 2025-01-13.\\n[38] LlamaCloud Demo Repository.\\nPatient case summary workflow using llamacloud.\\nhttps:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/\\npatient_case_summary/patient_case_summary.ipynb, 2025. Accessed: 2025-01-13.\\n[39] LlamaCloud Demo Repository.\\nContract review workflow using llamacloud.\\nhttps://github.com/\\nrun-llama/llamacloud-demo/blob/main/examples/document_workflows/contract_review/\\ncontract_review.ipynb, 2025. Accessed: 2025-01-13.\\n[40] LlamaCloud Demo Repository.\\nAuto insurance claims workflow using llamacloud.\\nhttps:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/auto_\\ninsurance_claims/auto_insurance_claims.ipynb, 2025. Accessed: 2025-01-13.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 35}, page_content='https:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/auto_\\ninsurance_claims/auto_insurance_claims.ipynb, 2025. Accessed: 2025-01-13.\\n[41] LlamaCloud Demo Repository.\\nResearch paper report generation workflow using llamacloud.\\nhttps://github.com/run-llama/llamacloud-demo/blob/main/examples/report_generation/\\nresearch_paper_report_generation.ipynb, 2025. Accessed: 2025-01-13.\\n[42] LangGraph Agentic RAG Tutorial. Langgraph agentic rag: Nodes and edges tutorial. https://langchain-ai.\\ngithub.io/langgraph/tutorials/rag/langgraph_agentic_rag/#nodes-and-edges.\\nAccessed:\\n2025-01-14.\\n[43] LlamaIndex\\nBlog.\\nAgentic\\nrag\\nwith\\nllamaindex.\\nhttps://www.llamaindex.ai/blog/\\nagentic-rag-with-llamaindex-2721b8a49ff6. Accessed: 2025-01-14.\\n[44] Hugging Face Cookbook. Agentic rag: Turbocharge your retrieval-augmented generation with query reformula-\\ntion and self-query. https://huggingface.co/learn/cookbook/en/agent_rag. Accessed: 2025-01-14.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 35}, page_content='tion and self-query. https://huggingface.co/learn/cookbook/en/agent_rag. Accessed: 2025-01-14.\\n[45] Qdrant Blog. Agentic rag: Combining rag with agents for enhanced information retrieval. https://qdrant.\\ntech/articles/agentic-rag/. Accessed: 2025-01-14.\\n[46] crewAI Inc. crewai: A github repository for ai projects. https://github.com/crewAIInc/crewAI, 2025.\\nAccessed: 2025-01-15.\\n[47] AG2AI Contributors. Ag2: A github repository for advanced generative ai research. https://github.com/\\nag2ai/ag2, 2025. Accessed: 2025-01-15.\\n[48] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun\\nZhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling\\nnext-gen llm applications via multi-agent conversation framework. 2023.\\n[49] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Training\\nlanguage model agents without modifying language models. ICML’24, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 35}, page_content='[49] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Training\\nlanguage model agents without modifying language models. ICML’24, 2024.\\n[50] OpenAI. Swarm: Lightweight multi-agent orchestration framework. https://github.com/openai/swarm.\\nAccessed: 2025-01-14.\\n[51] LlamaIndex Documentation. Agentic rag using vertex ai. https://docs.llamaindex.ai/en/stable/\\nexamples/agent/agentic_rag_using_vertex_ai/. Accessed: 2025-01-14.\\n[52] Microsoft. Semantic kernel overview, 2025. https://learn.microsoft.com/en-us/semantic-kernel/\\noverview/. Accessed: February 2, 2025.\\n[53] Microsoft. Semantic kernel github repository, 2025. https://github.com/microsoft/semantic-kernel.\\nAccessed: February 2, 2025.\\n[54] IBM Granite Community.\\nAgentic rag: Ai agents with ibm granite models.\\nhttps://github.com/\\nibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_\\nRAG.ipynb. Accessed: 2025-01-14.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 35}, page_content='Agentic rag: Ai agents with ibm granite models.\\nhttps://github.com/\\nibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_\\nRAG.ipynb. Accessed: 2025-01-14.\\n[55] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous\\nbenchmark for zero-shot evaluation of information retrieval models, 2021.\\n[56] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew\\nMcNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong\\nWang. Ms marco: A human generated machine reading comprehension dataset, 2018.\\n[57] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff.\\nOverview of the trec 2022 deep learning track. In Text REtrieval Conference (TREC). NIST, TREC, March 2023.\\n36'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 36}, page_content='[58] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\\nvia single-hop question composition, 2022.\\n[59] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset\\nfor comprehensive evaluation of reasoning steps, 2020.\\n[60] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\\n[61] Robert Friel, Masha Belyi, and Atindriyo Sanyal. Ragbench: Explainable benchmark for retrieval-augmented\\ngeneration systems, 2024.\\n[62] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and Stéphane\\nClinchant. Bergen: A benchmarking library for retrieval-augmented generation, 2024.\\n[63] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 36}, page_content='Clinchant. Bergen: A benchmarking library for retrieval-augmented generation, 2024.\\n[63] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient\\nretrieval-augmented generation research, 2024.\\n[64] Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning,\\n2024.\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei\\nChang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question\\nanswering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019.\\n[66] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\\nchallenge dataset for reading comprehension, 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 36}, page_content='[66] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\\nchallenge dataset for reading comprehension, 2017.\\n[67] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\\ncomprehension of text, 2016.\\n[68] Jonathan Berant, Andrew K. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-\\nanswer pairs. In Conference on Empirical Methods in Natural Language Processing, 2013.\\n[69] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to\\ntrust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers,\\nJordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada, July 2023. Association\\nfor Computational Linguistics.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 36}, page_content='Computational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada, July 2023. Association\\nfor Computational Linguistics.\\n[70] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form\\nquestion answering, 2019.\\n[71] Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward\\nGrefenstette. The narrativeqa reading comprehension challenge. 2017.\\n[72] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form\\nanswers, 2023.\\n[73] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. pages 5905–5921, June 2021.\\n[74] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 36}, page_content='multi-domain meeting summarization. pages 5905–5921, June 2021.\\n[74] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-\\nseeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke\\nZettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\\nZhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, pages 4599–4610, Online, June 2021. Association\\nfor Computational Linguistics.\\n[75] Timo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. COVID-QA: A question answering\\ndataset for COVID-19. In ACL 2020 Workshop on Natural Language Processing for COVID-19 (NLP-COVID),\\n2020.\\n[76] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 36}, page_content='2020.\\n[76] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang,\\nJianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. Cmb: A comprehensive medical benchmark in chinese,\\n2024.\\n[77] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh\\nPadmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R. Bowman. Quality: Question answering with\\nlong input texts, yes!, 2022.\\n37'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 37}, page_content='[78] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering\\nchallenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,\\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics.\\n[79] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan\\nHooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\n2024.\\n[80] Sha Li, Heng Ji, and Jiawei Han. Document-level event argument extraction by conditional generation, 2021.\\n[81] Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. Multi-sentence argument\\nlinking, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 37}, page_content='[81] Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. Multi-sentence argument\\nlinking, 2020.\\n[82] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia:\\nKnowledge-powered conversational agents, 2019.\\n[83] Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan,\\nIrwin King, and Kam-Fai Wong. Large language models as source planner for personalized knowledge-grounded\\ndialogue, 2023.\\n[84] Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. Long time\\nno see! open-domain conversation with long-term persona memory, 2022.\\n[85] Tsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David\\nVandyke, and Steve Young. Conditional generation and snapshot learning in neural dialogue systems. In\\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2153–2162,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 37}, page_content='Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2153–2162,\\nAustin, Texas, November 2016. Association for Computational Linguistics.\\n[86] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class\\ncollaborative filtering. In Proceedings of the 25th International Conference on World Wide Web, WWW ’16, page\\n507–517, Republic and Canton of Geneva, CHE, 2016. International World Wide Web Conferences Steering\\nCommittee.\\n[87] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really\\nfinish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4791–4800, Florence, Italy, July 2019.\\nAssociation for Computational Linguistics.\\n[88] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 37}, page_content='Association for Computational Linguistics.\\n[88] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The\\ncot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning,\\n2023.\\n[89] Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex\\nsequential question answering: Towards learning to converse over linked question answer pairs with a knowledge\\ngraph. 2018.\\n[90] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for\\nfact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018\\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 37}, page_content='Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n[91] Neema Kotonya and Francesca Toni. Explainable automated fact-checking for public health claims, 2020.\\n[92] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop?\\na question answering benchmark with implicit reasoning strategies, 2021.\\n[93] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig. Wikiasp:\\nA dataset for multi-domain aspect-based summarization, 2020.\\n[94] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware\\nconvolutional neural networks for extreme summarization, 2018.\\n[95] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher\\nPotts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 37}, page_content='Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky,\\nTimothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013\\nConference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington,\\nUSA, October 2013. Association for Computational Linguistics.\\n38'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 38}, page_content='[96] Sourav Saha, Jahedul Alam Junaed, Maryam Saleki, Arnab Sen Sharma, Mohammad Rashidujjaman Rifat,\\nMohamed Rahouti, Syed Ishtiaque Ahmed, Nabeel Mohammed, and Mohammad Ruhul Amin. Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms of communal violence and its evaluation. In\\nFiroj Alam, Sudipta Kar, Shammur Absar Chowdhury, Farig Sadeque, and Ruhul Amin, editors, Proceedings\\nof the First Workshop on Bangla Language Processing (BLP-2023), pages 72–84, Singapore, December 2023.\\nAssociation for Computational Linguistics.\\n[97] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet\\nchallenge: Evaluating the state of semantic code search, 2020.\\n[98] Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo,\\nXiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, and Jimmy Lin. \"knowing when you don’t know\":'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 38}, page_content='Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, and Jimmy Lin. \"knowing when you don’t know\":\\nA multilingual relevance assessment dataset for robust retrieval-augmented generation, 2024.\\n[99] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\\n[100] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\\nsolve math word problems, 2021.\\n[101] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, Dan Tufi¸s, and Dániel Varga.\\nThe JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Nicoletta Calzolari, Khalid\\nChoukri, Aldo Gangemi, Bente Maegaard, Joseph Mariani, Jan Odijk, and Daniel Tapias, editors, Proceedings of\\nthe Fifth International Conference on Language Resources and Evaluation (LREC‘06), Genoa, Italy, May 2006.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-05T01:26:00+00:00', 'source': '../data/pdf/AR-RAG.pdf', 'file_path': '../data/pdf/AR-RAG.pdf', 'total_pages': 39, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-02-05T01:26:00+00:00', 'trapped': '', 'modDate': 'D:20250205012600Z', 'creationDate': 'D:20250205012600Z', 'page': 38}, page_content='the Fifth International Conference on Language Resources and Evaluation (LREC‘06), Genoa, Italy, May 2006.\\nEuropean Language Resources Association (ELRA).\\n39'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0}, page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0}, page_content='transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0}, page_content='efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 2}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 3}, page_content='1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values h times with different, learned'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 3}, page_content='we found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 4}, page_content='MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 4}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 4}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 4}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5}, page_content='during training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 6}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 6}, page_content='convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 6}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 6}, page_content='(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\nResidual Dropout\\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [15]\\n23.75\\nDeep-Att + PosUnk [32]\\n39.2\\n1.0 · 1020\\nGNMT + RL [31]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [8]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [26]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [32]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [31]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [8]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.0\\n2.3 · 1019\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 7}, page_content='hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 7}, page_content='previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2\\nModel Variations'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 7}, page_content='model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 8}, page_content='(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 8}, page_content='multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 8}, page_content='tensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 9}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 9}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 9}, page_content='Recognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 9}, page_content='2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 10}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 10}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf/attention-is-all-you-need.pdf', 'file_path': '../data/pdf/attention-is-all-you-need.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 10}, page_content='networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 0}, page_content='ScienceDirect\\nAvailable online at www.sciencedirect.com\\nProcedia Computer Science 246 (2024) 3781–3790\\n1877-0509 © 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge \\nBased and Intelligent information and Engineering Systems\\n10.1016/j.procs.2024.09.178\\nKeywords: Large Language Models (LLMs); Natural Language Processing (NLP); Retrieval-Augmented Generation (RAG); Text generation; \\nDigital transformation. \\n1. Introduction \\nDigital transformation signifies the incorporation of digital technology across different facets of a business, \\nreshaping its operations and value delivery to customers [1]. At the forefront of driving such transformative \\npractices are Large Language Models (LLMs), advanced machine learning models trained extensively on textual'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 0}, page_content='practices are Large Language Models (LLMs), advanced machine learning models trained extensively on textual \\ndata to comprehend and produce human-like text [1]. LLMs, such as the Generative Pre-training Transformer (GPT) \\n \\n \\n* Corresponding author. Tel.: +33 03 80 39 50 00; fax: +33 03 80 39 50 69.  \\nE-mail address: muhammad.arslan@u-bourgogne.fr \\n28th International Conference on Knowledge-Based and Intelligent Information & Engineering \\nSystems (KES 2024) \\nA Survey on RAG with LLMs \\nMuhammad Arslana*, Hussam Ghanema, Saba Munawarb and Christophe Cruza \\naLaboratoire Interdisciplinaire Carnot de Bourgogne (ICB), Dijon, France \\nbNational University of Computer and Emerging Sciences (NUCES), Islamabad, Pakistan                                                                 \\nAbstract \\nIn the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 0}, page_content='Abstract \\nIn the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain \\ncompetitive and foster growth. Large Language Models (LLMs) have emerged as game-changers across industries, \\nrevolutionizing various sectors by harnessing extensive text data to analyze and generate human-like text. Despite their \\nimpressive capabilities, LLMs often encounter challenges when dealing with domain-specific queries, potentially leading to \\ninaccuracies in their outputs. In response, Retrieval-Augmented Generation (RAG) has emerged as a viable solution. By \\nseamlessly integrating external data retrieval into text generation processes, RAG aims to enhance the accuracy and relevance of \\nthe generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 0}, page_content='the generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG, \\noverlooking a comprehensive exploration of its applications. This paper seeks to address this gap by providing a thorough review \\nof RAG applications, encompassing both task-specific and discipline-specific studies, while also outlining potential avenues for \\nfuture research. By shedding light on current RAG research and outlining future directions, this review aims to catalyze further \\nexploration and development in this dynamic field, thereby contributing to ongoing digital transformation efforts. \\n© 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge Based and \\nIntelligent information and Engineering Systems'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 1}, page_content='3782\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nseries [2, 3] and others, have demonstrated remarkable capabilities in NLP tasks [4]. However, these models face \\nchallenges when dealing with domain-specific queries, often generating inaccurate or irrelevant information, \\ncommonly referred to as “hallucinations”, particularly when data is sparse [5]. This limitation makes deploying \\nLLMs in real-world settings impractical, as the generated output may not be reliable [4].  \\nIn the middle of 2020, Lewis et al. [6] introduced RAG, a significant advancement in the field of LLMs for \\nimproving generative tasks (see Fig. 1 (a)). RAG incorporates an initial step where LLMs search an external data \\nsource to retrieve relevant information before producing text or answering questions. RAG addresses these \\nlimitations by integrating external data retrieval into the generative process, thereby enhancing the accuracy and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 1}, page_content='limitations by integrating external data retrieval into the generative process, thereby enhancing the accuracy and \\nrelevance of the generated output. By dynamically retrieving information from knowledge bases during inference, \\nRAG provides a more informed and evidence-based approach to language generation, significantly reducing the risk \\nof hallucinations and improving the overall quality of the generated text [4, 6]. This approach has the potential to \\nmake LLMs more practical for real-world applications, as it ensures that the generated output is grounded in \\nretrieved evidence, leading to more reliable and accurate results. Fig. 1 (b) showcases how real-time business \\nsystems can leverage the RAG with LLM architecture. As an example, without RAG, the system lacks access to \\nreal-time or updated information. However, with RAG integration, leveraging external data sources such as news'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 1}, page_content='real-time or updated information. However, with RAG integration, leveraging external data sources such as news \\narticles, the system can respond to current business events, presenting opportunities for business intelligence \\nanalysts. \\n \\n                                  (a)                                                                                             (b) \\nFig. 1. (a) A generic RAG architecture, where users’ queries, potentially in different modalities (e.g., text, code, image, etc.), are inputted into \\nboth the retriever and the generator. The retriever scans for relevant data sources in storage, while the generator engages with the retrieval \\noutcomes, ultimately generating results across various modalities [6]; Fig. 1. (b) illustrates how RAG integration with the LLM handles queries \\nthat fall outside the scope of the LLM’s training data.  \\nWhile the field of RAG has seen substantial growth, several online surveys [4, 7, 8, 9] have explored'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 1}, page_content='that fall outside the scope of the LLM’s training data.  \\nWhile the field of RAG has seen substantial growth, several online surveys [4, 7, 8, 9] have explored \\ntechnological advancements in RAG. Although these surveys provide valuable insights and references, they offer \\nonly a limited overview of RAG applications. To address this gap, this paper aims to provide an exhaustive \\noverview of RAG applications, including both task-specific and discipline-specific studies, as well as future \\ndirections. By highlighting the current state of RAG research and its potential future directions, this review aims to \\ninspire further investigation and development in this exciting field. \\nThe paper’s structure is as follows: Section 2 presents the adopted research methodology for this survey. In \\nSection 3, we provide an overview of RAG applications, followed by a detailed discussion in Section 4. The paper \\nconcludes in Section 5, summarizing the key findings and implications of the study.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 1}, page_content='Section 3, we provide an overview of RAG applications, followed by a detailed discussion in Section 4. The paper \\nconcludes in Section 5, summarizing the key findings and implications of the study. \\n2. Background \\nThe research method (see Fig. 2) employed in this paper involves a thorough review and analysis of research \\npublications related to RAG. The main objective is to identify and categorize its applications across various NLP \\ntasks and disciplines. The paper begins by collecting research publications specific to RAG, focusing on their \\napplications. Since the RAG with LLM domain is relatively new and emerging, with many studies available as pre-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 2}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3783\\nprints online, limiting the search to platforms such as Scopus or IEEE would greatly reduce the number of studies. \\nTherefore, Google Scholar was utilized to access the studies on RAG. However, in cases where both pre-print and \\npublished versions of a study were available, the published version was chosen to cover the maximum number of \\npeer-reviewed studies. Each study underwent manual review to assess its comprehensiveness and depth, excluding \\nshort studies. It is important to note that the purpose of the survey is not to cover the most optimal studies, but rather \\nto provide an overview of how this field has attained significant attention in a short period, with researchers \\nexploring diverse application scenarios.  \\nThe keywords used to collect research publications included “retrieval augmented generation”, “RAG'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 2}, page_content='exploring diverse application scenarios.  \\nThe keywords used to collect research publications included “retrieval augmented generation”, “RAG \\napplications”, “generative models with retrieval”, “external data retrieval in text generation”, “enhancing text \\ngeneration with retrieval”, “integrating retrieval into generative models”, “external knowledge in text generation”, \\n“retrieval-based text generation”, “information retrieval for text generation”, and “contextualized retrieval in \\nlanguage models”. These publications are then classified into two principal categories: task-based classification and \\ndiscipline-based classification. Task-based classification focuses on categorizing RAG studies according to their \\nexecution of information processing tasks, particularly within NLP. Conversely, discipline-based classification \\ncategorizes studies based on their application to specific domains. Under the task-based classification, the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 2}, page_content='categorizes studies based on their application to specific domains. Under the task-based classification, the \\npublications are further subdivided into categories such as Question Answering (QA), Text Generation and \\nSummarization, Information Retrieval and Extraction, Text Analysis and Processing, Software Development and \\nMaintenance (SDM), Decision Making and Applications, and Other Categories. Similarly, under the discipline-\\nbased classification, the publications are further subdivided into categories such as Medical/Biomedical, Financial, \\nEducational, Technology and Software Development, Social and Communication, Literature, and Other Categories. \\nThese categories are selected based on an understanding of the context of the studies and the underlying problems \\nthey address. Within both classification methods, “software development” stands out as a common category. It \\ninvolves programming information processing tasks under task-based classification and encompasses systems for'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 2}, page_content='involves programming information processing tasks under task-based classification and encompasses systems for \\ndeveloping various applications across different domains under discipline-based classification. Figure 3 illustrates \\nthe number of publications related to RAG applications from 2020 to February 2024. Specifically, there was a single \\npublication found in 2020, 6 publications in 2022, 28 publications in 2023, and 16 publications until February 2024, \\nindicating a growing interest and research activity in the field of RAG applications. \\nFig. 2. Research Method'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 3}, page_content='3784\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Evolution of Research Publications on RAG Applications \\n3. Applications of RAG with LLMs \\nUpon thorough examination of the selected papers focusing on RAG applications, we uncovered a vast array of \\ndiverse applications. These findings are distilled into a comprehensive table format (see Table 1), detailing three \\ncrucial aspects: 1) Use case with RAG, 2) Used datasets/benchmarks, and 3) Application area. Noteworthy \\napplications span various domains, including biomedical, financial, and medical inquiries, alongside text \\nsummarization and book review generation. RAG’s versatility extends to commonsense QA, table-based queries, \\nand clinical decision-making, among others. It further encompasses educational decision making, textbook question \\nanswering, and enterprise search functionalities. RAG is instrumental in sentiments classification, health education,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 3}, page_content='answering, and enterprise search functionalities. RAG is instrumental in sentiments classification, health education, \\nand generating biomedical explanations, while also enhancing user writing accuracy and speed. Its utility spans \\nhumanitarian assistance, generating informative dialogues, crafting realistic images and intricate plotlines, and much \\nmore.  \\nAdditionally, RAG aids in natural language QA, disease identification, and information extraction. It handles \\ndecision-making tasks, hashtag management, hate speech detection, and scientific document classification. RAG \\nexcels in entity description generation, text correction, and SQL translation, while also enhancing open-domain QA \\nand professional knowledge inquiries. Also, it extends the capabilities of machine translation tasks beyond text-to-\\nSQL, such as neural text re-ranking [6]. Moreover, it supports multicultural enterprise queries, e-commerce'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 3}, page_content='SQL, such as neural text re-ranking [6]. Moreover, it supports multicultural enterprise queries, e-commerce \\nsearches, and personalized dialogue systems. Furthermore, RAG facilitates event argument extraction, intelligence \\nreport generation, short-form QA, automated transactions, and private data handling. Lastly, it contributes to science \\nQA, clinical writing, and pharmaceutical regulatory compliance inquiries. \\nAfter compiling all the applications of RAG, the subsequent step involves categorizing them based on the \\nspecific nature of the NLP tasks they tackle (see Table 2 and Fig. 4). From the compiled publications, it was \\nobserved that 20 studies were dedicated to QA, 6 to Text Generation and Summarization, 6 to Information Retrieval \\nand Extraction, 5 to Text Analysis and Processing, 4 to SDM, and 5 to Decision Making and Applications, while the \\nremaining 6 studies were classified under \"Other Categories.\" This classification is significant as it helps in'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 3}, page_content='remaining 6 studies were classified under \"Other Categories.\" This classification is significant as it helps in \\nunderstanding the distribution and focus of RAG applications across different NLP tasks. Additionally, since RAG \\napplications span various disciplines, further classification (see Table 3 and Fig. 5) reveals that 9 publications were \\nrelated to Medical/Biomedical, 2 to Financial, 2 to Educational, 9 to Technology and Software Development, 7 to \\nSocial and Communication, and 3 to Literature, with the remaining falling into \"Other Categories\". \\nTable 1. Applications of RAG  \\nNo. \\nUse case with RAG \\nUsed datasets / benchmarks \\nApplication area \\n1 \\nMIRAGE: Medical information RAG [10]  \\nMedical QA datasets \\nBiomedical QA \\n2 \\nRAG for improved context accuracy [11]  \\nFinancial reports \\nFinancial QA \\n3 \\nRetrieval-augmented Electrocardiography (ECG) [12]  \\nCardiac symptoms and sleep apnea diagnosis \\nMedical QA \\n4 \\nRepresentative Vector Summarization (RVS) [13]'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 3}, page_content='Financial reports \\nFinancial QA \\n3 \\nRetrieval-augmented Electrocardiography (ECG) [12]  \\nCardiac symptoms and sleep apnea diagnosis \\nMedical QA \\n4 \\nRepresentative Vector Summarization (RVS) [13]  \\nPDFs, text documents, spreadsheets, etc.  \\nMedical text summarization \\n5 \\nRetrieval-augmented controllable reviews [14] \\nAmazon book reviews \\nBook review generation'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3785\\n6 \\nRetrieval-augmented knowledge graph reasoning [15] \\nCommonsense QA and OpenBookQA. \\nCommonsense QA \\n7 \\nAnswers from table corpus via RAG [16] \\nWikipedia data \\nTable QA  \\n8 \\nLiVersa: a liver disease specific LLM using RAG [17] \\nLiver Diseases  \\nMedical QA \\n9 \\nAlmanac: RAG for clinical medicine [18] \\nGuidelines and treatment recommendations. \\nClinical decision-making \\n10 \\nAssessment of tutoring practices [19] \\nDialogue transcripts from a middle-school.  \\nEducational decision making \\n11 \\nHandling out of domain scenarios [20] \\nLife science, earth science, etc. lessons.  \\nTextbook QA \\n12 \\nAutomated form filling [21] \\nRequest forms for IT projects \\nEnterprise search \\n13 \\nFinancial sentiment analysis [22]  \\nTwitter financial news and FiQA datasets \\nSentiments classification \\n14 \\nFrontline health worker capacity building [23] \\nPregnancy-related guidelines \\nHealth education QA \\n15'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='Twitter financial news and FiQA datasets \\nSentiments classification \\n14 \\nFrontline health worker capacity building [23] \\nPregnancy-related guidelines \\nHealth education QA \\n15 \\nSelf-BioRAG: a framework for biomedical text [24] \\nBiomedical instruction sets \\nBiomedical Informatics \\n16 \\nHybrid RAG for real-time composition assistance [25] \\nWikiText-103, Enron Emails, etc.  \\nWriting speed and accuracy \\n17 \\nRAG-Fusion to obtain product information [26] \\nProduct datasheets \\nTechnical information QA \\n18 \\nCommit message generation for code intelligence [27]  \\nMCMD dataset  \\nSDM \\n19 \\nFloodBrain: Flood disaster reporting [28] \\nReliefWeb reports \\nHumanitarian assistance \\n20 \\nRich answer encoding [29] \\nMSMARCO QA and WoW dataset. \\nGenerative QA  \\n21 \\nText-to-image generator [30] \\nCOCO and WikiImages datasets. \\nRealistic images generation \\n22 \\nCode completion framework [31] \\nCodeXGLUE and CodeNet datasets. \\nSDM \\n23 \\nComplex story generation framework [32] \\nIMDB movie details dataset'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='Realistic images generation \\n22 \\nCode completion framework [31] \\nCodeXGLUE and CodeNet datasets. \\nSDM \\n23 \\nComplex story generation framework [32] \\nIMDB movie details dataset \\nGenerate stories  \\n24 \\nTRAC: Trustworthy retrieval augmented chatbot [33] \\nNatural Question dataset \\nNatural QA \\n25 \\nClinfo.ai using scientific literature [34] \\nPubMed dataset \\nMedical QA \\n26 \\nRealGen for controllable traffic scenarios [35] \\nnuScenes dataset \\nCritical traffic scenarios \\n27 \\nZero-shot disease phenotyping [36]  \\nClinical notes \\nIdentifying diseases \\n28 \\nRAP-Gen for automatic program repair [37]  \\nTFix, Defects4J, etc. datasets \\nSDM \\n29 \\nCode4UIE : retrieval-augmented code generation [38] \\nACE04, ACE05, CoNLL03, etc. datasets \\nInformation extraction  \\n30 \\nRAP: retrieval-augmented planning [39] \\nALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31 \\nRIGHT for mainstream hashtag recommendation [40] \\nTwitter and Weibo data.  \\nRetrieval-enhanced hashtags \\n32'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='ALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31 \\nRIGHT for mainstream hashtag recommendation [40] \\nTwitter and Weibo data.  \\nRetrieval-enhanced hashtags \\n32 \\nRAUCG for counter narrative generation for hate speech \\n[41] \\nMultitargetCONAN dataset \\nCombating hate speech \\n33 \\nWeakly-supervised scientific document classification \\n[42] \\nAGNews and MeSH datasets.  \\nScientific documents \\nclassification \\n34 \\nrT5 for Chinese entity description generation [43] \\nXunZi and MengZi datasets.  \\nEntity description generation \\n35 \\nRSpell: domain adaptive Chinese spelling check [44]  \\nCSC dataset \\nText error correction \\n36 \\nXRICL: cross-lingual retrieval-augmented in-context \\nlearning for cross-lingual text-to-SQL semantic parsing \\n[45] \\nXSPIDER and XKAGGLE-DBQA datasets. \\nText-to-SQL translation \\n37 \\nSELF-RAG: learning to retrieve, generate, and \\ncritique through self-reflection [46] \\nOpen-Instruct processed data. \\nOpen-domain QA and fact \\nverification \\n38'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='Text-to-SQL translation \\n37 \\nSELF-RAG: learning to retrieve, generate, and \\ncritique through self-reflection [46] \\nOpen-Instruct processed data. \\nOpen-domain QA and fact \\nverification \\n38 \\nChatDOC with enhanced PDF structure recognition [47] \\nAcademic papers, financial reports, \\ntextbooks, and legislative materials \\nProfessional knowledge QA \\n39 \\nG-Retriever for textual graph understanding [48] \\nGraphQA (ExplaGraphs, SceneGraphs and \\nWebQSP)  \\nChat with graphs \\n40 \\nEnhancing multilingual information retrieval in \\nmixed Human Resources (HR) environments [49] \\nHR standard operating procedures and \\nQuality Assurance (QA) documents \\nMulticultural enterprise QA \\n41 \\nDifferentiable RAG [50] \\nUser-clicked logs \\nE-commerce search (query \\nintent classification) \\n42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data \\nSDM \\n43 \\nUniMS-RAG: a unified multi-source RAG [52] \\nDuLeMon and KBP datasets \\nPersonalized dialogue \\nsystems \\n44'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data \\nSDM \\n43 \\nUniMS-RAG: a unified multi-source RAG [52] \\nDuLeMon and KBP datasets \\nPersonalized dialogue \\nsystems \\n44 \\nRAG QA for event argument extraction [53] \\nACE 2005 and WikiEvent datasets \\nEvent argument (answer) \\nextraction \\n45 \\nFABULA: retrieval-augmented narrative construction \\n[54] \\nOntoNotes and Pile datasets \\nIntelligence report \\ngeneration \\n46 \\nTime-Aware Adaptive Retrieval (TA-ARE) [55] \\nRetrievalQA dataset \\nShort-form open-domain \\nQA \\n47 \\nCash transaction booking via RAG [56] \\nCash Management Software (CMS) \\ntransactions. \\nAutomated cash transaction \\nbooking \\n48 \\nRetrieval-Augmented Thought Process (RATP) [57] \\nBoolq and emrQA datasets. \\nQA with private data \\n49 \\nATLANTIC for interdisciplinary science [58] \\nS2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50 \\nWriting documents for clinical trials [59] \\nFDA guidance database, ClinicalTrials.gov, \\nand AACT database.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 4}, page_content='S2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50 \\nWriting documents for clinical trials [59] \\nFDA guidance database, ClinicalTrials.gov, \\nand AACT database.  \\nClinical-related writing \\n51 \\nQA RAG model [60] \\nFDA Q&A datasets  \\nPharma industry regulatory \\ncompliance QA'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 5}, page_content='3786\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n4. Discussion  \\nThe classification of RAG applications according to the specific NLP tasks they target holds significant \\nimportance for several reasons. Firstly, it offers valuable insights into the distribution and focus of RAG applications \\nacross various tasks within the field of NLP. By quantifying the number of studies dedicated to each task, \\nresearchers gain a deeper understanding of where efforts and resources are predominantly concentrated within the \\nRAG domain. By analyzing the distribution of RAG applications, researchers can discern prevailing trends in \\nresearch interest and identify emerging areas of importance. The classification of RAG applications based on \\ndiscipline offers valuable insights into its widespread adoption across various domains. This classification not only \\nprovides a comprehensive understanding of RAG’s applicability but also underscores its potential to revolutionize'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 5}, page_content='provides a comprehensive understanding of RAG’s applicability but also underscores its potential to revolutionize \\nvarious domains, thereby contributing significantly to the advancement of NLP technologies.  \\nWhile this survey offers a comprehensive overview of RAG applications across various NLP tasks and \\ndisciplines, it also has its limitations. 1) Given that RAG technology is still emerging, the majority of RAG-based \\nstudies are available in pre-print formats on platforms like arXiv, lacking peer review. This raises questions about \\ntheir authenticity. 2) Additionally, the survey overlooks the technical implementation details and challenges \\nassociated with using RAG technology alongside open-source LLMs. Organizations may find RAG implementation \\ncostly if they do not opt for open-source LLM architectures, especially considering the expense of querying the \\nLLM via Application Programming Interface (API). 3) Furthermore, the performance of RAG concerning the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 5}, page_content='LLM via Application Programming Interface (API). 3) Furthermore, the performance of RAG concerning the \\nvolume and variety of datasets has not been discussed. Deploying RAG with large datasets of varying structures \\n(e.g., structured, semi-structured, or non-structured) may lead to processing delays, warranting further exploration \\nbefore selecting a RAG with LLM integrated solution for organizational deployment.  \\n4) Additionally, this survey did not cover the diverse range of RAG architectures and technologies available for \\nintegration with different LLMs. Future work should delve into these options to discuss how various RAG solutions \\ncan be adapted with LLMs for different NLP tasks and applications. 5) Furthermore, the survey did not address the \\naccuracy of information obtained from RAG with LLM solutions. It is essential to explore the reliability of these \\nsystems and assess the organizations’ dependency on their generated responses. LLMs often generate responses with'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 5}, page_content='systems and assess the organizations’ dependency on their generated responses. LLMs often generate responses with \\nhigh confidence, making it challenging to evaluate the accuracy of the information provided. 6) While the survey \\nprimarily focuses on task-based and discipline-based applications of RAG, there is a need for further research to \\nexplore ethical considerations associated with its usage, especially when dealing with sensitive datasets. For \\nexample, in the biomedical domain, RAG has the potential to accidentally expose private information to analysts, \\nraising concerns about data privacy and security. Additionally, in the legal domain, RAG may mistakeably reveal \\nprivileged information during document analysis, potentially violating client confidentiality and attorney-client \\nprivilege. Therefore, future studies should delve deeper into these ethical implications to ensure responsible and \\nethical use of RAG technology across various domains. \\n5. Conclusion'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 5}, page_content='privilege. Therefore, future studies should delve deeper into these ethical implications to ensure responsible and \\nethical use of RAG technology across various domains. \\n5. Conclusion  \\nThis article offers a thorough examination of the applications of RAG with LLMs, showcasing their potential to \\ndrive digital transformation across diverse industries. Initially, it gathers the latest publications on RAG from online \\nrepositories. These publications are then classified based on task-oriented and discipline-oriented criteria. A notable \\ntrend observed is the increasing number of research papers on RAG deposited in open-access sources, particularly \\nsince 2023. However, many works remain unpublished or are in the preprint stage, awaiting review by various \\njournals. A significant portion of these studies primarily focus on the task of QA in NLP. Conversely, there is a \\nnoticeable gap in research exploring Entity Linking, an essential NLP task that contributes to knowledge graph'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 5}, page_content='noticeable gap in research exploring Entity Linking, an essential NLP task that contributes to knowledge graph \\ndevelopment. Addressing this gap could unlock numerous applications in the realm of linked data. Regarding \\ndisciplines, the majority of research applications are concentrated in the fields of Medical/Biomedical and \\nTechnology and Software Development. In contrast, disciplines such as Business and Agriculture receive \\ncomparatively less attention. Future research endeavors should aim to bridge this gap by addressing the specific \\nneeds of these underrepresented disciplines.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 6}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3787\\nTable 2. Task-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 4. Task-based classification of RAG applications with count of publications. The word cloud is generated based on the publication counts \\nlisted under various headings in Table 2. \\n \\n \\n \\n \\n1) Question Answering (QA)  \\n- Biomedical QA [1] \\n- Financial QA [2] \\n- Medical QA [3] \\n- Commonsense QA [6] \\n- Textbook QA [11] \\n- Health education QA [14] \\n- Technical product information QA [17] \\n- Natural QA [24] \\n- Professional knowledge QA [38] \\n- Multicultural enterprise QA [40] \\n- Open-domain QA and fact verification [46] \\n- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29] \\n- Pharma industry regulatory compliance QA [51]'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 6}, page_content='- Open-domain QA and fact verification [46] \\n- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29] \\n- Pharma industry regulatory compliance QA [51] \\n- Science QA and document classification [49] \\n- Clinical-related writing [50] \\n- Personalized dialogue systems [43] \\n2) Text Generation and Summarization \\n- Medical text summarization [4] \\n- Book review generation [5] \\n- Biomedical Informatics [15] \\n- Generate stories with complex plots [23] \\n- Generate realistic and faithful images [21] \\n- Entity description generation [34] \\n \\n3) Information Retrieval and Extraction \\n- Table QA [7] \\n- Enterprise search [12] \\n- Retrieval-enhanced hashtags [31] \\n- Information extraction [29] \\n- Event argument (answer) extraction [44] \\n- E-commerce search (query intent classification) [41] \\n4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Scientific documents classification [33]'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 6}, page_content='4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Scientific documents classification [33] \\n- Combating online hate speech [32] \\n \\n5) Software Development and \\nMaintenance \\n- Code intelligence [18] \\n- Code completion [22] \\n- Automatic program repair [28] \\n- Elevate low-code developer skills [42] \\n \\n6) Decision Making and Applications \\n- Clinical decision-making [9] \\n- Educational decision making [10] \\n- Decision-making applications [30] \\n- Automated cash transaction booking [47] \\n- Intelligence report generation [45] \\n \\n7) Other Categories: \\n- Editing and crafting diverse behaviors, \\nincluding critical traffic scenarios [26] \\n- Identifying diseases [27] \\n- Chat with graphs [39] \\nTask: (Count of Publications)  \\nQuestion Answering (QA): (20) \\nText Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6) \\nText Analysis and Processing: (5) \\nSoftware Development and Maintenance: (4)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 6}, page_content='Question Answering (QA): (20) \\nText Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6) \\nText Analysis and Processing: (5) \\nSoftware Development and Maintenance: (4) \\nDecision Making and Applications: (5) \\nOther Categories: (6)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 7}, page_content='3788\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nTable 3. Discipline-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 5. Discipline-based classification of RAG applications with count of publications. The word cloud is generated based on the publication \\ncounts listed under various headings in Table 3. \\nAcknowledgements \\nThe authors thank the French Government and the National Research Agency (ANR) for their funding. \\nReferences \\n[1] Roumeliotis KI, Tselikas ND, & Nasiopoulos DK. (2024). “LLMs in e-commerce: a comparative analysis of GPT and LLaMA models in \\nproduct review evaluation,” Natural Language Processing Journal:1-6:100056.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 7}, page_content='product review evaluation,” Natural Language Processing Journal:1-6:100056. \\n[2] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). “Language Models are Few-Shot \\nLearners,” Advances in Neural Information Processing Systems 33 (NeurIPS 2020).  \\n[3] OpenAI, R. (2023). “Gpt-4 technical report,” arxiv 2303.08774. View in Article: 2(5). \\n1) Medical / Biomedical \\n- Biomedical QA [1] \\n- Medical QA [3] \\n- Medical text summarization [4] \\n- Health education QA [14] \\n- Identifying diseases [27] \\n- Clinical decision-making [9] \\n- Clinical-related writing [50] \\n- Science QA and scientific document classification [49] \\n- Pharma industry regulatory compliance QA [51] \\n2) Financial \\n- Financial QA [2] \\n- Automated cash transaction booking [47] \\n3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11] \\n4) Technology and Software Development \\n- Table QA [7] \\n- Technical product information QA [17]'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 7}, page_content='3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11] \\n4) Technology and Software Development \\n- Table QA [7] \\n- Technical product information QA [17] \\n- Software development and maintenance [18, 22, 28, 42] \\n- Generative QA and informative conversations [20] \\n- Information extraction [29] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Personalized dialogue systems [43] \\n- Event argument (answer) extraction [44] \\n5) Social and Communication \\n- Commonsense QA [6] \\n- Sentiments classification [13] \\n- Combating online hate speech [32] \\n- Retrieval-enhanced hashtags [31] \\n- Humanitarian assistance [19] \\n- Chat with graphs [39] \\n- Multicultural enterprise QA [40] \\n6) Literature  \\n- Book review generation guided by reference documents [5] \\n- Enhance user writing speed and accuracy [16] \\n- Generate stories with complex plots [23] \\n7) Other Categories   \\n- Enterprise search [12] \\n- Generate realistic and faithful images [21]'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 7}, page_content='- Enhance user writing speed and accuracy [16] \\n- Generate stories with complex plots [23] \\n7) Other Categories   \\n- Enterprise search [12] \\n- Generate realistic and faithful images [21] \\n- Decision-making applications [30] \\n- Open-domain question answering and fact verification [37] \\n- Professional knowledge QA [38] \\n- Intelligence report generation [45] \\n- Short-form open-domain QA [46] \\n- Question answering with private data [48] \\n \\nDiscipline: (Count of Publications)  \\nMedical / Biomedical: (9) \\nFinancial: (2) \\nEducational: (2) \\nTechnology and Software Development: (9) \\nSocial and Communication: (7) \\nLiterature (3) \\nOther Categories: (8)'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content='Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3789\\n[4] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., ... & Wang, H. (2023). “Retrieval-augmented generation for large language models: A \\nsurvey,” arXiv preprint arXiv:2312.10997. \\n[5] Kandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2023). “Large language models struggle to learn long-tail knowledge,” In \\nInternational Conference on Machine Learning. PMLR: 5696-15707. \\n[6] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). “Retrieval-augmented generation for knowledge-\\nintensive nlp tasks,” Advances in Neural Information Processing Systems 33: 9459-9474. \\n[7] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). “A survey on retrieval-augmented text generation,” arXiv preprint arXiv:2202.01110. \\n[8] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., ... & Scialom, T. (2023). “Augmented language models: a'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content='[8] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., ... & Scialom, T. (2023). “Augmented language models: a \\nsurvey,” arXiv preprint arXiv:2302.07842. \\n[9] Zhao, R., Chen, H., Wang, W., Jiao, F., Do, X. L., Qin, C., ... & Joty, S. (2023). “Retrieving multimodal information for augmented \\ngeneration: A survey,” arXiv preprint arXiv:2303.10868. \\n[10] Xiong, G., Jin, Q., Lu, Z., & Zhang, A. (2024). “Benchmarking retrieval-augmented generation for medicine,” arXiv preprint \\narXiv:2402.13178. \\n[11] Jimeno Yepes, A., You, Y., Milczek, J., Laverde, S., & Li, L. (2024). “Financial Report Chunking for Effective Retrieval Augmented \\nGeneration,” arXiv e-prints, arXiv-2402. \\n[12] Yu, H., Guo, P., & Sano, A. (2023). “Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation,” \\nIn Machine Learning for Health (ML4H) PMLR: 650-663.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content='[12] Yu, H., Guo, P., & Sano, A. (2023). “Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation,” \\nIn Machine Learning for Health (ML4H) PMLR: 650-663. \\n[13] Manathunga, S. S., & Illangasekara, Y. A. (2023). “Retrieval Augmented Generation and Representative Vector Summarization for large \\nunstructured textual data in Medical Education,” arXiv preprint arXiv:2308.00479. \\n[14] Kim, J., Choi, S., Amplayo, R. K., & Hwang, S. W. (2020). “Retrieval-augmented controllable review generation,” In Proceedings of the \\n28th International Conference on Computational Linguistics: 2284-2295. \\n[15] Sha, Y., Feng, Y., He, M., Liu, S., & Ji, Y. (2023). “Retrieval-augmented Knowledge Graph Reasoning for Commonsense Question \\nAnswering,” Mathematics 11(15): 3269; https://doi.org/10.3390/math11153269.  \\n[16] Pan, F., Canim, M., Glass, M., Gliozzo, A., & Hendler, J. (2022). “End-to-End Table Question Answering via Retrieval-Augmented'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content='[16] Pan, F., Canim, M., Glass, M., Gliozzo, A., & Hendler, J. (2022). “End-to-End Table Question Answering via Retrieval-Augmented \\nGeneration,” arXiv preprint arXiv:2203.16714. \\n[17] Ge, J., Sun, S., Owens, J., Galvez, V., Gologorskaya, O., Lai, J. C., ... & Lai, K. (2023). “Development of a Liver Disease-Specific Large \\nLanguage Model Chat Interface using Retrieval Augmented Generation,” medRxiv. \\n[18] Zakka, C., Shad, R., Chaurasia, A., Dalal, A. R., Kim, J. L., Moor, M., ... & Hiesinger, W. (2024). “Almanac—retrieval-augmented language \\nmodels for clinical medicine,” NEJM AI 1(2), AIoa2300068. \\n[19] Han, Z. FeiFei, Lin, J., Gurung, A., Thomas, D. R., Chen, E., Borchers, C., Gupta, S., & Koedinger, K. R. (2024). “Improving Assessment of \\nTutoring Practices using Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.14594. \\n[20] Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A., & Jamal, A. (2024). “Enhancing Textbook Question Answering Task with'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content=\"[20] Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A., & Jamal, A. (2024). “Enhancing Textbook Question Answering Task with \\nLarge Language Models and Retrieval Augmented Generation,” arXiv preprint arXiv:2402.05128. \\n[21] Bucur, M. (2023). “Exploring Large Language Models and Retrieval Augmented Generation for Automated Form Filling,” (Bachelor's \\nthesis, University of Twente). \\n[22] Zhang, B., Yang, H., Zhou, T., Ali Babar, M., & Liu, X. Y. (2023). “Enhancing financial sentiment analysis via retrieval augmented large \\nlanguage models,” In Proceedings of the Fourth ACM International Conference on AI in Finance: 349-356. \\n[23] Al Ghadban, Y., Lu, H. Y., Adavi, U., Sharma, A., Gara, S., Das, N., ... & Hirst, J. E. (2023). “Transforming healthcare education: \\nHarnessing large language models for frontline health worker capacity building using retrieval-augmented generation,” medRxiv, 2023-12.\"),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content='Harnessing large language models for frontline health worker capacity building using retrieval-augmented generation,” medRxiv, 2023-12. \\n[24] Jeong, M., Sohn, J., Sung, M., & Kang, J. (2024). “Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-\\nAugmented Large Language Models,” arXiv preprint arXiv:2401.15269. \\n[25] Xia, M., Zhang, X., Couturier, C., Zheng, G., Rajmohan, S., & Ruhle, V. (2023). “Hybrid retrieval-augmented generation for real-time \\ncomposition assistance,” arXiv preprint arXiv:2308.04215. \\n[26] Rackauckas, Z. (2024). “RAG-Fusion: A New Take on Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.03367. \\n[27] Shi, E., Wang, Y., Tao, W., Du, L., Zhang, H., Han, S., ... & Sun, H. (2022). “RACE: Retrieval-Augmented Commit Message \\nGeneration,” arXiv preprint arXiv:2203.02700. \\n[28] Colverd, G., Darm, P., Silverberg, L., & Kasmanoff, N. (2023). “FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content='Generation,” arXiv preprint arXiv:2203.02700. \\n[28] Colverd, G., Darm, P., Silverberg, L., & Kasmanoff, N. (2023). “FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented \\nGeneration with an LLM,” arXiv preprint arXiv:2311.02597. \\n[29] Huang, W., Lapata, M., Vougiouklis, P., Papasarantopoulos, N., & Pan, J. (2023). “Retrieval Augmented Generation with Rich Answer \\nEncoding,” In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-\\nPacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers): 1012-1025. \\n[30] Chen, W., Hu, H., Saharia, C., & Cohen, W. W. (2022). “Re-imagen: Retrieval-augmented text-to-image generator,” arXiv preprint \\narXiv:2209.14491. \\n[31] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S. W., & Svyatkovskiy, A. (2022). “Reacc: A retrieval-augmented code completion \\nframework,” arXiv preprint arXiv:2203.07722.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 8}, page_content='arXiv:2209.14491. \\n[31] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S. W., & Svyatkovskiy, A. (2022). “Reacc: A retrieval-augmented code completion \\nframework,” arXiv preprint arXiv:2203.07722. \\n[32] Wen, Z., Tian, Z., Wu, W., Yang, Y., Shi, Y., Huang, Z., & Li, D. (2023). “Grove: a retrieval-augmented complex story generation \\nframework with a forest of evidence,” arXiv preprint arXiv:2310.05388.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='3790\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n[33] Li, S., Park, S., Lee, I., & Bastani, O. (2023). “TRAC: Trustworthy Retrieval Augmented Chatbot,” arXiv preprint arXiv:2307.04642. \\n[34] Lozano, A., Fleming, S. L., Chiang, C. C., & Shah, N. (2023). “Clinfo. ai: An open-source retrieval-augmented large language model system \\nfor answering medical questions using scientific literature,” In Pacific symposium on Biocomputing 2024: 8-23. \\n[35] Ding, W., Cao, Y., Zhao, D., Xiao, C., & Pavone, M. (2023). “RealGen: Retrieval Augmented Generation for Controllable Traffic \\nScenarios,” arXiv preprint arXiv:2312.13303. \\n[36] Thompson, W. E., Vidmar, D. M., De Freitas, J. K., Pfeifer, J. M., Fornwalt, B. K., Chen, R., ... & Miotto, R. (2023). “Large Language \\nModels with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,” arXiv preprint arXiv:2312.06457.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,” arXiv preprint arXiv:2312.06457. \\n[37] Wang, W., Wang, Y., Joty, S., & Hoi, S. C. (2023). “Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program \\nrepair,” In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software \\nEngineering: 146-158. \\n[38] Guo, Y., Li, Z., Jin, X., Liu, Y., Zeng, Y., Liu, W., ... & Cheng, X. (2023). “Retrieval-augmented code generation for universal information \\nextraction,” arXiv preprint arXiv:2311.02962. \\n[39] Kagaya, T., Yuan, T. J., Lou, Y., Karlekar, J., Pranata, S., Kinose, A., ... & You, Y. (2024). “RAP: Retrieval-Augmented Planning with \\nContextual Memory for Multimodal LLM Agents,” arXiv preprint arXiv:2402.03610. \\n[40] Fan, R. Z., Fan, Y., Chen, J., Guo, J., Zhang, R., & Cheng, X. (2023). “RIGHT: Retrieval-augmented Generation for Mainstream Hashtag'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='[40] Fan, R. Z., Fan, Y., Chen, J., Guo, J., Zhang, R., & Cheng, X. (2023). “RIGHT: Retrieval-augmented Generation for Mainstream Hashtag \\nRecommendation,” arXiv preprint arXiv:2312.10466. \\n[41] Jiang, S., Tang, W., Chen, X., Tanga, R., Wang, H., & Wang, W. (2023). Raucg: Retrieval-augmented unsupervised counter narrative \\ngeneration for hate speech. arXiv preprint arXiv:2310.05650. \\n[42] Xu, R., Yu, Y., Ho, J., & Yang, C. (2023). “Weakly-supervised scientific document classification via retrieval-augmented multi-stage \\ntraining,” In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval: 2501-\\n2505. \\n[43] Hu, M., Zhao, X., Wei, J., Wu, J., Sun, X., Li, Z., ... & Zhang, Y. (2023). “rT5: A Retrieval-Augmented Pre-trained Model for Ancient \\nChinese Entity Description Generation,” In International Conference on NLP and Chinese Computing. Cham: Springer: 736-748.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='Chinese Entity Description Generation,” In International Conference on NLP and Chinese Computing. Cham: Springer: 736-748. \\n[44] Song, S., Lv, Q., Geng, L., Cao, Z., & Fu, G. (2023). “RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling \\nCheck,” In CCF International Conference on Natural Language Processing and Chinese Computing. Cham: Springer: 551-562.  \\n[45] Shi, P., Zhang, R., Bai, H., & Lin, J. (2022). “Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql \\nsemantic parsing,” arXiv preprint arXiv:2210.13693. \\n[46] Asai, A., Wu, Z., Wang, Y., Sil, A., & Hajishirzi, H. (2023). “Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection,” arXiv preprint arXiv:2310.11511. \\n[47] Lin, D. (2024). “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,” arXiv preprint \\narXiv:2401.12599.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='reflection,” arXiv preprint arXiv:2310.11511. \\n[47] Lin, D. (2024). “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,” arXiv preprint \\narXiv:2401.12599. \\n[48] He, X., Tian, Y., Sun, Y., Chawla, N. V., Laurent, T., LeCun, Y., ... & Hooi, B. (2024). “G-Retriever: Retrieval-Augmented Generation for \\nTextual Graph Understanding and Question Answering,” arXiv preprint arXiv:2402.07630. \\n[49] Ahmad, S. R. (2024). “Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model \\nImplementation for Multicultural Enterprise,” arXiv preprint arXiv:2401.01511. \\n[50] Zhao, C., Jiang, Y., Qiu, Y., Zhang, H., & Yang, W. Y. (2023). “Differentiable Retrieval Augmentation via Generative Language Modeling \\nfor E-commerce Query Intent Classification,” In Proceedings of the 32nd ACM International Conference on Information and Knowledge \\nManagement: 4445-4449.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='for E-commerce Query Intent Classification,” In Proceedings of the 32nd ACM International Conference on Information and Knowledge \\nManagement: 4445-4449. \\n[51] Nakhod, o. Using retrieval-augmented generation to elevate low-code developer skills. https://doi.org/10.15407/jai2023.03.126 \\n[52] Wang, H., Huang, W., Deng, Y., Wang, R., Wang, Z., Wang, Y., ... & Wong, K. F. (2024). “UniMS-RAG: A Unified Multi-source \\nRetrieval-Augmented Generation for Personalized Dialogue Systems,” arXiv preprint arXiv:2401.13256. \\n[53] Du, X., & Ji, H. (2022). “Retrieval-augmented generative question answering for event argument extraction,” arXiv preprint \\narXiv:2211.07067. \\n[54] Ranade, P., & Joshi, A. (2023). “FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction,” arXiv \\npreprint arXiv:2310.13848. \\n[55] Zhang, Z., Fang, M., & Chen, L. (2024). “RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='preprint arXiv:2310.13848. \\n[55] Zhang, Z., Fang, M., & Chen, L. (2024). “RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain \\nQuestion Answering,” arXiv preprint arXiv:2402.16457. \\n[56] Zhang, S., Yadav, D., & Jin, T. (2023). “Cash transaction booking via retrieval augmented LLM. KDD 2023 Workshop on Robust NLP for \\nFinance (RobustFin),” https://www.amazon.science/publications/cash-transaction-booking-via-retrieval-augmented-llm \\n[57] Pouplin, T., Sun, H., Holt, S., & Van der Schaar, M. (2024). “Retrieval-Augmented Thought Process as Sequential Decision Making,” arXiv \\npreprint arXiv:2402.07812. \\n[58] Munikoti, S., Acharya, A., Wagle, S., & Horawalavithana, S. (2023). “ATLANTIC: Structure-Aware Retrieval-Augmented Language Model \\nfor Interdisciplinary Science,” arXiv preprint arXiv:2311.12289. \\n[59] Markey, N., El-Mansouri, I., Rensonnet, G., van Langen, C., & Meier, C. (2024). “From RAGs to riches: Using large language models to'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows)', 'creator': 'Adobe InDesign 18.0 (Windows)', 'creationdate': '2012-08-24T16:03:10+05:30', 'source': '../data/pdf/LLMs, NLP and RAG.pdf', 'file_path': '../data/pdf/LLMs, NLP and RAG.pdf', 'total_pages': 10, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2013-02-20T16:20:50+05:30', 'trapped': '', 'modDate': \"D:20130220162050+05'30'\", 'creationDate': \"D:20120824160310+05'30'\", 'page': 9}, page_content='[59] Markey, N., El-Mansouri, I., Rensonnet, G., van Langen, C., & Meier, C. (2024). “From RAGs to riches: Using large language models to \\nwrite documents for clinical trials,” arXiv preprint arXiv:2402.16406. \\n[60] Kim, J., & Min, M. (2024). “From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process,” arXiv \\npreprint arXiv:2402.01717.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'source': '../data/pdf/sample.pdf', 'file_path': '../data/pdf/sample.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'sample', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20080701052447Z00'00'\", 'trapped': '', 'modDate': \"D:20080701052447Z00'00'\", 'creationDate': \"D:20080701052447Z00'00'\", 'page': 0}, page_content='Sample PDF\\nThis is a simple PDF ﬁle. Fun fun fun.\\nLorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. \\nCurabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget \\npharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. \\nInteger a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. \\nVestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla \\nerat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. \\nVivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique \\naccumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, \\nhendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. \\nSuspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \\npulvinar quis, nisl.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'source': '../data/pdf/sample.pdf', 'file_path': '../data/pdf/sample.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'sample', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20080701052447Z00'00'\", 'trapped': '', 'modDate': \"D:20080701052447Z00'00'\", 'creationDate': \"D:20080701052447Z00'00'\", 'page': 0}, page_content='Suspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \\npulvinar quis, nisl.\\nPellentesque sit amet lectus. Praesent pulvinar, nunc quis iaculis sagittis, justo quam \\nlobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. \\nPhasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh \\nante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis \\nsem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin \\nvelit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, \\nfeugiat eu, gravida eu, consequat molestie, ipsum. Nullam vel est ut ipsum volutpat \\nfeugiat. Aenean pellentesque.\\nIn mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, \\nscelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'source': '../data/pdf/sample.pdf', 'file_path': '../data/pdf/sample.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'sample', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20080701052447Z00'00'\", 'trapped': '', 'modDate': \"D:20080701052447Z00'00'\", 'creationDate': \"D:20080701052447Z00'00'\", 'page': 0}, page_content='In mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, \\nscelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat \\nvarius. Donec lacinia, neque a luctus aliquet, pede massa imperdiet ante, at varius lorem \\npede sed sapien. Fusce erat nibh, aliquet in, eleifend eget, commodo eget, erat. Fusce \\nconsectetuer. Cras risus tortor, porttitor nec, tristique sed, convallis semper, eros. Fusce \\nvulputate ipsum a mauris. Phasellus mollis. Curabitur sed urna. Aliquam nec sapien non \\nnibh pulvinar convallis. Vivamus facilisis augue quis quam. Proin cursus aliquet metus. \\nSuspendisse lacinia. Nulla at tellus ac turpis eleifend scelerisque. Maecenas a pede vitae \\nenim commodo interdum. Donec odio. Sed sollicitudin dui vitae justo.\\nMorbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus \\nmolestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'source': '../data/pdf/sample.pdf', 'file_path': '../data/pdf/sample.pdf', 'total_pages': 1, 'format': 'PDF 1.3', 'title': 'sample', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20080701052447Z00'00'\", 'trapped': '', 'modDate': \"D:20080701052447Z00'00'\", 'creationDate': \"D:20080701052447Z00'00'\", 'page': 0}, page_content='Morbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus \\nmolestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque \\npenatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit \\namet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa \\neget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, \\nquam.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Towards Agentic RAG with Deep Reasoning:\\nA Survey of RAG-Reasoning Systems in LLMs\\nYangning Li1*, Weizhi Zhang2*, Yuyao Yang2, Wei-Chieh Huang2, Yaozu Wu3\\nJunyu Luo4, Yuanchen Bei5, Henry Peng Zou2, Xiao Luo6, Yusheng Zhao4\\nChunkit Chan7, Yankai Chen2, Zhongfen Deng2, Yinghui Li1, Hai-Tao Zheng1,\\nDongyuan Li3, Renhe Jiang3, Ming Zhang4, Yangqiu Song7, Philip S. Yu1\\n1Tsinghua University 2University of Illinois Chicago 3The University of Tokyo\\n4Peking University 5University of Illinois Urbana-Champaign\\n6University of California, Los Angeles 7HKUST\\nynli23@mails.tsinghua.edu.cn, wzhan42@uic.edu\\nAbstract\\nRetrieval-Augmented Generation (RAG) lifts\\nthe factuality of Large Language Models\\n(LLMs) by injecting external knowledge, yet\\nit falls short on problems that demand multi-\\nstep inference; conversely, purely reasoning-\\noriented approaches often hallucinate or mis-\\nground facts. This survey synthesizes both\\nstrands under a unified reasoning-retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='step inference; conversely, purely reasoning-\\noriented approaches often hallucinate or mis-\\nground facts. This survey synthesizes both\\nstrands under a unified reasoning-retrieval per-\\nspective. We first map how advanced reason-\\ning optimizes each stage of RAG (Reasoning-\\nEnhanced RAG). Then, we show how re-\\ntrieved knowledge of different type supply\\nmissing premises and expand context for\\ncomplex inference (RAG-Enhanced Reason-\\ning).\\nFinally, we spotlight emerging Syn-\\nergized RAG-Reasoning frameworks, where\\n(agentic) LLMs iteratively interleave search\\nand reasoning to achieve state-of-the-art per-\\nformance across knowledge-intensive bench-\\nmarks.\\nWe categorize methods, datasets,\\nand open challenges, and outline research av-\\nenues toward deeper RAG-Reasoning systems\\nthat are more effective, multimodally-adaptive,\\ntrustworthy, and human-centric. The collec-\\ntion is available at https://github.com/\\nDavidZWZ/Awesome-RAG-Reasoning.\\n1\\nIntroduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='that are more effective, multimodally-adaptive,\\ntrustworthy, and human-centric. The collec-\\ntion is available at https://github.com/\\nDavidZWZ/Awesome-RAG-Reasoning.\\n1\\nIntroduction\\nThe remarkable progress in Large Language Mod-\\nels (LLMs) has transformed a wide array of fields,\\nshowcasing unprecedented capabilities across di-\\nverse tasks (Zhao et al., 2023). Despite these ad-\\nvancements, the effectiveness of LLMs remains\\nhindered by two fundamental limitations: knowl-\\nedge hallucinations, due to the static and parametric\\nmanner of their knowledge storage (Huang et al.,\\n2025b); and struggles with complex reasoning, es-\\npecially when tackling real-world problems (Chang\\net al., 2024). These limitations have driven the\\n* Equal Contribution.\\ndevelopment of two major directions: Retrieval-\\nAugmented Generation (RAG) (Fan et al., 2024a),\\nwhich provides LLMs with external knowledge;\\nand various methods aimed at enhancing their in-\\nherent reasoning abilities (Chen et al., 2025c).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='Augmented Generation (RAG) (Fan et al., 2024a),\\nwhich provides LLMs with external knowledge;\\nand various methods aimed at enhancing their in-\\nherent reasoning abilities (Chen et al., 2025c).\\nThe two limitations are inherently intertwined:\\nmissing knowledge can impede reasoning, and\\nflawed reasoning hinders knowledge utilization\\n(Tonmoy et al., 2024). Naturally, researchers have\\nincreasingly explored combining retrieval with rea-\\nsoning, though early work followed two separate,\\none-way enhancements.\\nThe first, Reasoning-\\nenhanced RAG (Gao et al., 2023b) (Reasoning\\n→RAG), leverages reasoning to improve specific\\nstages of the RAG pipeline. The second path, RAG-\\nenhanced Reasoning (Fan et al., 2024a) (RAG →\\nReasoning), supplies external factual grounding or\\ncontextual cues to bolster LLM reasoning.\\nWhile beneficial, the above methods remain\\nbound to a static Retrieval-Then-Reasoning (RTR)\\nframework, offering only localized improvements\\nto individual components. Several inherent limi-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='While beneficial, the above methods remain\\nbound to a static Retrieval-Then-Reasoning (RTR)\\nframework, offering only localized improvements\\nto individual components. Several inherent limi-\\ntations persist: (1) Retrieval Adequacy and Accu-\\nracy cannot be guaranteed; Pre-retrieved knowl-\\nedge may fail to align with the actual knowledge\\nneeds that emerge during reasoning, especially in\\ncomplex tasks (Zheng et al., 2025; Li et al., 2025d).\\n(2) Reasoning Depth remains constrained. When\\nretrieved knowledge contains errors or conflicts,\\nit can adversely interfere with the model’s inher-\\nent reasoning capabilities (Li et al., 2025b; Chen\\net al., 2025a). (3) System Adaptability proves in-\\nsufficient. The RTR framework lacks mechanisms\\nfor iterative feedback or dynamic retrieval during\\nreasoning. This rigidity limits its effectiveness in\\nscenarios that require adaptive reasoning, such as\\nopen-domain QA or scientific discovery (Xiong\\net al., 2025; Alzubi et al., 2025).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='reasoning. This rigidity limits its effectiveness in\\nscenarios that require adaptive reasoning, such as\\nopen-domain QA or scientific discovery (Xiong\\net al., 2025; Alzubi et al., 2025).\\nAs shown in Figure 1, these shortcomings have\\ncatalyzed a paradigm shift toward Synergized Re-\\n1\\narXiv:2507.09477v2  [cs.CL]  16 Jul 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Query\\nKnowledge\\nSource\\nDense/Sparse Retriever\\nRetrieval\\nIntegration\\nRe-rank \\n& Filter\\nData Fusion\\nGeneration\\nRetrieval Optimization\\n§3 Reasoning Enhanced RAG (Reasoning → RAG) \\nReasoning Enhanced\\n§4 RAG Enhanced Reasoning (RAG → Reasoning) \\nKnowledge Base\\nWeb Retrieval\\nTool Using\\nPrior Experience\\nExample & Training Data\\n§5 Synergized RAG and Reasoning (RAG ⇔ Reasoning) \\n§5.1 Reasoning Workflow\\n(a) Chain Based\\n(b) Tree Based\\n(c) Graph Based\\n§5.2 Agent Orchestration\\n(a) Single Agent\\nRAG Pipeline\\nLLM Reasoning\\nKnowledge 1: he is a South African \\npilot, sailor, consultant, who partly …\\nLet‘s Think Step by Step! …….\\nFind the solution!\\nRAG Enhanced\\n🎯Target: Accurate, and \\nReasoning-aware Retrieval\\n• Retrieve info tailored for \\nreasoning\\n• Filter and fuse evidence \\nlogically\\n• Faithful, logic-grounded output\\n🎯Target: Deeper, and \\nGrounded Reasoning\\n• Grounded by external \\nknowledge\\n• Verified by tools and evidence\\n• Guided by memory or \\nexamples\\n☹Drawback: \\nIrrelevant Knowledge'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='🎯Target: Deeper, and \\nGrounded Reasoning\\n• Grounded by external \\nknowledge\\n• Verified by tools and evidence\\n• Guided by memory or \\nexamples\\n☹Drawback: \\nIrrelevant Knowledge \\ndisrupts reasoning accuracy.\\nQuery\\n❌\\nOutput\\n…\\nAction\\nObservation\\nAgent\\nRAG\\n(b) Decentralized \\nMulti Agent\\n(c) Centralized \\nMulti Agent\\nReasoning\\nRAG\\nQuery\\n❌\\n❓\\n✅\\nIntegration Enhancement\\nGeneration\\nEnhancement\\n☹Drawback: \\nShallow Reasoning leads to \\ninaccurate retrieval.\\nAgent 1\\nAgent 2\\nAgent 3\\nAgent 4\\nRAG\\nManager\\nAgent\\nWork Agent 1\\nWork Agent 2\\nWork Agent 3\\nRAG\\nQuery\\nOutput\\n…\\nFigure 1: Overview of the RAG-Reasoning System. The Reasoning-Enhanced RAG methods and RAG-Enhanced\\nReasoning methods represent one-way enhancements. In contrast, the Synergized RAG-Reasoning System performs\\nreasoning and retrieval iteratively, enabling mutual enhancements.\\ntrieval and Reasoning within LLMs (RAG ⇔\\nReasoning). These methods support a dynamic,\\niterative interplay where reasoning actively guides'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='trieval and Reasoning within LLMs (RAG ⇔\\nReasoning). These methods support a dynamic,\\niterative interplay where reasoning actively guides\\nretrieval, and newly retrieved knowledge, in turn,\\ncontinuously refines the reasoning process. This\\ntrend is further exemplified by recent ”Deep Re-\\nsearch” products from OpenAI1, Gemini2, Perplex-\\nity3, and others, which emphasize tightly coupled\\nretrieval and reasoning (Zhang et al., 2025f). These\\nsystems employ agentic capabilities to orchestrate\\nmulti-step web search and leverage reasoning to\\ncomprehensively interpret retrieved content, solv-\\ning problems demanding in-depth investigation.\\nThis survey charts the shift from isolated en-\\nhancements to cutting-edge synergized frameworks\\nwhere retrieval and reasoning are deeply interwo-\\nven and co-evolve. While surveys on RAG (Fan\\net al., 2024a; Gao et al., 2023b) and LLM Reason-\\ning (Chen et al., 2025c; Li et al., 2025e) exist, a\\ndedicated synthesis focusing on their integration'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='ven and co-evolve. While surveys on RAG (Fan\\net al., 2024a; Gao et al., 2023b) and LLM Reason-\\ning (Chen et al., 2025c; Li et al., 2025e) exist, a\\ndedicated synthesis focusing on their integration\\nremains lacking. Our goal is to provide a compre-\\nhensive overview of how the symbiosis between\\nretrieval and reasoning is advancing LLM capabili-\\nties, with particular emphasis on the move towards\\na synergized RAG and Reasoning framework.\\nThe survey is structured as follows: Section 2\\nintroduces the background; Section 3 and 4 review\\ntwo one-way enhancements, respectively. Section 5\\n1https://openai.com/index/\\nintroducing-deep-research/\\n2https://gemini.google/overview/\\ndeep-research/\\n3https://www.perplexity.ai/hub/blog/\\nintroducing-perplexity-deep-research\\nunifies both lines into synergized RAG–Reasoning\\nframeworks. Section 6 lists benchmarks, and Sec-\\ntion 7 outlines open challenges.\\n2\\nBackground and Preliminary\\nRAG mitigates knowledge cut-off of LLMs through'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='frameworks. Section 6 lists benchmarks, and Sec-\\ntion 7 outlines open challenges.\\n2\\nBackground and Preliminary\\nRAG mitigates knowledge cut-off of LLMs through\\nthree sequential stages: (i) Retrieval, fetching task-\\nrelevant content from external knowledge stores;\\n(ii) Integration, deduplicating, resolving conflicts,\\nand re-ranking the retrieved content; and (iii) Gen-\\neration, reasoning over the curated context to pro-\\nduce the final answer.\\nConcurrently, Chain-of-\\nThought technique has significantly enhanced the\\nreasoning capabilities of modern LLMs by encour-\\naging them to “think step by step” before answering.\\nThe synergy between the structured RAG pipeline\\nand these multi-step reasoning capacities grounds\\nthe emerging RAG-Reasoning paradigm explored\\nin this survey.\\n3\\nReasoning-Enhanced RAG\\nTraditional RAG methods first retrieve relevant doc-\\numents, then concatenate the retrieved knowledge\\nwith the original query to generate the final answer.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='in this survey.\\n3\\nReasoning-Enhanced RAG\\nTraditional RAG methods first retrieve relevant doc-\\numents, then concatenate the retrieved knowledge\\nwith the original query to generate the final answer.\\nThese methods often fail to capture the deeper con-\\ntext or intricate relationships necessary for complex\\nreasoning tasks. By integrating reasoning capabili-\\nties across Retrieval, Integration, and Generation\\nstages of the RAG pipeline, the system can identify\\nand fetch the most relevant information, reducing\\nhallucinations and improving response accuracy.4\\n4If reasoning only serves to better leverage fixed retrieved\\nknowledge in a unidirectional manner, it is considered within\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='A survey of RAG and Reasoning\\nReasoning-enhanced\\nRAG (§3)\\nRetrieval\\nOptimization (§3.1)\\nReasoning-Aware Query Reformulation (§3.1.1)\\ne.g. Collab-RAG (Xu et al., 2025b), DynQR (Anonymous, 2025), DeepRetrieval (Jiang et al., 2025)\\nRetrieval Strategy and Planning (§3.1.2)\\ne.g. PAR-RAG (Zhang et al., 2025d), LPKG (Wang et al., 2024b), FIND (Jia et al., 2025)\\nRetrieval Model Enhancement (§3.1.3)\\ne.g. GNN-RAG (Mavromatis and Karypis, 2024), RuleRAG (Chen et al., 2024c),\\nIntegration\\nEnhancement (§3.2)\\nRelevance Assessment & Filtering (§3.2.1)\\ne.g. SEER (Zhao et al., 2024c), M-RAG-R (Yoran et al., 2024)\\nInformation Synthesis & Fusion (§3.2.2)\\ne.g. BeamAggR (Chu et al., 2024), DualRAG (Cheng et al., 2025) , CRP-RAG (Xu et al., 2024)\\nGeneration\\nEnhancement (§3.3)\\nContext-Aware Generation (§3.3.1)\\ne.g. Open-RAG (Islam et al., 2024), RARE (Wang et al., 2025d), Self-Reasoning (Xia et al., 2025b)\\nGrounded Generation Control (§3.3.2)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Generation\\nEnhancement (§3.3)\\nContext-Aware Generation (§3.3.1)\\ne.g. Open-RAG (Islam et al., 2024), RARE (Wang et al., 2025d), Self-Reasoning (Xia et al., 2025b)\\nGrounded Generation Control (§3.3.2)\\ne.g. RARR (Gao et al., 2023a), TRACE (Fang et al., 2024), AlignRAG (Wei et al., 2025b)\\nRAG-enhanced\\nReasoning (§4)\\nExternal Knowledge\\nRetrieval (§4.1)\\nKnowledge Base (§4.1.1)\\ne.g. Premise-Retrieval (Tao et al., 2025), ReaRAG (Lee et al., 2025), CBR-RAG (Wiratunga et al., 2024)\\nWeb Retrieval (§4.1.2)\\ne.g. ALR2 (Li et al., 2024d) , RARE (Tran et al., 2024), Open-RAG (Islam et al., 2024)\\nTool Using (§4.1.3)\\ne.g. TATU (Li et al., 2024g), TRICE(Qiao et al., 2024), Re-Invoke (Chen et al., 2024a)\\nIn-Context\\nRetrieval (§4.2)\\nPrior Experience (§4.2.1)\\ne.g. RAP (Kagaya et al., 2024), JARVIS-1 (Wang et al., 2024f), EM-LLM (Fountas et al., 2024)\\nExample or Training Data (§4.2.2)\\ne.g. MoD (Wang et al., 2024c), RE4 (Li et al., 2024c), UPRISE (Cheng et al., 2023)\\nSynergized RAG-\\nReasoning (§5)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Example or Training Data (§4.2.2)\\ne.g. MoD (Wang et al., 2024c), RE4 (Li et al., 2024c), UPRISE (Cheng et al., 2023)\\nSynergized RAG-\\nReasoning (§5)\\nReasoning Workflow\\n(§5.1)\\nChain-based (§5.1.1)\\ne.g. IRCoT (Trivedi et al., 2023), Rat (Wang et al., 2024g), CoV-RAG (He et al., 2024a), RAFT (Zhang et al., 2024a)\\nTree-based\\n(§5.1.2)\\nToT e.g. RATT (Zhang et al., 2025a), Tree of Clarifications (Kim et al., 2023), GROVE (Wen et al., 2023),\\nMCTS e.g. AirRAG (Feng et al., 2025), MCTS-RAG (Hu et al., 2025), SeRTS (Hu et al., 2024)\\nGraph-based\\n(§5.1.3)\\nWalk-on-Graph: e.g. QA-GNN (Yasunaga et al., 2021), LightRAG (Guo et al., 2024), StructRAG (Li et al., 2024h)\\nThink-on-Graph: e.g. ToG (Sun et al., 2024b), ToG-2.0 (Ma et al., 2024a), Graph-CoT (Jin et al., 2024),\\nAgent Orchestration\\n(§5.2)\\nSignle-Agent\\n(§ 5.2.1)\\nPrompting: e.g. ReAct (Yao et al., 2023b), Search-O1 (Li et al., 2025b); SFT: e.g. Toolformer (Schick et al., 2023),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Agent Orchestration\\n(§5.2)\\nSignle-Agent\\n(§ 5.2.1)\\nPrompting: e.g. ReAct (Yao et al., 2023b), Search-O1 (Li et al., 2025b); SFT: e.g. Toolformer (Schick et al., 2023),\\nINTERS (Zhu et al., 2024); RL: e.g. Search-R1 (Jin et al., 2025) R1-Searcher (Song et al., 2025)\\nMulti-Agent\\n(§ 5.2.2)\\nDecentralized: e.g. M-RAG (Wang et al., 2024e), MDocAgent (Han et al., 2025), Agentic reasoning (Wu et al., 2025c)\\nCentralized: e.g. HM-RAG (Liu et al., 2025), SurgRAW (Low et al., 2025), Chain of Agents (Zhang et al., 2024c)\\nFigure 2: Taxonomy of Recent Advances in RAG-Reasoning System.\\n3.1\\nRetrieval Optimization\\nRetrieval optimization leverages reasoning to im-\\nprove result relevance and quality. Existing meth-\\nods are broadly categorized (1) Reasoning-Aware\\nQuery Reformulation, (2) Retrieval Strategy and\\nPlanning, and (3) Retrieval Model Enhancement.\\n3.1.1\\nReasoning-Aware Query Reformulation\\nIt reformulates the original query to better retrieve\\nreasoning-relevant context. First, query decompo-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Planning, and (3) Retrieval Model Enhancement.\\n3.1.1\\nReasoning-Aware Query Reformulation\\nIt reformulates the original query to better retrieve\\nreasoning-relevant context. First, query decompo-\\nsition breaks down complex queries into simpler\\nsub-queries (Xu et al., 2025b). Second, query re-\\nformulation recasts ambiguous queries into more\\nclear ones.\\nTo align with reasoning needs of\\ngenerator, certain works train rewrites with RL\\nsignals (Anonymous, 2025; Wang et al., 2025c).\\nThird, query expansion enrich the semantic rich-\\nness of the query via CoT reasoning (Dhuliawala\\net al., 2024; Li et al., 2024e; Lee et al., 2024).\\n3.1.2\\nRetrieval Strategy and Planning\\nThis section covers global retrieval guidance. Ad-\\nvance planning uses a reasoning model to gener-\\nate a complete retrieval blueprint prior to execu-\\ntion. PAR-RAG (Zhang et al., 2025d) applies CoT\\nfor multi-step planning, mitigating local optima.\\nLPKG (Wang et al., 2024b) fine-tunes LLMs on'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='ate a complete retrieval blueprint prior to execu-\\ntion. PAR-RAG (Zhang et al., 2025d) applies CoT\\nfor multi-step planning, mitigating local optima.\\nLPKG (Wang et al., 2024b) fine-tunes LLMs on\\nknowledge graphs to encode relational structure. In\\ncontrast, adaptive retrieval decision methods make\\na one-step prediction on whether and how to re-\\ntrieve. FIND (Jia et al., 2025) and adaptive RAG\\n§3.3. In contrast, if reasoning dynamically triggers new\\nretrieval, it is discussed in §5.\\n(Jeong et al., 2024) use classifiers to assess query\\ncomplexity and select retrieval strategies, reducing\\nunnecessary calls. Marina et al. (2025) further adds\\nfeatures like entity popularity and question type.\\n3.1.3\\nRetrieval Model Enhancement\\nA line of work enhances retrievers with reason-\\ning via two strategies.\\nThe first one leverages\\nstructured knowledge: GNN-RAG (Mavromatis\\nand Karypis, 2024) encodes knowledge graphs\\nwith GNNs for implicit multi-hop reasoning, while'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='ing via two strategies.\\nThe first one leverages\\nstructured knowledge: GNN-RAG (Mavromatis\\nand Karypis, 2024) encodes knowledge graphs\\nwith GNNs for implicit multi-hop reasoning, while\\nRuleRAG (Chen et al., 2024c) appends symbolic\\nrules to guide retrieval toward logical consistency.\\nAnother strategy integrates explicit reasoning: Ji\\net al. (2024) combines CoT with the query to im-\\nprove intermediate knowledge recall in multi-hop\\nQA.\\n3.2\\nIntegration Enhancement\\nIntegration enhancement uses reasoning to assess\\nrelevance and merge heterogeneous evidence, pre-\\nventing irrelevant content from disrupting gener-\\nation. Methods fall into two categories: (1) rele-\\nvance assessment and (2) information synthesis.\\n3.2.1\\nRelevance Assessment & Filtering\\nThese methods assess the relevance of each re-\\ntrieved fragment to the user query through deeper\\nreasoning. SEER (Zhao et al., 2024c) employs\\nassessor experts to select faithful, helpful, and con-\\ncise evidence while discarding irrelevant content.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='reasoning. SEER (Zhao et al., 2024c) employs\\nassessor experts to select faithful, helpful, and con-\\ncise evidence while discarding irrelevant content.\\nYoran et al. (2024) improves robustness by filtering\\nnon-entailing passages using an NLI model, then\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='fine-tuning the LLM on mixed relevant/irrelevant\\ncontexts to help it ignore residual noise.\\n3.2.2\\nInformation Synthesis & Fusion\\nOnce relevant snippets are identified, the challenge\\nis to fuse them into a coherent evidence set. Beam-\\nAggR (Chu et al., 2024) enumerates sub-question\\nanswer combinations and aggregates them via prob-\\nabilistic reasoning. DualRAG (Cheng et al., 2025)\\ncombines reasoning-augmented querying with pro-\\ngressive knowledge aggregation to filter and or-\\nganize retrieved information into an evolving out-\\nline. CRP-RAG (Xu et al., 2024) builds a rea-\\nsoning graph to retrieve, evaluate, and aggregate\\nknowledge at each node, dynamically selecting\\nknowledge-sufficiency paths before generation.\\n3.3\\nGeneration Enhancement\\nEven with retrieved context, traditional RAG may\\nstill generate unfaithful content without reasoning.\\nReasoning during generation addresses this issue\\nthrough two main approaches: (1) context-aware\\nsynthesis and (2) grounded generation control.\\n3.3.1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Reasoning during generation addresses this issue\\nthrough two main approaches: (1) context-aware\\nsynthesis and (2) grounded generation control.\\n3.3.1\\nContext-Aware Synthesis Strategies\\nContext-aware generation ensures outputs remain\\nrelevance while reducing noise. Selective–context\\nutilization prunes or re-weights content based on\\ntask relevance. Open-RAG (Islam et al., 2024)\\nuses a sparse expert mixture to dynamically select\\nknowledge modules, while RARE (Wang et al.,\\n2025d) adds domain knowledge to prompts to pro-\\nmote reliance on external context over memoriza-\\ntion. Reasoning path generation builds explicit log-\\nical chains to enhance transparency, e.g., Ranaldi\\net al. (2024) generate contrasting explanations by\\ncomparing paragraph relevance step-by-step, guid-\\ning the model toward accurate conclusions. Self-\\nReasoning (Xia et al., 2025b) constructs structured\\nreasoning chains through sequential evidence se-\\nlection and verification.\\n3.3.2\\nGrounded Generation Control'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Reasoning (Xia et al., 2025b) constructs structured\\nreasoning chains through sequential evidence se-\\nlection and verification.\\n3.3.2\\nGrounded Generation Control\\nGrounded generation control introduces verifica-\\ntion mechanisms to ensure outputs remain an-\\nchored to retrieved evidence through reasoning.\\nFact verification methods use reasoning to assess\\nfactual consistency between generated content and\\nretrieved evidence, e.g., Self-RAG (Asai et al.,\\n2023) introduces reflection markers during decod-\\ning to trigger critical review and correction. Cita-\\ntion generation links generated content to source\\nmaterials to enhance traceability and credibility, as\\nin RARR (Gao et al., 2023a), which inserts cita-\\ntions while preserving stylistic coherence. Faith-\\nful reasoning ensures that each reasoning step ad-\\nheres to retrieved evidence without introducing\\nunverified content. TRACE (Fang et al., 2024)\\nbuilds knowledge graphs to form coherent evidence'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='ful reasoning ensures that each reasoning step ad-\\nheres to retrieved evidence without introducing\\nunverified content. TRACE (Fang et al., 2024)\\nbuilds knowledge graphs to form coherent evidence\\nchains, while AlignRAG (Wei et al., 2025b) applies\\ncriticism alignment to refine reasoning paths.\\n4\\nRAG-Enhanced Reasoning\\nIntegrating external knowledge or in-context\\nknowledge during reasoning can help LLMs reduce\\nhallucinations and bridge logical gaps. External re-\\ntrieval leverages structured sources like databases\\nor web content, providing factual grounding, like\\nIAG (Zhang et al., 2023). In-context retrieval uti-\\nlizes internal contexts like prior interactions or\\ntraining examples, enhancing contextual coherence,\\nlike RA-DT (Schmied et al., 2024). Both strategies\\ncollectively improve factual accuracy, interpretabil-\\nity, and logical consistency of reasoning processes.\\n4.1\\nExternal Knowledge Retrieval\\nExternal knowledge retrieval incorporates web'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='collectively improve factual accuracy, interpretabil-\\nity, and logical consistency of reasoning processes.\\n4.1\\nExternal Knowledge Retrieval\\nExternal knowledge retrieval incorporates web\\ncontent, database information, or external tools\\ninto reasoning, effectively filling knowledge gaps.\\nTargeted retrieval improves factual accuracy, en-\\nabling language models to reliably address complex\\nqueries by grounding reasoning steps in verified\\nexternal evidence.\\n4.1.1\\nKnowledge Base\\nKnowledge base (KB) typically stores arithmetic,\\ncommonsense, or logical knowledge in databases,\\nbooks, or documents, with retrieval approaches\\nvarying by task. For question answering (QA) rea-\\nsoning, AlignRAG (Wei et al., 2025b), MultiHop-\\nRAG (Tang and Yang, 2024), and CRP-RAG (Xu\\net al., 2025a) retrieve interconnected factual entries\\nfrom general KBs to enhance sequential reasoning.\\nIn specialized reasoning tasks, mathematical ap-\\nproaches like Premise-Retrieval (Tao et al., 2025)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='from general KBs to enhance sequential reasoning.\\nIn specialized reasoning tasks, mathematical ap-\\nproaches like Premise-Retrieval (Tao et al., 2025)\\nand ReaRAG (Lee et al., 2025) utilize formal lem-\\nmas from theorem libraries for structured deduc-\\ntion; legal approaches like CASEGPT (Yang, 2024)\\nand CBR-RAG (Wiratunga et al., 2024) extract\\njudicial precedents for analogical reasoning. For\\ncode generation tasks, CodeRAG (Li et al., 2025a)\\nand Koziolek et al. (2024) access code snippets\\nfrom repositories, ensuring syntactic correctness.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='4.1.2\\nWeb Retrieval\\nWeb retrieval accesses dynamic online content like\\nweb pages, news or social media. Specifically,\\nin fact-checking tasks, approaches such as Ver-\\naCT Scan (Niu et al., 2024), Ragar (Khaliq et al.,\\n2024), PACAR (Zhao et al., 2024b), and STEEL\\n(Li et al., 2024b) verify claims step-by-step using\\nevidence from news or social media, enhancing log-\\nical reasoning. Meanwhile, QA-based reasoning\\nlike RARE (Tran et al., 2024), RAG-Star (Jiang\\net al., 2024), MindSearch (Chen et al., 2024b),\\nand OPEN-RAG (Islam et al., 2024) iteratively\\nrefine reasoning with broad web content, align-\\ning with current trends in agentic search, which\\ninvolve synthesizing complex online materials to\\nenhance context-aware and robust reasoning. Con-\\nversely, in specialized areas like medical reasoning,\\napproaches such as FRVA (Fan et al., 2024b) and\\nALR2 (Li et al., 2024d), retrieve literature for accu-\\nrate diagnostics.\\n4.1.3\\nTool Using\\nTool-using approaches leverage external resources'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='approaches such as FRVA (Fan et al., 2024b) and\\nALR2 (Li et al., 2024d), retrieve literature for accu-\\nrate diagnostics.\\n4.1.3\\nTool Using\\nTool-using approaches leverage external resources\\nlike calculators, libraries, or APIs to enhance rea-\\nsoning interactively. In QA-based reasoning, Re-\\nInvoke (Chen et al., 2024a), AVATAR (Wu et al.,\\n2024), ToolkenGPT (Hao et al., 2023), and Tool-\\nLLM (Qin et al., 2023) invoke calculators or APIs\\n(e.g., Yahoo Finance, Wikidata), improving numer-\\nical accuracy and factual precision. Within the con-\\ntext of scientific modeling, SCIAGENT (Ma et al.,\\n2024b) and TRICE (Qiao et al., 2024) integrate\\nsymbolic computation tools (e.g., WolframAlpha),\\nstrengthening computational robustness. Similarly,\\nin mathematical computation, llm-tool-use (Luo\\net al., 2025b) autonomously employs calculators\\nfor accurate numerical reasoning. Distinctively in\\ncode generation tasks, RAR (Dutta et al., 2024)\\nretrieves code documentation via OSCAT libraries,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='et al., 2025b) autonomously employs calculators\\nfor accurate numerical reasoning. Distinctively in\\ncode generation tasks, RAR (Dutta et al., 2024)\\nretrieves code documentation via OSCAT libraries,\\nensuring syntactic accuracy and executable logic.\\n4.2\\nIn-context Retrieval\\nIn-context retrieval leverages a model’s internal\\nexperiences or retrieved examples from demonstra-\\ntions and training data to guide reasoning. This\\nretrieval provides relevant exemplars, guiding mod-\\nels to emulate reasoning patterns and enhancing\\naccuracy and logical coherence in novel questions.\\n4.2.1\\nPrior Experience\\nPrior experience refers to past interactions or suc-\\ncessful strategies stored in a model’s internal mem-\\nory, with retrieval varying by task. In tasks in-\\nvolving planning and decision-making tasks such\\nas robot path finding, RAHL (Sun et al., 2024a)\\nand RA-DT (Schmied et al., 2024) leverage past\\ndecisions and reinforcement signals for sequential\\nreasoning. For interactive reasoning tasks, JARVIS-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='as robot path finding, RAHL (Sun et al., 2024a)\\nand RA-DT (Schmied et al., 2024) leverage past\\ndecisions and reinforcement signals for sequential\\nreasoning. For interactive reasoning tasks, JARVIS-\\n1 (Wang et al., 2024f), RAP (Kagaya et al., 2024),\\nand EM-LLM (Fountas et al., 2024) dynamically\\nrecall multimodal interactions and conversational\\nhistories, facilitating adaptive reasoning for person-\\nalized interactions. In the domain for logical rea-\\nsoning, CoPS (Yang et al., 2024a) retrieves struc-\\ntured prior cases like medical and legal cases for\\nrobust logical reasoning in medical and legal sce-\\nnarios.\\n4.2.2\\nExample or Training Data\\nUnlike approaches relying on prior experiences,\\nexample-based reasoning retrieves external exam-\\nples from demonstrations or training data. For ex-\\nample, In complex text-understanding, RE4 (Li\\net al., 2024c) and Fei et al. (2024) utilize anno-\\ntated sentence pairs to enhance relation recogni-\\ntion. Addressing QA-based reasoning, OpenRAG'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='ample, In complex text-understanding, RE4 (Li\\net al., 2024c) and Fei et al. (2024) utilize anno-\\ntated sentence pairs to enhance relation recogni-\\ntion. Addressing QA-based reasoning, OpenRAG\\n(Zhou and Chen, 2025), UPRISE (Cheng et al.,\\n2023), MoD (Wang et al., 2024c), and Dr.ICL (Luo\\net al., 2023) select demonstrations closely match-\\ning queries, improving generalization. Addition-\\nally, in code generation tasks, PERC (Yoo et al.,\\n2025) retrieves pseudocode by semantic or struc-\\ntural similarity from datasets like HumanEval, en-\\nsuring alignment with target code.\\n5\\nSynergized RAG-Reasoning\\nMany real-world problems, such as open-domain\\nquestion answering (Yang et al., 2015; Chen and\\nYih, 2020) and scientific discovery (Lu et al., 2024;\\nWang et al., 2023; Baek et al., 2024; Schmidgall\\net al., 2025), require an iterative approach where\\nnew evidence continuously informs better reason-\\ning and vice versa. A single retrieval step may not\\nprovide sufficient information, and a single round'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='new evidence continuously informs better reason-\\ning and vice versa. A single retrieval step may not\\nprovide sufficient information, and a single round\\nof reasoning may overlook key insights (Trivedi\\net al., 2023). By tightly integrating retrieval and\\nreasoning in a multi-step, interactive manner, these\\nsystems can progressively refine both the search rel-\\nevance of retrieved information and the reasoning-\\nbased understanding of the original query. We\\nfocus on two complementary perspectives within\\nexisting approaches: reasoning workflows, which\\nemphasize structured, often pre-defined inference\\nformats for multi-step reasoning; and agent orches-\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='tration, which focus on how agents interact with\\nenvironment and coordinate with each others.\\n5.1\\nReasoning Workflow\\nBroadly, the reasoning workflows can be catego-\\nrized as chain-based, tree-based, or graph-based,\\nreflecting an evolution from linear reasoning chains\\nto branching and expressive reasoning structures.\\n5.1.1\\nChain-based\\nChain-of-Thought (CoT) (Wei et al., 2022) struc-\\ntures the reasoning process as a linear sequence\\nof intermediate steps. However, relying solely on\\nthe parametric knowledge of LLMs can lead to\\nerror propagation. To solve this, IRCoT (Trivedi\\net al., 2023) and Rat (Wang et al., 2024g) interleave\\nretrieval operations between reasoning steps. Sev-\\neral recent methods further improve the robustness\\nand rigor of this chain-based paradigm via verifi-\\ncation and filtering. CoV-RAG (He et al., 2024a)\\nintroduces a chain-of-verification that checks and\\ncorrects each reasoning step against retrieved ref-\\nerences. To combat noisy or irrelevant context, ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='introduces a chain-of-verification that checks and\\ncorrects each reasoning step against retrieved ref-\\nerences. To combat noisy or irrelevant context, ap-\\nproaches like RAFT (Zhang et al., 2024a) fine-tune\\nLLMs to ignore distractor documents, while Chain-\\nof-Note (Yu et al., 2024) prompts the model to take\\nsequential “reading notes” on retrieved documents\\nto filter out unhelpful information.\\n5.1.2\\nTree-based\\nTree-based reasoning methods typically adopt ei-\\nther Tree-of-Thought (ToT) (Yao et al., 2023a)\\nor Monte Carlo Tree Search (MCTS) (Browne\\net al., 2012) approaches. ToT extends the CoT\\nto explicitly construct a deterministic reasoning\\ntree and branch multiple logical pathways. Exam-\\nples include RATT (Zhang et al., 2025a), which\\nconstruct retrieval-augmented thought trees to si-\\nmultaneously evaluate multiple reasoning trajec-\\ntories.\\nSuch ToT principles avoid LLM being\\ntrapped by an early mistaken assumption and have\\nbeen applied to address ambiguous questions (Kim'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='multaneously evaluate multiple reasoning trajec-\\ntories.\\nSuch ToT principles avoid LLM being\\ntrapped by an early mistaken assumption and have\\nbeen applied to address ambiguous questions (Kim\\net al., 2023), to cover different diagnostic possibili-\\nties (Yang and Huang, 2025), and to create complex\\nstories (Wen et al., 2023). Conversely, MCTS-\\nbased approaches like AirRAG (Feng et al., 2025),\\nMCTS-RAG (Hu et al., 2025), and SeRTS (Hu\\net al., 2024) employ probabilistic tree search, dy-\\nnamically prioritizing exploration based on heuris-\\ntic probabilities. To ensure retrieval and reason-\\ning quality, AirRAG (Feng et al., 2025) incorpo-\\nrates self-consistency checks, and MCTS-RAG (Hu\\net al., 2025) integrates adaptive MCTS retrieval to\\nrefine evidence and reduce hallucinations.\\n5.1.3\\nGraph-based\\nWalk-on-Graph methods mainly rely on graph\\nlearning techniques for the retrieval and rea-\\nsoning.\\nFor example, PullNet (Sun et al.,\\n2019), QA-GNN (Yasunaga et al., 2021), and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='5.1.3\\nGraph-based\\nWalk-on-Graph methods mainly rely on graph\\nlearning techniques for the retrieval and rea-\\nsoning.\\nFor example, PullNet (Sun et al.,\\n2019), QA-GNN (Yasunaga et al., 2021), and\\nGreaseLM (Zhang et al., 2022b) directly integrate\\ngraph neural networks (GNNs) to iteratively aggre-\\ngate information from neighbor nodes, excelling\\nat modeling the intricate relationships inherent in\\ngraph-structured data. Methods such as SR (Zhang\\net al., 2022a), LightRAG (Guo et al., 2024), and\\nStructRAG (Li et al., 2024h) employ lightweight\\ngraph techniques such as vector indexing and\\nPageRank to efficiently retrieve and reason in multi-\\nhop context, providing the LLM with high-quality,\\nstructured content tailored for the queries. In con-\\ntrast, Think-on-Graph methods integrate graph\\nstructures directly into the LLM reasoning loop,\\nenabling dynamic and iterative retrieval and reason-\\ning processes guided by the LLMs themselves. In\\nthe Think-on-Graph (ToG) framework (Sun et al.,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='structures directly into the LLM reasoning loop,\\nenabling dynamic and iterative retrieval and reason-\\ning processes guided by the LLMs themselves. In\\nthe Think-on-Graph (ToG) framework (Sun et al.,\\n2024b; Ma et al., 2024a), the LLM uses the KG as\\na “reasoning playground”: at each step, it decides\\nwhich connected entity or relation to explore next,\\ngradually building a path that leads to the answer.\\nWhile Graph-CoT (Jin et al., 2024) introduces a\\nthree-stage iterative loop (reasoning, graph inter-\\naction, and execution), KGP (Wang et al., 2024d)\\nprioritize first constructing a document-level KG,\\nboth enabling LLM-driven graph traversal agent to\\nnavigate passages in each step with globally coher-\\nent context. GraphReader (Li et al., 2024f) further\\nrefines this paradigm by coupling LLM reasoning\\nwith explicit subgraph retrieval and evidence an-\\nchoring at each step\\n5.2\\nAgent Orchestration\\nAccording to agent architectures (Luo et al., 2025a),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='refines this paradigm by coupling LLM reasoning\\nwith explicit subgraph retrieval and evidence an-\\nchoring at each step\\n5.2\\nAgent Orchestration\\nAccording to agent architectures (Luo et al., 2025a),\\nwe organize existing work into single-agent and\\nmulti-agent. Particularly, we have attached recent\\nadvances in agentic deep research and implementa-\\ntions in Appendix B.\\n5.2.1\\nSingle-Agent\\nSingle agentic system interweaves knowledge re-\\ntrieval (search) into an LLM’s reasoning loop, en-\\nabling dynamic information lookup at each step\\nof problem solving and incentivizing it to actively\\nseek out relevant evidence when needed.\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='The ReAct (Yao et al., 2023b) paradigm and its\\nderivatives (Li et al., 2025b; Alzubi et al., 2025)\\nhave pioneered this prompting strategy by guid-\\ning LLMs to explicitly alternate between reason-\\ning steps and external tool interactions, such as\\ndatabase searches. Different from ReAct that sepa-\\nrates reasoning and action, with explicit commands\\nlike “search” triggering external retrieval, meth-\\nods such as Self-Ask (Press et al., 2023) and IR-\\nCoT (Trivedi et al., 2023) prompt the model to\\nrecursively formulate and answer sub-questions,\\nenabling interleaved retrieval within the Chain-of-\\nThought (step-by-step retrieval and reasoning). In-\\nvolving self-reflection strategies, DeepRAG (Guan\\net al., 2025) and Self-RAG (Asai et al., 2024) em-\\npower LLMs to introspectively assess their knowl-\\nedge limitations and retrieve only when necessary.\\nRather than relying solely on prompting or static\\nretrievers, Toolformer (Schick et al., 2023) and IN-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='edge limitations and retrieve only when necessary.\\nRather than relying solely on prompting or static\\nretrievers, Toolformer (Schick et al., 2023) and IN-\\nTERS (Zhu et al., 2024) represent a complementary\\napproach via supervised fine-tuning (SFT) LLMs\\non instruction-based or synthetic datasets that inter-\\nleave search and reasoning. Synthetic data genera-\\ntion (Schick et al., 2023; Mao et al., 2024; Zhang\\net al., 2024a) aims to create large-scale, diverse,\\nand task-specific datasets for search without the\\nneed for extensive human annotation. In contrast,\\ninstruction-based data reformulation (Zhu et al.,\\n2024; Wang et al., 2024a; Lin et al., 2023; Nguyen\\net al., 2024) repurposes existing datasets into in-\\nstructional formats to fine-tune models for im-\\nproved generalization and alignment with human-\\nlike reasoning. INTERS (Zhu et al., 2024) exem-\\nplifies this approach by introducing a SFT dataset\\nencompassing 20 tasks, derived from 43 distinct\\ndatasets with manually written templates.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='like reasoning. INTERS (Zhu et al., 2024) exem-\\nplifies this approach by introducing a SFT dataset\\nencompassing 20 tasks, derived from 43 distinct\\ndatasets with manually written templates.\\nReinforcement learning (RL)-incentivized ap-\\nproaches provides a mechanism to optimize answer\\nquality via reward signals on incentivizing agents’\\nbehaviors – what to search, how to integrate re-\\ntrieved evidence, and when to stop, aiming at com-\\nplex knowledge-intensive tasks (or “deep research”\\nquestions). Notable efforts like WebGPT (Nakano\\net al., 2021) and RAG-RL (Huang et al., 2025a)\\nfocus on improving reasoning fidelity by rewarding\\noutputs based on factual correctness or human pref-\\nerence. More recent contributions operate directly\\nin dynamic environments (e.g., live web search, lo-\\ncal search tools), training agents to explore, reflect,\\nand self-correct in noisy real-world conditions. For\\nexample, Search-R1 (Jin et al., 2025) learns to gen-\\nerate <search> token during reasoning and con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='and self-correct in noisy real-world conditions. For\\nexample, Search-R1 (Jin et al., 2025) learns to gen-\\nerate <search> token during reasoning and con-\\ncurrently R1-Searcher (Song et al., 2025) builds\\non RL-driven search demonstrating strong gener-\\nalization across domains. Deep-Researche (Zheng\\net al., 2025) make step further by introducing the\\nfirst end-to-end RL-trained research agent that in-\\nteracts with the open web. These settings showcase\\nemergent capabilities, like decomposition, itera-\\ntive verification, and retrieval planning, that su-\\npervised methods often hard to instill. Moreover,\\nReSearch (Chen et al., 2025b) and ReARTeR (Sun\\net al., 2025c) tackle a deeper challenge: not just\\nproducing correct answers, but aligning reasoning\\nsteps with both factuality and interpretability.\\n5.2.2\\nMulti-Agent\\nThe exploration of multi-agent collaboration within\\nRAG and reasoning has led to diverse orchestra-\\ntions: centralized architectures (harness collective'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='5.2.2\\nMulti-Agent\\nThe exploration of multi-agent collaboration within\\nRAG and reasoning has led to diverse orchestra-\\ntions: centralized architectures (harness collective\\nintelligence from workers-manager paradigm) and\\ndecentralized architectures (leverage complemen-\\ntary capabilities from role-specialized agents).\\nDecentralized architectures deploy multiple\\nagents to collaboratively perform retrieval, reason-\\ning, and knowledge integration, aiming to broaden\\ncoverage of relevant information and fully exploit\\nthe heterogeneous strengths of specialized agents.\\nWang et al. (2024e) and Salve et al. (2024) in-\\ntroduce multi-agent systems where each agent re-\\ntrieves from a partitioned database or a specific data\\nsource (relational databases, NoSQL document\\nstores, etc.). Beyond retrieval, Collab-RAG (Xu\\net al., 2025b) and RAG-KG-IL (Yu and McQuade,\\n2025) integrate different model capacities and as-\\nsign them different roles in reasoning and knowl-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='stores, etc.). Beyond retrieval, Collab-RAG (Xu\\net al., 2025b) and RAG-KG-IL (Yu and McQuade,\\n2025) integrate different model capacities and as-\\nsign them different roles in reasoning and knowl-\\nedge integration. This philosophy extends to multi-\\nmodal settings as in MDocAgent (Han et al., 2025),\\nwhich employs a team of text and image agents to\\nprocess and reason the document-based QA. A gen-\\neral formulation is seen in Agentic reasoning (Wu\\net al., 2025c), which unites tool-using agents for\\nsearch, computation, and structured reasoning, or-\\nchestrated to solve complex analytical tasks.\\nCentralized architectures structure agents in hi-\\nerarchical centralized patterns, supporting efficient\\ntask decomposition and progressive refinement.\\nHM-RAG (Liu et al., 2025) and SurgRAW (Low\\net al., 2025) both employ decomposer-retriever-\\ndecider architectures, where different agent roles\\nisolate subproblems such as multimodal processing\\nor surgical decision-making. Wu et al. (2025a) and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='et al., 2025) both employ decomposer-retriever-\\ndecider architectures, where different agent roles\\nisolate subproblems such as multimodal processing\\nor surgical decision-making. Wu et al. (2025a) and\\nIannelli et al. (2024) emphasize dynamic routing\\nand system reconfiguration, respectively—enabling\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='intelligent agent selection based on task relevance\\nor resource constraints. Chain of Agents (Zhang\\net al., 2024c) and the cooperative multi-agent con-\\ntrol framework for on-ramp merging (Zhang et al.,\\n2025c) illustrate hierarchical agent designs where\\nlayered processing enables long-context summa-\\nrization or policy refinement. Collectively, these\\nworks demonstrate how centralized control and hi-\\nerarchical pipelining foster efficiency and adapt-\\nability in multi-agent RAG-reasoning systems.\\n6\\nBenchmarks and Datasets\\nBenchmarks and datasets for simultaneously evalu-\\nating knowledge (RAG) and reasoning capability\\ncover a wide range of complexities, from basic\\nfact retrieval to intricate multi-step reasoning in\\ngeneral or specific domains. We categorize no-\\ntable benchmarks in several tasks and list them in\\nTable 1 and highlight their details and properties.\\nThese representative tasks include Web browsing,\\nsuch as BrowseComp (Wei et al., 2025a), single-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='table benchmarks in several tasks and list them in\\nTable 1 and highlight their details and properties.\\nThese representative tasks include Web browsing,\\nsuch as BrowseComp (Wei et al., 2025a), single-\\nhop QA, such as TriviaQA (Joshi et al., 2017),\\nmulti-hop QA, such as HotpotQA (Yang et al.,\\n2018), multiple-choice QA, such as MMLU-Pro\\n(Wang et al., 2025b), mathematics, such as MATH\\n(Hendrycks et al., 2021), and code-centric eval-\\nuations from LiveCodeBench (Jain et al., 2024).\\nMore tasks can refer to Appendix A and Table 2.\\n7\\nFuture Work\\nFuture research directions for Synergized RAG-\\nReasoning systems center around enhancing both\\nreasoning and retrieval capabilities to meet real-\\nworld demands for accuracy, efficiency, trust, and\\nuser alignment. We outline several key challenges\\nand opportunities below.\\n• Reasoning Efficiency. Despite their advantages\\nin complex reasoning, Synergized RAG-Reasoning\\nsystems can suffer significant latency due to itera-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='and opportunities below.\\n• Reasoning Efficiency. Despite their advantages\\nin complex reasoning, Synergized RAG-Reasoning\\nsystems can suffer significant latency due to itera-\\ntive retrieval and multi-step reasoning loops (Sui\\net al., 2025). For instance, executing a single deep\\nresearch query can take over 10 minutes in prac-\\ntical settings. This issue is especially pronounced\\nin chain-based workflows discussed in Section 5.\\nFuture research should explore reasoning efficiency\\nthrough latent reasoning approaches and strategic\\ncontrol over reasoning depth via thought distilla-\\ntion and length-penalty (Xia et al., 2025a; Zhang\\net al., 2025b). Beyond reasoning itself, emerging\\ndirections in models compression like quantization,\\npruning, and knowledge distillation is worth to ex-\\nplore for efficient small RAG-reasoning systems.\\n• Retrieval Efficiency. On the retrieval side, effi-\\nciency demands budget-aware query planning and\\nmemory-aware mechanisms that cache prior evi-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='plore for efficient small RAG-reasoning systems.\\n• Retrieval Efficiency. On the retrieval side, effi-\\nciency demands budget-aware query planning and\\nmemory-aware mechanisms that cache prior evi-\\ndence or belief states to reduce redundant access\\n(Zhao et al., 2024a). Additionally, adaptive re-\\ntrieval control, learning when and how much to\\nretrieve based on uncertainty signals can reduce\\nwasteful operations. These technical paths push\\nthe system beyond static RAG, toward dynamic\\nself-regulation of efficient retreival behaviors un-\\nder real-world constraints.\\n• Human-Agent Collaboration. Many applica-\\ntions of RAG-Reasoning, such as literature reviews\\nor interactive programming, are inherently person-\\nalized and cannot assume users know precisely\\nwhat to ask or how to process retrieved results (Sun\\net al., 2025b). Corresponding to Section 5.2, hu-\\nmans can act as advanced agents, providing nu-\\nanced feedback to steer reasoning processes. Fu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='what to ask or how to process retrieved results (Sun\\net al., 2025b). Corresponding to Section 5.2, hu-\\nmans can act as advanced agents, providing nu-\\nanced feedback to steer reasoning processes. Fu-\\nture systems should develop methods for modeling\\nuser intent under uncertainty (Zhang et al., 2025e;\\nYang et al., 2025), building interactive interfaces\\nfor iterative clarification, and designing agents that\\nadapt reasoning strategies based on user exper-\\ntise and preferences (Zhang et al., 2025g). This\\nhuman-in-the-loop approach (Zou et al., 2025) is\\nessential for creating robust and user-aligned RAG-\\nReasoning systems in open-ended domains.\\n• Agentic Structures and Capabilities. A key fea-\\nture of Synergized RAG-Reasoning is its agentic ar-\\nchitecture, where the system autonomously decides\\nthe roles of different agents and which tools or re-\\ntrieval strategies to invoke during inference stages\\n(Luo et al., 2025a; Bei et al., 2025). To fully ex-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='the roles of different agents and which tools or re-\\ntrieval strategies to invoke during inference stages\\n(Luo et al., 2025a; Bei et al., 2025). To fully ex-\\nploit this potential, future research should focus on\\ndeveloping agent frameworks capable of dynamic\\ntool selection, retrieval planning, and adaptive or-\\nchestration across reasoning workflows. Such ca-\\npabilities enable flexible, context-aware problem\\nsolving and are critical for handling diverse, com-\\nplex tasks (Schneider, 2025).\\n• Multimodal Retrieval. As also shown in our\\nbenchmark analysis, most existing Synergized\\nRAG-Reasoning systems remain confined to text-\\nonly tasks. However, real-world applications in-\\ncreasingly require the ability to retrieve and in-\\ntegrate multimodal content (Liang et al., 2024).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Task\\nDataset\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning\\nSize\\nInput\\nOutput\\nWeb Browsing\\nBrowseComp (Wei et al., 2025a)\\nGeneral\\nHuman, Internet\\nCommonsense, Logical\\nDeductive\\n1,266\\nQuestion/Text\\nNatural Language\\nGAIA (Mialon et al., 2023)\\nGeneral\\nInternet, TooL\\nCommonsense, Logical\\nDeductive\\n466\\nQuestion/Text,\\nImage/File/Code\\nNatural Language\\nWebWalkerQA (Wu et al., 2025b)\\nGeneral\\nHuman, LLM\\nCommonsense, Logical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nSingle-hop QA\\nTriviaQA (Joshi et al., 2017)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n650,000+\\nQuestion/Text\\nNatural Language\\nNQ (Kwiatkowski et al., 2019)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n307,373\\nQuestion/Text\\nNatural Language\\nMulti-hop QA\\n2WikiMultiHopQA (Ho et al., 2020)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al., 2018)\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al., 2022)\\nGeneral'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Deductive\\n192,606\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al., 2018)\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al., 2022)\\nGeneral\\nPrevious Resource,\\nInternet\\nCommonsense, Logical\\nDeductive\\n25,000\\nQuestion/Text\\nNatural Language\\nMulti-choice QA QuALITY (Pang et al., 2022)\\nNarrative\\nBooks\\nCommonsense, Logical\\nDeductive,\\nAbductive\\n6,737\\nQuestion/Text,\\nOptions\\nOptions\\nMMLU-Pro (Wang et al., 2025b)\\nScience\\nPrevious Resource,\\nInternet\\nArithmetic, Commonsense,\\nLogical\\nDeductive,\\nInductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Langue,\\nNumber, Options\\nMath\\nMATH (Hendrycks et al., 2021)\\nMath\\nExam\\nArithmetic, Logic\\nDeductive\\n12,500\\nQuestion/Text,\\nFigure, Equation\\nNatural Langue,\\nNumber\\nAQuA (Ling et al., 2017)\\nMath\\nExam, Internet,\\nPrevious Resource\\nArithmetic, Logic\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equation\\nNatural Langue,\\nOptions\\nCode\\nRefactoring Oracle (Tsantalis et al.,\\n2020)\\nSoftware\\nInternet, Human\\nLogical\\nDeductive'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Arithmetic, Logic\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equation\\nNatural Langue,\\nOptions\\nCode\\nRefactoring Oracle (Tsantalis et al.,\\n2020)\\nSoftware\\nInternet, Human\\nLogical\\nDeductive\\n7,226\\nCode, Instruction\\nCode\\nLiveCodeBench (Jain et al., 2024)\\nContest\\nInternet\\nLogical\\nDeductive,\\nAbductive\\n500+\\nQuestion/Text,\\nCode, Instruction\\nCode, Test Output\\nTable 1: Overview of representative knowledge and reasoning intensive benchmarks by task category.\\nFuture research should move beyond the tradi-\\ntional vision-text paradigm to achieve genuine mul-\\ntimodality. This advancement necessitates strength-\\nening foundational abilities of MLLMs, including\\ngrounding and cross-modal reasoning (Liang et al.,\\n2024). Additionally, enhancing the agentic capabil-\\nities of these models through hybrid-modal chain-\\nof-thought reasoning is crucial, enabling interac-\\ntion with the real world via multimodal search tools\\n(Wang et al., 2025a). Concurrently, developing uni-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='of-thought reasoning is crucial, enabling interac-\\ntion with the real world via multimodal search tools\\n(Wang et al., 2025a). Concurrently, developing uni-\\nfied multimodal retrievers that can jointly embed\\nimages, tables, text, and heterogeneous documents\\nis essential.\\n• Retrieval Trustworthiness. Synergized RAG-\\nReasoning systems remain vulnerable to adversar-\\nial attacks through poisoned or misleading external\\nknowledge sources. Ensuring the trustworthiness\\nof retrieved content is therefore crucial for main-\\ntaining fully reliable downstream reasoning (Huang\\net al., 2024). Techniques like watermarking and\\ndigital fingerprinting have been employed to en-\\nhance system traceability. However, there’s a press-\\ning need to develop more dynamic and adaptive\\nmethods that can keep pace with the evolving land-\\nscape of LLMs, emerging attack techniques, and\\nshifting model contexts (Liu et al., 2024). Existing\\nstudies have also individually explored uncertainty'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='scape of LLMs, emerging attack techniques, and\\nshifting model contexts (Liu et al., 2024). Existing\\nstudies have also individually explored uncertainty\\nquantification and robust generation to bolster sys-\\ntem reliability (Shorinwa et al., 2025). Future re-\\nsearch should aim to integrate these approaches,\\nas their combination can mutually reinforce sys-\\ntem robustness and trustworthiness. Moreover, fu-\\nture efforts should also focus on extending current\\nbenchmarks to encompass multi-dimensional trust\\nmetrics beyond mere accuracy.\\n8\\nConclusion\\nThis survey charts the rapid convergence of re-\\ntrieval and reasoning in LLMs.\\nWe reviewed\\nthree evolutionary stages: (1) Reasoning-Enhanced\\nRAG, which uses multi-step reasoning to refine\\neach stage of RAG; (2) RAG-Enhanced Reason-\\ning, which leverages retrieved knowledge to bridge\\nfactual gaps during long CoT; and (3) Synergized\\nRAG-Reasoning systems, where single- or multi-\\nagents iteratively refine both search and reason-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='ing, which leverages retrieved knowledge to bridge\\nfactual gaps during long CoT; and (3) Synergized\\nRAG-Reasoning systems, where single- or multi-\\nagents iteratively refine both search and reason-\\ning, exemplified by recent “Deep Research” plat-\\nforms. Collectively, these lines demonstrate that\\ntight retrieval–reasoning coupling improves fac-\\ntual grounding, logical coherence, and adaptability\\nbeyond one-way enhancement. Looking forward,\\nwe identify research avenues toward synergized\\nRAG-Reasoning systems that are more effective,\\nmultimodally-adaptive, trustworthy, and human-\\ncentric.\\nLimitations\\nWhile this survey synthesizes over 200 research\\npapers across RAG and reasoning with large lan-\\nguage models, its scope favors breadth over depth.\\nIn striving to provide a unified and comprehen-\\nsive taxonomy, we may not delve deeply into the\\ntechnical nuances or implementation details of indi-\\nvidual methods-especially within specialized sub-\\nfields of either RAG (e.g., sparse vs. dense re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='sive taxonomy, we may not delve deeply into the\\ntechnical nuances or implementation details of indi-\\nvidual methods-especially within specialized sub-\\nfields of either RAG (e.g., sparse vs. dense re-\\ntrieval, memory-augmented retrievers) or reason-\\ning (e.g., formal logic solvers, symbolic methods,\\nor long-context reasoning). Moreover, our cate-\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='gorization framework (reasoning-enhanced RAG,\\nRAG-enhanced reasoning, and synergized RAG\\nand reasoning) abstracts across diverse methodolo-\\ngies. While this facilitates a high-level understand-\\ning of design patterns, it may obscure the finer-\\ngrained trade-offs, assumptions, and limitations\\nunique to each class of approach.\\nReferences\\nVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-\\nman, Harm de Vries, and Siva Reddy. 2022. Topi-\\nocqa: Open-domain conversational question answer-\\ning with topic switching. Transactions of the Associ-\\nation for Computational Linguistics, 10:468–483.\\nFiroj Alam, Ferda Ofli, and Muhammad Imran. 2018.\\nCrisismmd: Multimodal twitter datasets from natural\\ndisasters. In Proceedings of the international AAAI\\nconference on web and social media, volume 12.\\nSalaheddin Alzubi, Creston Brooks, Purva Chiniya,\\nEdoardo Contente, Chiara von Gerlach, Lucas Irwin,\\nYihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong\\nOh, et al. 2025. Open deep search: Democratizing'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Salaheddin Alzubi, Creston Brooks, Purva Chiniya,\\nEdoardo Contente, Chiara von Gerlach, Lucas Irwin,\\nYihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong\\nOh, et al. 2025. Open deep search: Democratizing\\nsearch with open-source reasoning agents. arXiv\\npreprint arXiv:2503.20201.\\nAnonymous. 2025.\\nDynQR: Dynamic uncertainty-\\nguided query rewriting for effective retrieval-\\naugmented generation. In Submitted to ACL Rolling\\nReview - December 2024. Under review.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil,\\nand Hannaneh Hajishirzi. 2023.\\nSelf-RAG: Self-\\nreflective retrieval augmented generation.\\nIn\\nNeurIPS 2023 Workshop on Instruction Tuning and\\nInstruction Following.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2024. Self-RAG: Learning to\\nretrieve, generate, and critique through self-reflection.\\nIn The Twelfth International Conference on Learning\\nRepresentations.\\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,\\nand Sung Ju Hwang. 2024.\\nResearchagent: Iter-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='In The Twelfth International Conference on Learning\\nRepresentations.\\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,\\nand Sung Ju Hwang. 2024.\\nResearchagent: Iter-\\native research idea generation over scientific liter-\\nature with large language models. arXiv preprint\\narXiv:2404.07738.\\nYuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi\\nChen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu,\\nShirui Pan, Yizhou Yu, et al. 2025. Graphs meet ai\\nagents: Taxonomy, progress, and future opportunities.\\narXiv preprint arXiv:2506.18019.\\nCameron B Browne, Edward Powley, Daniel White-\\nhouse, Simon M Lucas, Peter I Cowling, Philipp\\nRohlfshagen, Stephen Tavener, Diego Perez, Spyri-\\ndon Samothrakis, and Simon Colton. 2012. A survey\\nof monte carlo tree search methods. IEEE Transac-\\ntions on Computational Intelligence and AI in games,\\n4(1):1–43.\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='4(1):1–43.\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-\\nvey on evaluation of large language models. ACM\\ntransactions on intelligent systems and technology,\\n15(3):1–45.\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\\nquestion answering. In Proceedings of the 58th an-\\nnual meeting of the association for computational\\nlinguistics: tutorial abstracts, pages 34–37.\\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou,\\nChenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen\\nZhang, Huajun Chen, Fan Yang, et al. 2025a.\\nResearch:\\nLearning to reason with search for\\nllms via reinforcement learning.\\narXiv preprint\\narXiv:2503.19470.\\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou,\\nChenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng\\nChen, Haofen Wang, Jeff Z Pan, et al. 2025b. Learn-\\ning to reason with search for llms via reinforcement\\nlearning. arXiv preprint arXiv:2503.19470.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng\\nChen, Haofen Wang, Jeff Z Pan, et al. 2025b. Learn-\\ning to reason with search for llms via reinforcement\\nlearning. arXiv preprint arXiv:2503.19470.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng,\\nJiannan Guan, Peng Wang, Mengkang Hu, Yuhang\\nZhou, Te Gao, and Wanxiang Che. 2025c. Towards\\nreasoning era: A survey of long chain-of-thought\\nfor reasoning large language models. arXiv preprint\\narXiv:2503.09567.\\nYanfei Chen, Jinsung Yoon, Devendra Sachan, Qingze\\nWang, Vincent Cohen-Addad, Mohammadhossein\\nBateni, Chen-Yu Lee, and Tomas Pfister. 2024a. Re-\\ninvoke: Tool invocation rewriting for zero-shot tool\\nretrieval. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 4705–4726.\\nZehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu,\\nWenwei Zhang, Kai Chen, and Feng Zhao. 2024b.\\nMindsearch: Mimicking human minds elicits deep ai\\nsearcher. arXiv preprint arXiv:2407.20183.\\nZhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='Wenwei Zhang, Kai Chen, and Feng Zhao. 2024b.\\nMindsearch: Mimicking human minds elicits deep ai\\nsearcher. arXiv preprint arXiv:2407.20183.\\nZhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen\\nHuang, Yong Dou, Xuhui Jiang, and Jian Guo. 2024c.\\nRulerag: Rule-guided retrieval-augmented genera-\\ntion with language models for question answering.\\narXiv preprint arXiv:2410.22353.\\nDaixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng\\nZhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu\\nWei, Weiwei Deng, and Qi Zhang. 2023. Uprise:\\nUniversal prompt retrieval for improving zero-shot\\nevaluation. In Proceedings of the 2023 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 12318–12337.\\nRong Cheng, Jinyi Liu, Yan Zheng, Fei Ni, Jiazhen Du,\\nHangyu Mao, Fuzheng Zhang, Bo Wang, and Jianye\\nHao. 2025. Dualrag: A dual-process approach to in-\\ntegrate reasoning and retrieval for multi-hop question\\nanswering. arXiv preprint arXiv:2504.18243.\\nZheng Chu, Jingchang Chen, Qianglong Chen, Haotian'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='tegrate reasoning and retrieval for multi-hop question\\nanswering. arXiv preprint arXiv:2504.18243.\\nZheng Chu, Jingchang Chen, Qianglong Chen, Haotian\\nWang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu,\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='and Bing Qin. 2024. Beamaggr: Beam aggregation\\nreasoning over multi-source knowledge for multi-\\nhop question answering. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 1229–\\n1248.\\nDebrup Das, Debopriyo Banerjee, Somak Aditya,\\nand Ashish Kulkarni. 2024. Mathsensei: A tool-\\naugmented large language model for mathematical\\nreasoning. In Proceedings of the 2024 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies (Volume 1: Long Papers), pages 942–966.\\nChao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-\\nZhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song,\\nBo Zheng, et al. 2024. Longdocurl: a comprehensive\\nmultimodal long document benchmark integrating un-\\nderstanding, reasoning, and locating. arXiv preprint\\narXiv:2412.18424.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='derstanding, reasoning, and locating. arXiv preprint\\narXiv:2412.18424.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. 2024. Chain-of-verification reduces\\nhallucination in large language models. In Findings\\nof the Association for Computational Linguistics ACL\\n2024, pages 3563–3578.\\nAvik Dutta, Mukul Singh, Gust Verbruggen, Sumit Gul-\\nwani, and Vu Le. 2024. Rar: Retrieval-augmented re-\\ntrieval for code generation in low resource languages.\\nIn Proceedings of the 2024 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n21506–21515.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang,\\nHengyun Li, Dawei Yin, Tat-Seng Chua, and Qing\\nLi. 2024a. A survey on rag meeting llms: Towards\\nretrieval-augmented large language models. In Pro-\\nceedings of the 30th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, pages 6491–\\n6501.\\nYue Fan, Hu Zhang, Ru Li, Yujie Wang, Hongye Tan,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='ceedings of the 30th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, pages 6491–\\n6501.\\nYue Fan, Hu Zhang, Ru Li, Yujie Wang, Hongye Tan,\\nand Jiye Liang. 2024b. Frva: Fact-retrieval and ver-\\nification augmented entailment tree generation for\\nexplainable question answering. In Findings of the\\nAssociation for Computational Linguistics ACL 2024,\\npages 9111–9128.\\nJinyuan Fang, Zaiqiao Meng, and Craig Macdonald.\\n2024. Trace the evidence: Constructing knowledge-\\ngrounded reasoning chains for retrieval-augmented\\ngeneration. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2024, pages 8472–\\n8494.\\nWeizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang,\\nBo Bai, Lei Deng, and Wei Han. 2024.\\nRe-\\ntrieval meets reasoning: Dynamic in-context edit-\\ning for long-text understanding.\\narXiv preprint\\narXiv:2406.12331.\\nWenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi\\nSong, and Hao Wang. 2025.\\nAirrag:\\nActivat-\\ning intrinsic reasoning for retrieval augmented gen-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='arXiv preprint\\narXiv:2406.12331.\\nWenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi\\nSong, and Hao Wang. 2025.\\nAirrag:\\nActivat-\\ning intrinsic reasoning for retrieval augmented gen-\\neration via tree-based search.\\narXiv preprint\\narXiv:2501.10053.\\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi,\\nTushar Khot, and Pradeep Dasigi. 2020.\\nIirc: A\\ndataset of incomplete information reading compre-\\nhension questions. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 1137–1147.\\nZafeirios Fountas, Martin A Benfeghoul, Adnan Oomer-\\njee, Fenia Christopoulou, Gerasimos Lampouras,\\nHaitham Bou-Ammar, and Jun Wang. 2024. Human-\\nlike episodic memory for infinite context llms. arXiv\\npreprint arXiv:2407.09450.\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\\nKelvin Guu. 2023a. RARR: Researching and revis-\\ning what language models say, using language mod-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\\nKelvin Guu. 2023a. RARR: Researching and revis-\\ning what language models say, using language mod-\\nels. In Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 16477–16508, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang\\nJia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun,\\nHaofen Wang, and Haofen Wang. 2023b. Retrieval-\\naugmented generation for large language models: A\\nsurvey. arXiv preprint arXiv:2312.10997, 2:1.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 9:346–\\n361.\\nXinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin,\\nYaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='Association for Computational Linguistics, 9:346–\\n361.\\nXinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin,\\nYaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and\\nJie Zhou. 2025. Deeprag: Thinking to retrieval step\\nby step for large language models. arXiv preprint\\narXiv:2502.01142.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and\\nChao Huang. 2024.\\nLightrag: Simple and fast\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2410.05779.\\nSiwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li,\\nHongtu Zhu, and Huaxiu Yao. 2025. Mdocagent: A\\nmulti-modal multi-agent framework for document\\nunderstanding. arXiv preprint arXiv:2503.13964.\\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.\\n2023.\\nToolkengpt: Augmenting frozen language\\nmodels with massive tools via tool embeddings. In\\nAdvances in Neural Information Processing Systems,\\nvolume 36, pages 45870–45894.\\nBolei He, Nuo Chen, Xinran He, Lingyong Yan,\\nZhenkai Wei, Jinchang Luo, and Zhen-Hua Ling.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='2024a.\\nRetrieving, rethinking and revising: The\\nchain-of-verification can improve retrieval aug-\\nmented generation. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2024, pages\\n10371–10393.\\nJie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Z\\nPan. 2024b. Mintqa: A multi-hop question answer-\\ning benchmark for evaluating llms on new and tail\\nknowledge. arXiv preprint arXiv:2412.17032.\\nXiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla,\\nThomas Laurent, Yann LeCun, Xavier Bresson,\\nand Bryan Hooi. 2024c.\\nG-retriever: Retrieval-\\naugmented generation for textual graph understand-\\ning and question answering. Advances in Neural\\nInformation Processing Systems, 37:132876–132907.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and\\nJacob Steinhardt. 2021. Measuring mathematical\\nproblem solving with the MATH dataset. In Thirty-\\nfifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Jacob Steinhardt. 2021. Measuring mathematical\\nproblem solving with the MATH dataset. In Thirty-\\nfifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing a multi-hop\\nqa dataset for comprehensive evaluation of reasoning\\nsteps. In Proceedings of the 28th International Con-\\nference on Computational Linguistics, pages 6609–\\n6625.\\nMinda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou,\\nJingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li,\\nand Irwin King. 2024. Serts: Self-rewarding tree\\nsearch for biomedical retrieval-augmented genera-\\ntion. arXiv preprint arXiv:2406.11258.\\nYunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan.\\n2025. Mcts-rag: Enhancing retrieval-augmented gen-\\neration with monte carlo tree search. arXiv preprint\\narXiv:2503.20757.\\nJerry Huang, Siddarth Madala, Risham Sidhu, Cheng\\nNiu, Julia Hockenmaier, and Tong Zhang. 2025a.\\nRag-rl: Advancing retrieval-augmented generation'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='arXiv:2503.20757.\\nJerry Huang, Siddarth Madala, Risham Sidhu, Cheng\\nNiu, Julia Hockenmaier, and Tong Zhang. 2025a.\\nRag-rl: Advancing retrieval-augmented generation\\nvia rl and curriculum learning.\\narXiv preprint\\narXiv:2503.12759.\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\\nZhangyin Feng, Haotian Wang, Qianglong Chen,\\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al.\\n2025b. A survey on hallucination in large language\\nmodels: Principles, taxonomy, challenges, and open\\nquestions. ACM Transactions on Information Sys-\\ntems, 43(2):1–55.\\nXiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie\\nJin, Yi Dong, Changshun Wu, Saddek Bensalem,\\nRonghui Mu, Yi Qi, Xingyu Zhao, et al. 2024. A sur-\\nvey of safety and trustworthiness of large language\\nmodels through the lens of verification and validation.\\nArtificial Intelligence Review, 57(7):175.\\nYulong Hui, Yao Lu, and Huanchen Zhang. 2024. Uda:\\nA benchmark suite for retrieval augmented genera-\\ntion in real-world document analysis. In The Thirty-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Artificial Intelligence Review, 57(7):175.\\nYulong Hui, Yao Lu, and Huanchen Zhang. 2024. Uda:\\nA benchmark suite for retrieval augmented genera-\\ntion in real-world document analysis. In The Thirty-\\neight Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.\\nMichael Iannelli, Sneha Kuchipudi, and Vera Dvorak.\\n2024. Sla management in reconfigurable multi-agent\\nrag: A systems approach to question answering.\\narXiv preprint arXiv:2412.06832.\\nShayekh Islam, Md Asib Rahman, KSM Tozammel Hos-\\nsain, Enamul Hoque, Shafiq Joty, and Md Rizwan\\nParvez. 2024. Open-rag: Enhanced retrieval aug-\\nmented reasoning with open-source large language\\nmodels. In Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2024, pages 14231–\\n14244.\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia\\nYan, Tianjun Zhang, Sida Wang, Armando Solar-\\nLezama, Koushik Sen, and Ion Stoica. 2024. Live-\\ncodebench: Holistic and contamination free eval-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia\\nYan, Tianjun Zhang, Sida Wang, Armando Solar-\\nLezama, Koushik Sen, and Ion Stoica. 2024. Live-\\ncodebench: Holistic and contamination free eval-\\nuation of large language models for code. arXiv\\npreprint arXiv:2403.07974.\\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\\ning to adapt retrieval-augmented large language mod-\\nels through question complexity. In Proceedings of\\nthe 2024 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Pa-\\npers), pages 7029–7043.\\nYixin Ji, Kaixin Wu, Juntao Li, Wei Chen, Mingjie\\nZhong, Xu Jia, and Min Zhang. 2024. Retrieval and\\nreasoning on kgs: Integrate knowledge graphs into\\nlarge language models for complex question answer-\\ning. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 7598–7610.\\nMingyi Jia, Junwen Duan, Yan Song, and Jianxin Wang.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='ing. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 7598–7610.\\nMingyi Jia, Junwen Duan, Yan Song, and Jianxin Wang.\\n2025. Find: Fine-grained information density guided\\nadaptive retrieval-augmented generation for disease\\ndiagnosis. arXiv preprint arXiv:2502.14614.\\nJinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie\\nWang, Wayne Xin Zhao, Yang Song, and Tao Zhang.\\n2024. Rag-star: Enhancing deliberative reasoning\\nwith retrieval augmented verification and refinement.\\narXiv preprint arXiv:2412.12881.\\nPengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu\\nTian, SeongKu Kang, Zifeng Wang, Jimeng Sun,\\nand Jiawei Han. 2025. Deepretrieval: Hacking real\\nsearch engines and retrievers with large language\\nmodels via reinforcement learning. arXiv preprint\\narXiv:2503.00223.\\nBowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar\\nRoy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang,\\nSuhang Wang, Yu Meng, et al. 2024. Graph chain-\\nof-thought: Augmenting large language models by'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar\\nRoy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang,\\nSuhang Wang, Yu Meng, et al. 2024. Graph chain-\\nof-thought: Augmenting large language models by\\nreasoning on graphs. In Findings of the Association\\nfor Computational Linguistics ACL 2024, pages 163–\\n184.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang,\\nHamed Zamani, and Jiawei Han. 2025.\\nSearch-\\nr1: Training llms to reason and leverage search en-\\ngines with reinforcement learning. arXiv preprint\\narXiv:2503.09516.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 1601–1611.\\nTomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou,\\nJayashree Karlekar, Sugiri Pranata, Akira Kinose,\\nKoki Oguri, Felix Wick, and Yang You. 2024. Rap:\\nRetrieval-augmented planning with contextual mem-\\nory for multimodal llm agents.\\narXiv preprint\\narXiv:2402.03610.\\nMohammed Khaliq, Paul Chang, Mingyang Ma, Bern-\\nhard Pflugfelder, and Filip Mileti´c. 2024.\\nRagar,\\nyour falsehood radar: Rag-augmented reasoning for\\npolitical fact-checking using multimodal large lan-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='hard Pflugfelder, and Filip Mileti´c. 2024.\\nRagar,\\nyour falsehood radar: Rag-augmented reasoning for\\npolitical fact-checking using multimodal large lan-\\nguage models. In Proceedings of the Seventh Fact Ex-\\ntraction and VERification Workshop (FEVER), pages\\n280–296.\\nGangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joon-\\nsuk Park, and Jaewoo Kang. 2023. Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-\\naugmented large language models. In Proceedings\\nof the 2023 Conference on Empirical Methods in\\nNatural Language Processing, pages 996–1009.\\nNeema Kotonya and Francesca Toni. 2020. Explainable\\nautomated fact-checking for public health claims. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 7740–7754.\\nHeiko Koziolek, Sten Grüner, Rhaban Hark, Viren-\\ndra Ashiwal, Sofia Linsbauer, and Nafise Eskandani.\\n2024. Llm-based and retrieval-augmented control\\ncode generation. In Proceedings of the 1st Inter-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Heiko Koziolek, Sten Grüner, Rhaban Hark, Viren-\\ndra Ashiwal, Sofia Linsbauer, and Nafise Eskandani.\\n2024. Llm-based and retrieval-augmented control\\ncode generation. In Proceedings of the 1st Inter-\\nnational Workshop on Large Language Models for\\nCode, pages 22–29.\\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mo-\\nhananey, Steven Schwarcz, Adam Stambler, Shyam\\nUpadhyay, and Manaal Faruqui. 2024.\\nFact,\\nfetch,\\nand reason:\\nA unified evaluation of\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2409.12941.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, et al. 2019. Natural questions: a benchmark\\nfor question answering research. Transactions of the\\nAssociation for Computational Linguistics, 7:453–\\n466.\\nSung-Min Lee, Eunhwan Park, Donghyeon Jeon, Inho\\nKang, and Seung-Hoon Na. 2024. Radcot: Retrieval-\\naugmented distillation to specialization models for'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='466.\\nSung-Min Lee, Eunhwan Park, Donghyeon Jeon, Inho\\nKang, and Seung-Hoon Na. 2024. Radcot: Retrieval-\\naugmented distillation to specialization models for\\ngenerating chain-of-thoughts in query expansion. In\\nProceedings of the 2024 Joint International Con-\\nference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING 2024),\\npages 13514–13523.\\nZhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang,\\nWeichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi\\nLi. 2025. Rearag: Knowledge-guided reasoning en-\\nhances factuality of large reasoning models with iter-\\native retrieval augmented generation. arXiv preprint\\narXiv:2503.21729.\\nDawei Li, Shu Yang, Zhen Tan, Jae Baik, Sukwon Yun,\\nJoseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-\\nTran, Ying Ding, et al. 2024a. Dalk: Dynamic co-\\naugmentation of llms and kg to answer alzheimer’s\\ndisease questions with scientific literature. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2024, pages 2187–2205.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='augmentation of llms and kg to answer alzheimer’s\\ndisease questions with scientific literature. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2024, pages 2187–2205.\\nGuanghua Li, Wensheng Lu, Wei Zhang, Defu Lian,\\nKezhong Lu, Rui Mao, Kai Shu, and Hao Liao.\\n2024b. Re-search for the truth: Multi-round retrieval-\\naugmented large language models are strong fake\\nnews detectors. arXiv preprint arXiv:2403.09747.\\nGuozheng Li, Peng Wang, Wenjun Ke, Yikai Guo, Ke Ji,\\nZiyu Shang, Jiajun Liu, and Zijie Xu. 2024c. Recall,\\nretrieve and reason: towards better in-context relation\\nextraction. In Proceedings of the Thirty-Third Inter-\\nnational Joint Conference on Artificial Intelligence,\\npages 6368–6376.\\nHuayang Li, Pat Verga, Priyanka Sen, Bowen Yang,\\nVijay Viswanathan, Patrick Lewis, Taro Watanabe,\\nand Yixuan Su. 2024d.\\nAlr2:\\nA retrieve-then-\\nreason framework for long-context question answer-\\ning. arXiv preprint arXiv:2410.03227.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Vijay Viswanathan, Patrick Lewis, Taro Watanabe,\\nand Yixuan Su. 2024d.\\nAlr2:\\nA retrieve-then-\\nreason framework for long-context question answer-\\ning. arXiv preprint arXiv:2410.03227.\\nJia Li, Xianjie Shi, Kechi Zhang, Lei Li, Ge Li, Zheng-\\nwei Tao, Fang Liu, Chongyang Tao, and Zhi Jin.\\n2025a. Coderag: Supportive code retrieval on bi-\\ngraph for real-world code generation. arXiv preprint\\narXiv:2504.10046.\\nMinghan Li, Honglei Zhuang, Kai Hui, Zhen Qin,\\nJimmy Lin, Rolf Jagerman, Xuanhui Wang, and\\nMichael Bendersky. 2024e. Can query expansion im-\\nprove generalization of strong cross-encoder rankers?\\nIn Proceedings of the 47th International ACM SIGIR\\nConference on Research and Development in Infor-\\nmation Retrieval, pages 2321–2326.\\nShilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu,\\nGe Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yang-\\nguang Li, Wanli Ouyang, et al. 2024f. Graphreader:\\nBuilding graph-based agent to enhance long-context\\nabilities of large language models. In Findings of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='guang Li, Wanli Ouyang, et al. 2024f. Graphreader:\\nBuilding graph-based agent to enhance long-context\\nabilities of large language models. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2024, pages 12758–12786.\\nXiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang,\\nYujia Zhou,\\nYutao Zhu,\\nPeitian Zhang,\\nand\\nZhicheng Dou. 2025b. Search-o1: Agentic search-\\nenhanced large reasoning models. arXiv preprint\\narXiv:2501.05366.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yu-\\ntao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng\\nDou. 2025c. Webthinker: Empowering large rea-\\nsoning models with deep research capability. arXiv\\npreprint arXiv:2504.21776.\\nYangning Li, Yinghui Li, Xinyu Wang, Yong Jiang,\\nZhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao\\nZheng, Fei Huang, Jingren Zhou, and Philip S. Yu.\\n2025d.\\nBenchmarking multimodal retrieval aug-\\nmented generation with dynamic VQA dataset and\\nself-adaptive planning agent. In The Thirteenth Inter-\\nnational Conference on Learning Representations.\\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\\nCao, and Shuzi Niu. 2017. Dailydialog: A manually\\nlabelled multi-turn dialogue dataset. arXiv preprint\\narXiv:1710.03957.\\nZhi Li, Yicheng Li, Hequan Ye, and Yin Zhang. 2024g.\\nTowards autonomous tool utilization in language\\nmodels: A unified, efficient and scalable frame-\\nwork. In Proceedings of the 2024 Joint International\\nConference on Computational Linguistics, Language'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Towards autonomous tool utilization in language\\nmodels: A unified, efficient and scalable frame-\\nwork. In Proceedings of the 2024 Joint International\\nConference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING 2024),\\npages 16422–16432.\\nZhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Ji-\\naxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian\\nXu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al.\\n2025e. From system 1 to system 2: A survey of\\nreasoning large language models.\\narXiv preprint\\narXiv:2502.17419.\\nZhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu\\nLin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xian-\\npei Han, Le Sun, and Yongbin Li. 2024h. Struc-\\ntrag: Boosting knowledge intensive reasoning of llms\\nvia inference-time hybrid information structurization.\\narXiv preprint arXiv:2410.08815.\\nZijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang,\\nQi Wang, Qiang Fu, and Ke Liu. 2024. A survey of\\nmultimodel large language models. In Proceedings\\nof the 3rd International Conference on Computer,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang,\\nQi Wang, Qiang Fu, and Ke Liu. 2024. A survey of\\nmultimodel large language models. In Proceedings\\nof the 3rd International Conference on Computer,\\nArtificial Intelligence and Control Engineering, pages\\n405–409.\\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\\nMaria Lomeli, Richard James, Pedro Rodriguez, Ja-\\ncob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.\\nRa-dit: Retrieval-augmented dual instruction tuning.\\nIn The Twelfth International Conference on Learning\\nRepresentations.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. arXiv preprint arXiv:1705.04146.\\nAiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming\\nHu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong,\\nand Philip Yu. 2024. A survey of text watermarking\\nin the era of large language models. ACM Computing\\nSurveys, 57(2):1–36.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong,\\nand Philip Yu. 2024. A survey of text watermarking\\nin the era of large language models. ACM Computing\\nSurveys, 57(2):1–36.\\nPei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan\\nMeng, Ding Wang, and Jun Ma. 2025. Hm-rag: Hier-\\narchical multi-agent multimodal retrieval augmented\\ngeneration. arXiv preprint arXiv:2504.12330.\\nChang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao\\nZeng, Zhu Zhuo, Evangelos B Mazomenos, and\\nYueming Jin. 2025. Surgraw: Multi-agent workflow\\nwith chain-of-thought reasoning for surgical intelli-\\ngence. arXiv preprint arXiv:2503.10265.\\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foer-\\nster, Jeff Clune, and David Ha. 2024. The ai scientist:\\nTowards fully automated open-ended scientific dis-\\ncovery. arXiv preprint arXiv:2408.06292.\\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='covery. arXiv preprint arXiv:2408.06292.\\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:\\nMultimodal reasoning via thought chains for science\\nquestion answering. Advances in Neural Information\\nProcessing Systems, 35:2507–2521.\\nJunyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Jun-\\nwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue\\nQiao, Qingqing Long, et al. 2025a. Large language\\nmodel agent: A survey on methodology, applications\\nand challenges. arXiv preprint arXiv:2503.21460.\\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasu-\\npat, Mehran Kazemi, Chitta Baral, Vaiva Im-\\nbrasaite, and Vincent Y Zhao. 2023.\\nDr. icl:\\nDemonstration-retrieved in-context learning. arXiv\\npreprint arXiv:2305.14128.\\nNe Luo, Aryo Pradipta Gema, Xuanli He, Emile\\nvan Krieken, Pietro Lesci, and Pasquale Minervini.\\n2025b.\\nSelf-training large language models for\\ntool-use without demonstrations.\\narXiv preprint'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile\\nvan Krieken, Pietro Lesci, and Pasquale Minervini.\\n2025b.\\nSelf-training large language models for\\ntool-use without demonstrations.\\narXiv preprint\\narXiv:2502.05867.\\nShengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li,\\nHuaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo.\\n2024a. Think-on-graph 2.0: Deep and faithful large\\nlanguage model reasoning with knowledge-guided\\nretrieval augmented generation.\\narXiv preprint\\narXiv:2407.10805.\\nYubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu,\\nShuohang Wang, Liangming Pan, Yujiu Yang, Yixin\\nCao, and Aixin Sun. 2024b.\\nSciagent:\\nTool-\\naugmented language models for scientific reasoning.\\nIn Proceedings of the 2024 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n15701–15736.\\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen,\\nYizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. 2025. Mmlongbench-doc:\\nBenchmarking long-context document understanding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen,\\nYizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. 2025. Mmlongbench-doc:\\nBenchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information\\nProcessing Systems, 37:95963–96010.\\nKelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo,\\nChenlong Deng, and Zhicheng Dou. 2024.\\nRag-\\nstudio: Towards in-domain adaptation of retrieval\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='augmented generation through self-alignment. In\\nFindings of the Association for Computational Lin-\\nguistics: EMNLP 2024, pages 725–735.\\nMaria Marina, Nikolay Ivanov, Sergey Pletenev,\\nMikhail Salnikov,\\nDaria Galimzianova,\\nNikita\\nKrayko, Vasily Konovalov, Alexander Panchenko,\\nand Viktor Moskvoretskii. 2025. Llm-independent\\nadaptive rag: Let the question speak for itself. arXiv\\npreprint arXiv:2505.04253.\\nCostas Mavromatis and George Karypis. 2024. Gnn-\\nrag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139.\\nGrégoire Mialon, Clémentine Fourrier, Thomas Wolf,\\nYann LeCun, and Thomas Scialom. 2023. Gaia: a\\nbenchmark for general ai assistants. In The Twelfth\\nInternational Conference on Learning Representa-\\ntions.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021.\\nWebgpt: Browser-assisted question-\\nanswering with human feedback.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Long Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021.\\nWebgpt: Browser-assisted question-\\nanswering with human feedback.\\narXiv preprint\\narXiv:2112.09332.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\n2018. Don’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for ex-\\ntreme summarization. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 1797–1807.\\nXuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam,\\nAustin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Sil-\\nvio Savarese, Caiming Xong, and Shafiq Joty. 2024.\\nSfr-rag: Towards contextually faithful llms. arXiv\\npreprint arXiv:2409.09916.\\nCheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Jun-\\ntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu,\\nShizhe Diao, and Tong Zhang. 2024. Veract scan:\\nRetrieval-augmented fake news detection with justifi-\\nable reasoning. In Proceedings of the 62nd Annual'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='tong Song, Randy Zhong, Kaihua Zhu, Siliang Xu,\\nShizhe Diao, and Tong Zhang. 2024. Veract scan:\\nRetrieval-augmented fake news detection with justifi-\\nable reasoning. In Proceedings of the 62nd Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 3: System Demonstrations), pages\\n266–277.\\nYasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and\\nGreg Durrett. 2021. Creak: A dataset for common-\\nsense reasoning over entity knowledge. OpenReview.\\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\\nPadmakumar, Johnny Ma, Jana Thompson, He He,\\net al. 2022. Quality: Question answering with long\\ninput texts, yes! In Proceedings of the 2022 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, pages 5336–5358.\\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li,\\nJosephina Hu, Hugh Zhang, Chen Bo Calvin Zhang,\\nMohamed Shaaban, John Ling, Sean Shi, et al.\\n2025.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='guage Technologies, pages 5336–5358.\\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li,\\nJosephina Hu, Hugh Zhang, Chen Bo Calvin Zhang,\\nMohamed Shaaban, John Ling, Sean Shi, et al.\\n2025.\\nHumanity’s last exam.\\narXiv preprint\\narXiv:2501.14249.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language\\nmodels. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2023, pages 5687–5711.\\nShuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai\\nJia, Huajun Chen, and Ningyu Zhang. 2024. Making\\nlanguage models better tool learners with execution\\nfeedback. In Proceedings of the 2024 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies (Volume 1: Long Papers), pages 3550–\\n3568.\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\\nBill Qian, et al. 2023. Toolllm: Facilitating large'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='3568.\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\\nBill Qian, et al. 2023. Toolllm: Facilitating large\\nlanguage models to master 16000+ real-world apis.\\narXiv preprint arXiv:2307.16789.\\nLeonardo Ranaldi, Marco Valentino, and Andrè Fre-\\nitas. 2024. Eliciting critical reasoning in retrieval-\\naugmented language models via contrastive explana-\\ntions. arXiv preprint arXiv:2410.22874.\\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jack-\\nson Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-\\nlian Michael, and Samuel R Bowman. 2024. Gpqa:\\nA graduate-level google-proof q&a benchmark. In\\nFirst Conference on Language Modeling.\\nAniruddha Salve, Saba Attar, Mahesh Deshmukh, Say-\\nali Shivpuje, and Arnab Mitra Utsab. 2024. A collab-\\norative multi-agent approach to retrieval-augmented\\ngeneration across diverse data.\\narXiv preprint\\narXiv:2412.05838.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='orative multi-agent approach to retrieval-augmented\\ngeneration across diverse data.\\narXiv preprint\\narXiv:2412.05838.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. 2023.\\nToolformer: Language models can teach themselves\\nto use tools. Advances in Neural Information Pro-\\ncessing Systems, 36:68539–68551.\\nSamuel Schmidgall, Yusheng Su, Ze Wang, Ximeng\\nSun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng\\nLiu, and Emad Barsoum. 2025.\\nAgent labora-\\ntory: Using llm agents as research assistants. arXiv\\npreprint arXiv:2501.04227.\\nThomas Schmied, Fabian Paischer, Vihang Patil,\\nMarkus Hofmarcher, Razvan Pascanu, and Sepp\\nHochreiter. 2024.\\nRetrieval-augmented decision\\ntransformer:\\nExternal memory for in-context rl.\\narXiv preprint arXiv:2410.07071.\\nJohannes Schneider. 2025. Generative to agentic ai:\\nSurvey, conceptualization, and challenges. arXiv\\npreprint arXiv:2504.18875.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Eva Sharma, Chen Li, and Lu Wang. 2019. Bigpatent:\\nA large-scale dataset for abstractive and coherent\\nsummarization. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, pages 2204–2213.\\nOla Shorinwa, Zhiting Mei, Justin Lidard, Allen Z Ren,\\nand Anirudha Majumdar. 2025. A survey on un-\\ncertainty quantification of large language models:\\nTaxonomy, open research challenges, and future di-\\nrections. ACM Computing Surveys.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote,\\nYonatan Bisk,\\nAdam Trischler,\\nand Matthew\\nHausknecht. Alfworld: Aligning text and embod-\\nied environments for interactive learning. In Interna-\\ntional Conference on Learning Representations.\\nHuatong Song, Jinhao Jiang, Yingqian Min, Jie Chen,\\nZhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-\\nRong Wen. 2025. R1-searcher: Incentivizing the\\nsearch capability in llms via reinforcement learning.\\narXiv preprint arXiv:2503.05592.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Rong Wen. 2025. R1-searcher: Incentivizing the\\nsearch capability in llms via reinforcement learning.\\narXiv preprint arXiv:2503.05592.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta,\\nAdrià Garriga-Alonso, et al. 2022.\\nBeyond the\\nimitation game: Quantifying and extrapolating the\\ncapabilities of language models.\\narXiv preprint\\narXiv:2206.04615.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu\\nZhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, An-\\ndrew Wen, Shaochen Zhong, Hanjie Chen, et al.\\n2025. Stop overthinking: A survey on efficient rea-\\nsoning for large language models. arXiv preprint\\narXiv:2503.16419.\\nChuanneng Sun, Songjun Huang, and Dario Pompili.\\n2024a. Retrieval-augmented hierarchical in-context\\nreinforcement learning and hindsight modular reflec-\\ntions for task planning with llms. arXiv preprint\\narXiv:2408.06520.\\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='reinforcement learning and hindsight modular reflec-\\ntions for task planning with llms. arXiv preprint\\narXiv:2408.06520.\\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.\\n2019. Pullnet: Open domain question answering\\nwith iterative retrieval on knowledge bases and text.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP), pages 2380–2390.\\nHao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan\\nHou, Yong Jiang, Pengjun Xie, Fei Huang, and Yan\\nZhang. 2025a. Zerosearch: Incentivize the search\\ncapability of llms without searching. arXiv preprint\\narXiv:2505.04588.\\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\\nWang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-\\nYeung Shum, and Jian Guo. 2024b. Think-on-graph:\\nDeep and responsible reasoning of large language\\nmodel on knowledge graph. In The Twelfth Interna-\\ntional Conference on Learning Representations.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Yeung Shum, and Jian Guo. 2024b. Think-on-graph:\\nDeep and responsible reasoning of large language\\nmodel on knowledge graph. In The Twelfth Interna-\\ntional Conference on Learning Representations.\\nQiang Sun, Tingting Bi, Sirui Li, Eun-Jung Holden,\\nPaul Duuring, Kai Niu, and Wei Liu. 2025b. Sym-\\nbioticrag: Enhancing document intelligence through\\nhuman-llm symbiotic collaboration. arXiv preprint\\narXiv:2505.02418.\\nZhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue\\nZang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang,\\nand Han Li. 2025c. Rearter: Retrieval-augmented\\nreasoning with trustworthy process rewarding. arXiv\\npreprint arXiv:2501.07861.\\nAlon Talmor and Jonathan Berant. 2018. The web as\\na knowledge-base for answering complex questions.\\nIn Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 641–651.\\nYixuan Tang and Yi Yang. 2024. Multihop-rag: Bench-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 641–651.\\nYixuan Tang and Yi Yang. 2024. Multihop-rag: Bench-\\nmarking retrieval-augmented generation for multi-\\nhop queries. arXiv preprint arXiv:2401.15391.\\nYicheng Tao, Haotian Liu, Shanwen Wang, and\\nHongteng Xu. 2025. Assisting mathematical for-\\nmalization with a learning-based premise retriever.\\narXiv preprint arXiv:2501.13959.\\nJames\\nThorne,\\nAndreas\\nVlachos,\\nChristos\\nChristodoulopoulos,\\nand\\nArpit\\nMittal.\\n2018.\\nFever: a large-scale dataset for fact extraction and\\nverification. arXiv preprint arXiv:1803.05355.\\nSM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vip-\\nula Rawte, Aman Chadha, and Amitava Das. 2024.\\nA comprehensive survey of hallucination mitigation\\ntechniques in large language models. arXiv preprint\\narXiv:2401.01313.\\nHieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang,\\nZhichao Yang, and Hong Yu. 2024. Rare: Retrieval-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='techniques in large language models. arXiv preprint\\narXiv:2401.01313.\\nHieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang,\\nZhichao Yang, and Hong Yu. 2024. Rare: Retrieval-\\naugmented reasoning enhancement for large lan-\\nguage models. arXiv preprint arXiv:2412.02830.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022.\\nmusique: Multi-\\nhop questions via single-hop question composition.\\nTransactions of the Association for Computational\\nLinguistics, 10:539–554.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2023. Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-\\nintensive multi-step questions. In Proceedings of the\\n61st Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n10014–10037.\\nNikolaos Tsantalis, Ameya Ketkar, and Danny Dig.\\n2020. Refactoringminer 2.0. IEEE Transactions\\non Software Engineering, 48(3):930–950.\\nBoxin Wang, Wei Ping, Lawrence Mcafee, Peng Xu,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='10014–10037.\\nNikolaos Tsantalis, Ameya Ketkar, and Danny Dig.\\n2020. Refactoringminer 2.0. IEEE Transactions\\non Software Engineering, 48(3):930–950.\\nBoxin Wang, Wei Ping, Lawrence Mcafee, Peng Xu,\\nBo Li, Mohammad Shoeybi, and Bryan Catanzaro.\\n2024a. Instructretro: Instruction tuning post retrieval-\\naugmented pretraining. In International Conference\\non Machine Learning, pages 51255–51272. PMLR.\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao\\nGao, Kexin Huang, Ziming Liu, Payal Chandak,\\nShengchao Liu, Peter Van Katwyk, Andreea Deac,\\net al. 2023. Scientific discovery in the age of artificial\\nintelligence. Nature, 620(7972):47–60.\\nJunjie Wang, Mingyang Chen, Binbin Hu, Dan Yang,\\nZiqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jin-\\njie Gu, Jun Zhou, et al. 2024b. Learning to plan\\nfor retrieval-augmented large language models from\\nknowledge graphs. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2024, pages\\n7813–7835.\\nSong Wang, Zihan Chen, Chengshuai Shi, Cong Shen,\\nand Jundong Li. 2024c. Mixture of demonstrations\\nfor in-context learning. Advances in Neural Informa-\\ntion Processing Systems, 37:88091–88116.\\nYaoting Wang, Shengqiong Wu, Yuecheng Zhang,\\nShuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao\\nFei. 2025a.\\nMultimodal chain-of-thought reason-\\ning:\\nA comprehensive survey.\\narXiv preprint\\narXiv:2503.12605.\\nYu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Fei. 2025a.\\nMultimodal chain-of-thought reason-\\ning:\\nA comprehensive survey.\\narXiv preprint\\narXiv:2503.12605.\\nYu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi\\nZhang, and Tyler Derr. 2024d. Knowledge graph\\nprompting for multi-document question answering.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 38, pages 19206–19214.\\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,\\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\\nAaran Arulraj, Xuan He, Ziyan Jiang, et al. 2025b.\\nMmlu-pro: A more robust and challenging multi-task\\nlanguage understanding benchmark. Advances in\\nNeural Information Processing Systems, 37:95266–\\n95290.\\nYujing Wang, Hainan Zhang, Liang Pang, Binghui\\nGuo, Hongwei Zheng, and Zhiming Zheng. 2025c.\\nMaferw: Query rewriting with multi-aspect feed-\\nbacks for retrieval-augmented large language models.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 39, pages 25434–25442.\\nZheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='backs for retrieval-augmented large language models.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 39, pages 25434–25442.\\nZheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and\\nWei Shi. 2024e. M-rag: Reinforcing large language\\nmodel performance through retrieval-augmented gen-\\neration with multiple partitions.\\nIn Proceedings\\nof the 62nd Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 1966–1978.\\nZhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen,\\nYu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang,\\nLinpeng Tang, Wentao Zhang, et al. 2025d. Rare:\\nRetrieval-augmented reasoning modeling.\\narXiv\\npreprint arXiv:2503.23513.\\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\\nHe, Zilong Zheng, Yaodong Yang, et al. 2024f.\\nJarvis-1: Open-world multi-task agents with memory-\\naugmented multimodal language models.\\nIEEE\\nTransactions on Pattern Analysis and Machine In-\\ntelligence.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Jarvis-1: Open-world multi-task agents with memory-\\naugmented multimodal language models.\\nIEEE\\nTransactions on Pattern Analysis and Machine In-\\ntelligence.\\nZihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xi-\\naojian Ma, and Yitao Liang. 2024g.\\nRat:\\nRe-\\ntrieval augmented thoughts elicit context-aware rea-\\nsoning in long-horizon generation. arXiv preprint\\narXiv:2403.05313.\\nJason Wei, Nguyen Karina, Hyung Won Chung,\\nYunxin Joy Jiao, Spencer Papay, Amelia Glaese, John\\nSchulman, and William Fedus. 2024. Measuring\\nshort-form factuality in large language models. arXiv\\npreprint arXiv:2411.04368.\\nJason Wei, Zhiqing Sun, Spencer Papay, Scott McK-\\ninney, Jeffrey Han, Isa Fulford, Hyung Won Chung,\\nAlex Tachard Passos, William Fedus, and Amelia\\nGlaese. 2025a. Browsecomp: A simple yet challeng-\\ning benchmark for browsing agents. arXiv preprint\\narXiv:2504.12516.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='ing benchmark for browsing agents. arXiv preprint\\narXiv:2504.12516.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\\net al. 2022. Chain-of-thought prompting elicits rea-\\nsoning in large language models. Advances in neural\\ninformation processing systems, 35:24824–24837.\\nJiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie\\nQiu, Wei Wei, Jinzhe Li, Wanli Ouyang, and Siqi\\nSun. 2025b. Alignrag: An adaptable framework for\\nresolving misalignments in retrieval-aware reasoning\\nof rag. arXiv preprint arXiv:2504.14858.\\nZhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi\\nShi, Zhen Huang, and Dongsheng Li. 2023. Grove: A\\nretrieval-augmented complex story generation frame-\\nwork with a forest of evidence. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2023, pages 3980–3998.\\nNirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawar-\\ndena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-\\nOrji, Ruvan Weerasinghe, Anne Liret, and Bruno'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='2023, pages 3980–3998.\\nNirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawar-\\ndena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-\\nOrji, Ruvan Weerasinghe, Anne Liret, and Bruno\\nFleisch. 2024. Cbr-rag: case-based reasoning for\\nretrieval augmented generation in llms for legal ques-\\ntion answering. In International Conference on Case-\\nBased Reasoning, pages 445–460. Springer.\\nFeijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding,\\nand Jing Gao. 2025a. Talk to right specialists: Rout-\\ning and planning in multi-agent system for question\\nanswering. arXiv preprint arXiv:2501.07813.\\nJialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang,\\nZekun Xi, Runnan Fang, Linhai Zhang, Yulan He,\\nDeyu Zhou, Pengjun Xie, et al. 2025b. Webwalker:\\nBenchmarking llms in web traversal. arXiv preprint\\narXiv:2501.07572.\\nJunde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025c. Agen-\\ntic reasoning: Reasoning llms with tools for the deep\\nresearch. arXiv preprint arXiv:2502.04644.\\nShirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Junde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025c. Agen-\\ntic reasoning: Reasoning llms with tools for the deep\\nresearch. arXiv preprint arXiv:2502.04644.\\nShirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang,\\nMichihiro Yasunaga, Kaidi Cao, Vassilis Ioannidis,\\nKarthik Subbian, Jure Leskovec, and James Y Zou.\\n2024. Avatar: Optimizing llm agents for tool us-\\nage via contrastive reasoning. Advances in Neural\\nInformation Processing Systems, 37:25981–26010.\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang,\\nand Wenjie Li. 2025a.\\nTokenskip: Controllable\\nchain-of-thought compression in llms. arXiv preprint\\narXiv:2502.12067.\\nYuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, and\\nHaifeng Huang. 2025b.\\nImproving retrieval aug-\\nmented language model with self-reasoning. In Pro-\\nceedings of the AAAI conference on artificial intelli-\\ngence, volume 39, pages 25534–25542.\\nGuangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang,\\nHaolin Liu, Yifan Yang, Fangyuan Chen, Zhix-\\ning Song, Dengyu Wang, Minjia Zhang, et al.\\n2025. Rag-gym: Optimizing reasoning and search\\nagents with process supervision.\\narXiv preprint\\narXiv:2502.13957.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2024. Crp-rag: A retrieval-\\naugmented generation framework for supporting\\ncomplex logical reasoning and knowledge planning.\\nElectronics, 14(1):47.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2025a. Crp-rag: A retrieval-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='complex logical reasoning and knowledge planning.\\nElectronics, 14(1):47.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2025a. Crp-rag: A retrieval-\\naugmented generation framework for supporting\\ncomplex logical reasoning and knowledge planning.\\nElectronics (2079-9292), 14(1).\\nRan Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C\\nHo, Haoyu Wang, and Carl Yang. 2025b. Collab-rag:\\nBoosting retrieval-augmented generation for complex\\nquestion answering via white-box and black-box llm\\ncollaboration. arXiv preprint arXiv:2504.04915.\\nChen Yang, Chenyang Zhao, Quanquan Gu, and Don-\\ngruo Zhou. 2024a. Cops: Empowering llm agents\\nwith provable cross-task experience sharing. arXiv\\npreprint arXiv:2410.16670.\\nRui Yang. 2024. Casegpt: a case reasoning framework\\nbased on language models and retrieval-augmented\\ngeneration. arXiv preprint arXiv:2407.07913.\\nWooseong Yang, Weizhi Zhang, Yuqing Liu, Yuwei\\nHan, Yu Wang, Junhyun Lee, and Philip S Yu. 2025.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='based on language models and retrieval-augmented\\ngeneration. arXiv preprint arXiv:2407.07913.\\nWooseong Yang, Weizhi Zhang, Yuqing Liu, Yuwei\\nHan, Yu Wang, Junhyun Lee, and Philip S Yu. 2025.\\nCold-start recommendation with knowledge-guided\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2505.20773.\\nXiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,\\nXiangsen Chen, Sajal Choudhary, Rongze Gui, Ziran\\nJiang, Ziyu Jiang, et al. 2024b. Crag-comprehensive\\nrag benchmark. Advances in Neural Information\\nProcessing Systems, 37:10470–10490.\\nYahe Yang and Chengyue Huang. 2025. Tree-based\\nrag-agent recommendation system: A case study in\\nmedical test data. arXiv preprint arXiv:2501.02727.\\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\\nWikiqa: A challenge dataset for open-domain ques-\\ntion answering.\\nIn Proceedings of the 2015 con-\\nference on empirical methods in natural language\\nprocessing, pages 2013–2018.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='tion answering.\\nIn Proceedings of the 2015 con-\\nference on empirical methods in natural language\\nprocessing, pages 2013–2018.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D Manning. 2018. Hotpotqa: A dataset for\\ndiverse, explainable multi-hop question answering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n2369–2380.\\nShunyu Yao, Howard Chen, John Yang, and Karthik\\nNarasimhan. 2022. Webshop: Towards scalable real-\\nworld web interaction with grounded language agents.\\nAdvances in Neural Information Processing Systems,\\n35:20744–20757.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\\n2023a. Tree of thoughts: Deliberate problem solving\\nwith large language models.\\nAdvances in neural\\ninformation processing systems, 36:11809–11822.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='with large language models.\\nAdvances in neural\\ninformation processing systems, 36:11809–11822.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.\\nReact: Synergizing reasoning and acting in language\\nmodels. In International Conference on Learning\\nRepresentations (ICLR).\\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\\nPercy Liang, and Jure Leskovec. 2021. Qa-gnn: Rea-\\nsoning with language models and knowledge graphs\\nfor question answering. In North American Chap-\\nter of the Association for Computational Linguistics\\n(NAACL).\\nJaeseok Yoo, Hojae Han, Youngwon Lee, Jaejin Kim,\\nand Seung-won Hwang. 2025. Perc: Plan-as-query\\nexample retrieval for underrepresented code genera-\\ntion. In Proceedings of the 31st International Con-\\nference on Computational Linguistics, pages 7982–\\n7997.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\\nBerant. 2024. Making retrieval-augmented language\\nmodels robust to irrelevant context. In ICLR 2024'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='7997.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\\nBerant. 2024. Making retrieval-augmented language\\nmodels robust to irrelevant context. In ICLR 2024\\nWorkshop on Large Language Model (LLM) Agents.\\nHong Qing Yu and Frank McQuade. 2025. Rag-kg-il:\\nA multi-agent hybrid framework for reducing halluci-\\nnations and enhancing llm reasoning through rag and\\nincremental knowledge graph learning integration.\\narXiv preprint arXiv:2503.13514.\\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Peixin\\nCao, Kaixin Ma, Jian Li, Hongwei Wang, and Dong\\nYu. 2024. Chain-of-note: Enhancing robustness in\\nretrieval-augmented language models. In Proceed-\\nings of the 2024 Conference on Empirical Methods in\\nNatural Language Processing, pages 14672–14685.\\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie\\nTang, Cuiping Li, and Hong Chen. 2022a. Subgraph\\nretrieval enhanced model for multi-hop knowledge\\nbase question answering. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='retrieval enhanced model for multi-hop knowledge\\nbase question answering. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 5773–\\n5784.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang,\\nDongjie Wang, and Kunpeng Liu. 2025a. Ratt: A\\nthought structure for coherent and correct llm reason-\\ning. In Proceedings of the AAAI Conference on Arti-\\nficial Intelligence, volume 39, pages 26733–26741.\\nJintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo,\\nShuofei Qiao, Lun Du, Da Zheng, Huajun Chen,\\nand Ningyu Zhang. 2025b.\\nLightthinker: Think-\\ning step-by-step compression.\\narXiv preprint\\narXiv:2502.15589.\\nMiao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang,\\nShuai Lu, Junfeng Jiao, and Tianyu Shi. 2025c. A\\ncascading cooperative multi-agent framework for on-\\nramp merging control integrating large language mod-\\nels. arXiv preprint arXiv:2503.08199.\\nNingning Zhang, Chi Zhang, Zhizhong Tan, Xingxing\\nYang, Weiping Deng, and Wenyong Wang. 2025d.\\nCredible plan-driven rag method for multi-hop ques-\\ntion answering. arXiv preprint arXiv:2504.16787.\\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Yang, Weiping Deng, and Wenyong Wang. 2025d.\\nCredible plan-driven rag method for multi-hop ques-\\ntion answering. arXiv preprint arXiv:2504.16787.\\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gon-\\nzalez. 2024a. Raft: Adapting language model to do-\\nmain specific rag. In First Conference on Language\\nModeling.\\nWeizhi\\nZhang,\\nYuanchen\\nBei,\\nLiangwei\\nYang,\\nHenry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui\\nLi, Hao Chen, Jianling Wang, Yu Wang, et al. 2025e.\\nCold-start recommendation towards the era of large\\nlanguage models (llms): A comprehensive survey\\nand roadmap. arXiv preprint arXiv:2501.01945.\\nWeizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo,\\nGuancheng Wan, Liangwei Yang, Chenxuan Xie,\\nYuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al.\\n2025f. From web search towards agentic deep re-\\nsearch: Incentivizing search with reasoning agents.\\narXiv preprint arXiv:2506.18959.\\nWeizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liang-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='2025f. From web search towards agentic deep re-\\nsearch: Incentivizing search with reasoning agents.\\narXiv preprint arXiv:2506.18959.\\nWeizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liang-\\nwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng\\nZou, Zijie Huang, Zhengyang Wang, Yifan Gao,\\net al. 2025g. Personaagent: When large language\\nmodel agents meet personalization at test time. arXiv\\npreprint arXiv:2506.06254.\\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\\nHongyu Ren, Percy Liang, Christopher D Manning,\\nand Jure Leskovec. 2022b. Greaselm: Graph rea-\\nsoning enhanced language models. In International\\nConference on Learning Representations.\\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang\\nXu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai,\\nShuo Wang, Zhiyuan Liu, et al. 2024b. ∞bench:\\nExtending long context evaluation beyond 100k to-\\nkens. In Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 15262–15277.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Extending long context evaluation beyond 100k to-\\nkens. In Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 15262–15277.\\nYusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister,\\nRui Zhang, and Sercan Arik. 2024c. Chain of agents:\\nLarge language models collaborating on long-context\\ntasks. Advances in Neural Information Processing\\nSystems, 37:132208–132237.\\nZhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang\\nShi, Meng Han, Yongkang Wu, Ruofei Lai, and Zhao\\nCao. 2023. Iag: Induction-augmented generation\\nframework for answering reasoning questions. arXiv\\npreprint arXiv:2311.18397.\\nSiyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He,\\nLuna K Qiu, and Lili Qiu. 2024a. Retrieval aug-\\nmented generation (rag) and beyond: A comprehen-\\nsive survey on how to make your llms use external\\ndata more wisely. arXiv preprint arXiv:2409.14924.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='sive survey on how to make your llms use external\\ndata more wisely. arXiv preprint arXiv:2409.14924.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\\nsurvey of large language models.\\narXiv preprint\\narXiv:2303.18223, 1(2).\\nXiaoyan Zhao, Lingzhi Wang, Zhanghao Wang, Hong\\nCheng, Rui Zhang, and Kam-Fai Wong. 2024b.\\nPacar: Automated fact-checking with planning and\\ncustomized action reasoning using large language\\nmodels.\\nIn Proceedings of the 2024 Joint In-\\nternational Conference on Computational Linguis-\\ntics, Language Resources and Evaluation (LREC-\\nCOLING 2024), pages 12564–12573.\\nXinping Zhao, Dongfang Li, Yan Zhong, Boren Hu,\\nYibin Chen, Baotian Hu, and Min Zhang. 2024c.\\nSeer: Self-aligned evidence extraction for retrieval-\\naugmented generation. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 3027–3041.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Seer: Self-aligned evidence extraction for retrieval-\\naugmented generation. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 3027–3041.\\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu.\\n2021. Minif2f: a cross-system benchmark for for-\\nmal olympiad-level mathematics.\\narXiv preprint\\narXiv:2109.00110.\\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai,\\nLyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025.\\nDeepresearcher: Scaling deep research via reinforce-\\nment learning in real-world environments. arXiv\\npreprint arXiv:2504.03160.\\nJiawei Zhou and Lei Chen. 2025. Openrag: Optimiz-\\ning rag end-to-end via in-context retrieval learning.\\narXiv preprint arXiv:2503.08398.\\nPeilin Zhou, Bruce Leon, Xiang Ying, Can Zhang,\\nYifan Shao, Qichen Ye, Dading Chong, Zhiling\\nJin, Chenxuan Xie, Meng Cao, et al. 2025a.\\nBrowsecomp-zh: Benchmarking web browsing abil-\\nity of large language models in chinese.\\narXiv\\npreprint arXiv:2504.19314.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 18}, page_content='Jin, Chenxuan Xie, Meng Cao, et al. 2025a.\\nBrowsecomp-zh: Benchmarking web browsing abil-\\nity of large language models in chinese.\\narXiv\\npreprint arXiv:2504.19314.\\nYifei Zhou, Song Jiang, Yuandong Tian, Jason Weston,\\nSergey Levine, Sainbayar Sukhbaatar, and Xian Li.\\n2025b.\\nSweet-rl: Training multi-turn llm agents\\non collaborative reasoning tasks.\\narXiv preprint\\narXiv:2503.15478.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 19}, page_content='Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen,\\nBinyu Xie, Zheng Liu, Ji-Rong Wen, and Zhicheng\\nDou. 2024. Inters: Unlocking the power of large\\nlanguage models in search with instruction tuning.\\nIn Proceedings of the 62nd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), pages 2782–2809.\\nHenry Peng Zou, Wei-Chieh Huang, Yaozu Wu,\\nYankai Chen, Chunyu Miao, Hoang Nguyen, Yue\\nZhou, Weizhi Zhang, Liancheng Fang, Langzhou\\nHe, et al. 2025.\\nA survey on large language\\nmodel based human-agent systems. arXiv preprint\\narXiv:2505.00753.\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='A\\nFull Benchmark\\nSection 6 introduces representative benchmarks for\\ndifferent RAG-reasoning tasks. This appendix com-\\nplements that discussion with a comprehensive list\\nof benchmarks organized by task and domain. Ta-\\nble 2 details each benchmark’s attributes, including\\nthe publication venue, code repository, task cate-\\ngory, domain, primary knowledge sources, knowl-\\nedge type, and reasoning capabilities. By consoli-\\ndating these attributes into a single table, we facili-\\ntate the selection and comparison of benchmarks,\\nenabling researchers to identify the most suitable\\ndatasets for future studies on RAG-enhanced rea-\\nsoning.\\nOur benchmark compilation is primarily derived\\nfrom the methods surveyed in Sections 3 to 5 of\\nthis paper, with a particular focus on synergized\\napproaches discussed in Section 5. We deliber-\\nately targeted benchmarks that require both exter-\\nnal knowledge retrieval and internal deep reason-\\ning, as this dual requirement reflects real-world'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='approaches discussed in Section 5. We deliber-\\nately targeted benchmarks that require both exter-\\nnal knowledge retrieval and internal deep reason-\\ning, as this dual requirement reflects real-world\\nscenarios where models must not only access rel-\\nevant information but also integrate and reason\\nover it effectively. For example, in the QA do-\\nmain, we include datasets that necessitate synthe-\\nsizing evidence across multiple documents to an-\\nswer questions that cannot be resolved through\\nsingle-sentence retrieval. HotpotQA (Yang et al.,\\n2018) exemplifies this challenge, requiring reason-\\ning across different Wikipedia articles. In coding\\ntasks, benchmarks such as LiveCodeBench (Jain\\net al., 2024) and Refactoring Oracle (Tsantalis et al.,\\n2020) extend beyond pure algorithmic problem-\\nsolving by demanding retrieval of external code\\nsnippets and documentation. Similarly, in mathe-\\nmatics, benchmarks like MATH (Hendrycks et al.,\\n2021) and AQUA-RAT (Das et al., 2024) assess'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='solving by demanding retrieval of external code\\nsnippets and documentation. Similarly, in mathe-\\nmatics, benchmarks like MATH (Hendrycks et al.,\\n2021) and AQUA-RAT (Das et al., 2024) assess\\nnot only computational proficiency but also the re-\\ntrieval of relevant theorems and formulas, testing\\nthe model’s ability to integrate external mathemati-\\ncal knowledge with internal reasoning processes.\\nIn addition to established benchmarks, we have\\nincorporated newer and more challenging datasets\\nthat better mirror real-world applications. These\\ndatasets often demand extensive retrieval processes\\ncombined with expert-level or domain-specific\\nreasoning, as seen in Humanity’s Last Exam\\n(HLE) (Phan et al., 2025) and web search evalua-\\ntion tasks like BrowseComp (Wei et al., 2025a).\\nOverall, our collection encompasses 46 bench-\\nmarks covering 13 distinct tasks across 12 domains,\\neach explicitly annotated with features such as\\nknowledge source, knowledge type, and reasoning'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='Overall, our collection encompasses 46 bench-\\nmarks covering 13 distinct tasks across 12 domains,\\neach explicitly annotated with features such as\\nknowledge source, knowledge type, and reasoning\\ncapacity. This breadth ensures coverage of diverse\\ndomains and task types, forming a solid foundation\\nfor evaluating the interplay between retrieval and\\nreasoning in RAG systems.\\nWithin this benchmark set, single-hop QA\\ndatasets like TriviaQA (Joshi et al., 2017) focus on\\nprecise retrieval and fact recall, requiring models\\nto locate and synthesize a single piece of evidence.\\nIn contrast, multi-hop QA benchmarks such as Hot-\\npotQA (Yang et al., 2018) and MuSiQue (Trivedi\\net al., 2022) challenge models to chain information\\nfrom multiple documents and employ deductive\\nreasoning to bridge disparate facts into coherent\\nanswers. Structured knowledge benchmarks, such\\nas GraphQA (He et al., 2024c), require reasoning\\nover relational graph representations, integrating'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='reasoning to bridge disparate facts into coherent\\nanswers. Structured knowledge benchmarks, such\\nas GraphQA (He et al., 2024c), require reasoning\\nover relational graph representations, integrating\\nnodes and edges to resolve complex queries be-\\nyond plain text retrieval. Complementing these\\nopen-ended tasks, multiple-choice evaluations like\\nMMLU-Pro (Wang et al., 2025b) test domain-\\nspecific knowledge in areas such as science, history,\\nor law, assessing the model’s ability to perform\\nvarious reasoning styles, including inductive and\\nabductive inference. Multimodal QA benchmarks,\\nlike WebShop (Yao et al., 2022), test a model’s\\ncapacity to align textual and visual information\\nto determine the correct answer. Long-form QA\\ndatasets such as ∞BENCH (Zhang et al., 2024b)\\nevaluate models’ ability to maintain logical consis-\\ntency and perform inductive reasoning over lengthy\\ncontexts. Collectively, these benchmarks establish\\na comprehensive evaluation chain for systemati-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 20}, page_content='tency and perform inductive reasoning over lengthy\\ncontexts. Collectively, these benchmarks establish\\na comprehensive evaluation chain for systemati-\\ncally assessing RAG-reasoning capabilities.\\nBeyond text-based QA, RAG-augmented bench-\\nmarks span diverse tasks involving long-form\\ngeneration, interactive reasoning, and domain-\\nspecific challenges in mathematics and pro-\\ngramming.\\nMathematics benchmarks such as\\nMATH (Hendrycks et al., 2021) draw from\\ncompetition-level problems to assess arithmetic\\nand symbolic reasoning.\\nSummarization tasks\\nlike XSum (Narayan et al., 2018) evaluate a\\nmodel’s ability to condense entire news articles\\ninto concise summaries while preserving factual\\ncorrectness.\\nFact-checking benchmarks, such\\nas FEVER (Thorne et al., 2018), test the ca-\\npacity for evidence retrieval and claim verifica-\\ntion. Code-focused evaluations, including Live-\\nCodeBench (Jain et al., 2024), examine deductive\\nand abductive reasoning in the context of algo-\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Dataset\\nVenue\\nResource\\nTask\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning Capability\\nSize\\nInput\\nOutput\\nCode\\nLiveCodeBench (Jain\\net al., 2024)\\nArxiv’24\\nLink\\nCode\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n1,055\\nQuestion/Text, Code,\\nInstruction\\nCode Instance, Test\\nOutput\\nRefactoring Oracle\\n(Tsantalis et al., 2020)\\nIEEE’22\\nLink\\nCode\\nSoftware\\nInternet, Human\\nLogical\\nDeductive\\n7,226\\nCode, Instruction\\nCode Instance\\nColBench (Zhou et al.,\\n2025b)\\nArxiv’25\\nLink\\nCode\\nSoftware\\nLLM, Human\\nLogical\\nAbductive, Inductive\\n10,000+\\nQuestion/Text,\\nLinks/Sources, Code\\nCode Instance\\nMath\\nMATH (Hendrycks\\net al., 2021)\\nNeurIPS’21\\nLink\\nDomain-specific\\nQA\\nMath\\nExam/Competition\\nLogical, Arithmetic\\nDeductive\\n12,500\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nMiniF2F (Zheng et al.,\\n2021)\\nICLR’22\\nLink\\nDomain-specific\\nQA\\nMath\\nExam/Competition,\\nBooks\\nLogical, Arithmetic\\nDeductive\\n488\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nAQuA (Ling et al.,\\n2017)\\nArxiv’17\\nLink\\nDomain-specific\\nQA\\nMath'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Link\\nDomain-specific\\nQA\\nMath\\nExam/Competition,\\nBooks\\nLogical, Arithmetic\\nDeductive\\n488\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nAQuA (Ling et al.,\\n2017)\\nArxiv’17\\nLink\\nDomain-specific\\nQA\\nMath\\nPrevious Source,\\nExam/Competition,\\nInternet\\nArithmetic, Logical\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equations\\nNatural Language,\\nOptions/Labels\\nFact Checking\\nCRAG (Yang et al.,\\n2024b)\\nNeurIPS’24\\nLink\\nFact Checking\\nGeneral\\nInternet\\nCommonsense\\nDeductive, Abductive\\n4,409\\nQuestion/Text\\nNatural Language\\nCREAK (Onoe et al.,\\n2021)\\nNeurIPS’21\\nLink\\nFact Checking\\nGeneral\\nHuman\\nCommonsense\\nDeductive, Abductive,\\nAnalogical\\n13,000\\nQuestion/Text\\nOptions/Labels,\\nNatural Language\\nFever (Thorne et al.,\\n2018)\\nACL’18\\nLink\\nFact Checking\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n185,445\\nQuestion/Text,\\nLinks/Sources\\nNatural Language,\\nOptions/Labels\\nPubHealth (Kotonya\\nand Toni, 2020)\\nEMNLP’20\\nLink\\nFact Checking\\nHealth\\nInternet\\nCommonsense,\\nLogical\\nAbductive, Deductive\\n11,800\\nQuestion/Text'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Links/Sources\\nNatural Language,\\nOptions/Labels\\nPubHealth (Kotonya\\nand Toni, 2020)\\nEMNLP’20\\nLink\\nFact Checking\\nHealth\\nInternet\\nCommonsense,\\nLogical\\nAbductive, Deductive\\n11,800\\nQuestion/Text\\nNatural Language,\\nOptions\\nGraph QA\\nGraphQA (He et al.,\\n2024c)\\nNeurIPS’24\\nLink\\nGraph QA\\nGeneral\\nPrevious Source\\nCommonsense,\\nMultimodal\\nDeductive, Abductive\\n107,503\\nQuestion/Text\\nNatural Language\\nGRBENCH (Jin et al.,\\n2024)\\nACL’24\\nLink\\nGraph QA\\nGeneral\\nLLM, Human\\nLogical\\nDeductive, Inductive\\n1,740\\nQuestion/Text\\nNatural Language\\nLong-form QA\\n∞BENCH (Zhang\\net al., 2024b)\\nArxiv’24\\nLink\\nLong-form QA\\nGeneral\\nInternet, Human\\nMultimodal, Logical Inductive, Abductive\\n3,946\\nQuestion/Text, Code,\\nEquations\\nNatural Language,\\nNumber, Code\\nInstance\\nMultimodal QA\\nCrisisMMD (Alam\\net al., 2018)\\nArxiv’18\\nLink\\nMultimodal QA Crisis Response\\nMedia, Internet\\nCommonsense,\\nMultimodal\\nAbductive\\n16,097\\nQuestion/Text,\\nFigure/Image\\nOptions, Natural\\nLanguage\\nALFWORLD (Shridhar\\net al.)\\nICLR’21\\nLink\\nMultimodal QA\\nGame'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Multimodal QA Crisis Response\\nMedia, Internet\\nCommonsense,\\nMultimodal\\nAbductive\\n16,097\\nQuestion/Text,\\nFigure/Image\\nOptions, Natural\\nLanguage\\nALFWORLD (Shridhar\\net al.)\\nICLR’21\\nLink\\nMultimodal QA\\nGame\\nPrevious Source\\nMultimodal\\nDeductive, Abductive\\n3,827\\nQuestion/Text,\\nFigure/Image\\nNatural Language\\nMMLongBench-DOC\\n(Ma et al., 2025)\\nNeurIPS’24\\nLink\\nMultimodal QA\\nNarrative\\nPrevious Source,\\nInternet\\nMultimodal\\nDeductive, Abductive\\n1,082\\nFigure/Image,\\nQuestion/Text,\\nDocuments\\nNatural Language,\\nNumber\\nLongDocURL (Deng\\net al., 2024)\\nArxiv’24\\nLink\\nMultimodal QA\\nNarrative\\nInternet, Previous\\nSource, LLM\\nMultimodal\\nDeductive, Abductive\\n2,325\\nFigure/Image,\\nQuestion/Text,\\nDocuments\\nNatural Language,\\nNumber\\nUDA (Hui et al., 2024)\\nNIPS’24\\nLink\\nMultimodal QA\\nNarrative\\nInternet,\\nPaper/Report\\nMultimodal\\nDeductive\\n29,590\\nDocuments,\\nQuestion/Text\\nNatural Language,\\nNumber\\nSCIENCEQA (Lu et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nScience\\nHuman\\nLogical, Multimodal\\nDeductive\\n21,000\\nQuestion/Text,\\nOptions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 21}, page_content='Deductive\\n29,590\\nDocuments,\\nQuestion/Text\\nNatural Language,\\nNumber\\nSCIENCEQA (Lu et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nScience\\nHuman\\nLogical, Multimodal\\nDeductive\\n21,000\\nQuestion/Text,\\nOptions,\\nFigure/Image\\nOptions, Natural\\nLanguage, Number\\nWebShop (Yao et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nE-commerce\\nInternet\\nMultimodal\\nInductive, Abductive\\n12,087\\nInstruction,\\nQuestion/Text\\nNatural Language,\\nImage/Figure\\nSurgCoTBench (Low\\net al., 2025)\\nArxiv’25\\n—\\nMultimodal QA\\nHealth\\nHuman\\nMultimodal, Logical Abductive, Deductive\\n14,176\\nQuestion/Text,\\nFigure/Image,\\nOptions\\nOptions, Natural\\nLanguage, Number\\nTable 2: Full representative knowledge and reasoning intensive benchmarks across diverse task categories (Part 1).\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Dataset\\nVenue\\nResource\\nTask\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning Capability\\nSize\\nInput\\nOutput\\nMulti-choice QA\\nBamboogle (Press et al.,\\n2023)\\nEMNLP’23\\nLink\\nMulti-choice QA\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n125\\nQuestion/Text\\nNatural Language\\nBIG-Bench (Srivastava\\net al., 2022)\\nArxiv’22\\nLink\\nMulti-choice QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive, Abductive,\\nInductive, Analogical\\n204\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber,\\nOptions/Labels\\nADQA (Li et al.,\\n2024a)\\nEMNLP’24\\nLink\\nMulti-choice QA\\nHealth\\nPrevious Source\\nCommonsense,\\nLogical\\nDeductive, Abductive\\n446\\nQuestion/Text,\\nOptions\\nOptions\\nQuALITY (Pang et al.,\\n2022)\\nNAACL’22\\nLink\\nMulti-choice QA\\nNarrative\\nBooks\\nCommonsense,\\nLogical\\nDeductive, Abductive\\n6,737\\nQuestion/Text,\\nOptions\\nOptions\\nMMLU-Pro (Wang\\net al., 2025b)\\nNeurIPS’24\\nLink\\nMulti-choice QA\\nScience\\nPrevious Source,\\nInternet\\nArithmetic,\\nCommonsense,\\nLogical\\nDeductive, Inductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='et al., 2025b)\\nNeurIPS’24\\nLink\\nMulti-choice QA\\nScience\\nPrevious Source,\\nInternet\\nArithmetic,\\nCommonsense,\\nLogical\\nDeductive, Inductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options\\nMulti-hop QA\\nFRAMES (Krishna\\net al., 2024)\\nArxiv’24\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical, Arithmetic\\nDeductive\\n824\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al.,\\n2018)\\nEMNLP’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nGPQA (Rein et al.,\\n2024)\\nArxiv’24\\nLink\\nMulti-hop QA\\nScience\\nHuman\\nLogical\\nDeductive, Abductive\\n448\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options\\nHLE (Phan et al., 2025)\\nArxiv’25\\nLink\\nMulti-hop QA\\nScience\\nHuman\\nLogical, Arithmetic,\\nMultimodal\\nDeductive, Abductive\\n2,500\\nQuestion/Text,\\nOptions,\\nFigure/Image\\nNatural Language,\\nNumber, Options\\nCWQ (Talmor and\\nBerant, 2018)\\nNAACL’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n34,689\\nQuestion/Text\\nNatural Language'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Options,\\nFigure/Image\\nNatural Language,\\nNumber, Options\\nCWQ (Talmor and\\nBerant, 2018)\\nNAACL’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n34,689\\nQuestion/Text\\nNatural Language\\nIIRC (Ferguson et al.,\\n2020)\\nEMNLP’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n13,000+\\nQuestion/Text,\\nLinks/Sources\\nNumber, Natural\\nLanguage\\nMINTQA (He et al.,\\n2024b)\\nArxiv’24\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n10,479\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al.,\\n2022)\\nACL’22\\nLink\\nMulti-hop QA\\nGeneral\\nPrevious Source,\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n25,000\\nQuestion/Text\\nNatural Language\\nTopiOCQA (Adlakha\\net al., 2022)\\nTACL’22\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n54,494\\nQuestion/Text\\nNatural Language\\n2WikiMultiHopQA (Ho\\net al., 2020)\\nCOLING’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nMulti-step QA\\nStrategyQA (Geva\\net al., 2021)\\nTACL’21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='et al., 2020)\\nCOLING’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nMulti-step QA\\nStrategyQA (Geva\\net al., 2021)\\nTACL’21\\nLink\\nMulti-step QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n2,780\\nQuestion/Text\\nNatural Language\\nSingle-hop QA\\nSimpleQA (Wei et al.,\\n2024)\\nArxiv’24\\nLink\\nSingle-hop QA\\nGeneral\\nLLM, Human\\nCommonsense\\nDeductive\\n4,326\\nQuestion/Text\\nNatural Language\\nTriviaQA (Joshi et al.,\\n2017)\\nACL’17\\nLink\\nSingle-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n650,000+\\nQuestion/Text\\nNatural Language\\nNQ (Kwiatkowski et al.,\\n2019)\\nACL’19\\nLink\\nSingle-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n307,373\\nQuestion/Text\\nNatural Language\\nText Summarization\\nXSum (Narayan et al.,\\n2018)\\nEMNLP’18\\nLink\\nText\\nSummarization\\nNarrative\\nInternet, Media\\nLogical,\\nCommonsense\\nAbductive\\n226,711\\nQuestion/Text\\nNatural Language\\nBIGPATENT (Sharma\\net al., 2019)\\nACL’19\\nLink\\nText\\nSummarization\\nPatent\\nInternet\\nCommonsense,\\nLogical'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Narrative\\nInternet, Media\\nLogical,\\nCommonsense\\nAbductive\\n226,711\\nQuestion/Text\\nNatural Language\\nBIGPATENT (Sharma\\net al., 2019)\\nACL’19\\nLink\\nText\\nSummarization\\nPatent\\nInternet\\nCommonsense,\\nLogical\\nAbductive\\n1.3 M\\nQuestion/Text\\nNatural Language\\nWeb Browsing\\nBrowseComp (Wei\\net al., 2025a)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, Internet\\nCommonsense,\\nLogical\\nDeductive\\n1,266\\nQuestion/Text\\nNatural Language\\nBrowseComp-ZH\\n(Zhou et al., 2025a)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, Internet\\nCommonsense,\\nLogical\\nDeductive\\n289\\nQuestion/Text\\nNatural Language\\nGAIA (Mialon et al.,\\n2023)\\nICLR’23\\nLink\\nWeb Browsing\\nGeneral\\nInternet, TooL\\nCommonsense,\\nLogical\\nDeductive\\n466\\nQuestion/Text,\\nImage/File/Code\\nNatural Language\\nWebWalkerQA (Wu\\net al., 2025b)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, LLM\\nCommonsense,\\nLogical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nDialog\\nDailyDialog (Li et al.,\\n2017)\\nArxiv’17\\nLink\\nDialog\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\n–\\n13,118\\nQuestion/Text\\nNatural Language'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 22}, page_content='Logical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nDialog\\nDailyDialog (Li et al.,\\n2017)\\nArxiv’17\\nLink\\nDialog\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\n–\\n13,118\\nQuestion/Text\\nNatural Language\\nTable 3: Full epresentative knowledge and reasoning intensive benchmarks across diverse task categories (Part 2,\\ncontinued).\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='Benchmark\\nDomain\\nPrimary Retrieval Challenge\\nPrimary Reasoning Challenge\\nTriviaQA, NQ\\nGeneral\\nScale & Noise: Retrieval from massive, noisy cor-\\npora.\\nAmbiguity: Handling real-world queries that are of-\\nten underspecified or ambiguous.\\nHotpotQA,\\n2WikiMultiHopQA,\\nMuSiQue, HLE\\nGeneral\\nMulti-document / High-dependency Synthesis: Re-\\nquires finding and connecting evidence scattered\\nacross multiple Wikipedia articles.\\nMulti-hop Deduction: Explicitly designed to test\\nthe ability to link two or more discrete facts into a\\ncoherent reasoning path.\\nMMLU-Pro, QUALITY\\nScience, Narrative\\nExpert-level Retrieval: Requires accessing deep spe-\\ncialized knowledge from academic or densely written\\nnarrative sources.\\nComplex & Long-form Reasoning: MMLU-Pro de-\\nmands expert-level problem-solving over rote memo-\\nrization. QUALITY uniquely requires comprehension\\nof very long texts (often >5,000 tokens).\\nMATH, AQUA-RAT\\nMath\\nFormal Knowledge Retrieval: Locating precise'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='rization. QUALITY uniquely requires comprehension\\nof very long texts (often >5,000 tokens).\\nMATH, AQUA-RAT\\nMath\\nFormal Knowledge Retrieval: Locating precise\\nmathematical theorems, lemmas, or formulas in for-\\nmal corpora.\\nSymbolic & Deductive Reasoning: Involves per-\\nforming precise, multi-step logical and algebraic oper-\\nations where each step must be correct. AQUA-RAT\\nis unique in providing natural language rationales,\\nthus testing the model’s ability to explain its formal\\nreasoning.\\nLiveCodeBench\\nCode\\nStructural & Modal Heterogeneity: Must retrieve\\nfrom diverse, heterogeneous sources such as code\\nrepositories, documentation, and community forums\\nlike Stack Overflow.\\nTool Use & Self-correction Reasoning: Requires ap-\\nplying retrieved code snippets/APIs, executing code,\\nand reasoning based on test outputs to debug and iter-\\natively improve solutions.\\nBrowseComp,\\nWebWalkerQA\\nGeneral (Web)\\nDynamism, Interactivity, and Long-tail Retrieval:'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='and reasoning based on test outputs to debug and iter-\\natively improve solutions.\\nBrowseComp,\\nWebWalkerQA\\nGeneral (Web)\\nDynamism, Interactivity, and Long-tail Retrieval:\\nTests agentic planning and tool use in live, unstruc-\\ntured web environments. BrowseComp requires cre-\\native, persistent navigation to locate hard-to-find, in-\\ntertwined information, while WebWalkerQA focuses\\non systematic traversal of a website’s subpages.\\nAgentic & Strategic Reasoning: Requires planning\\nand executing multi-step strategies (e.g., searching,\\nclicking, extracting) in dynamic and unpredictable\\ncontexts to achieve a defined goal.\\nTable 4: The primary retrieval and reasoning challenges for different RAG-Reasoning benchmarks.\\nrithmic problem-solving. Web-based tasks, exem-\\nplified by BrowseComp (Wei et al., 2025a), emu-\\nlate real-world search behavior, requiring iterative\\nquery formulation and navigation across multiple\\nwebpages.\\nIn addition to cataloging datasets, Table 4 pro-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='late real-world search behavior, requiring iterative\\nquery formulation and navigation across multiple\\nwebpages.\\nIn addition to cataloging datasets, Table 4 pro-\\nvides a synthesized overview of the primary re-\\ntrieval and reasoning challenges associated with\\neach benchmark discussed in this survey. This\\ncomparative analysis reveals critical gaps in cur-\\nrent benchmark coverage that future research must\\naddress. From a domain perspective, most bench-\\nmarks still focus on a limited set of general or\\nacademic scenarios, with few tackling real-world,\\nrealistic industrial or vertical-domain tasks where\\nretrieval sources might be personalized, proprietary\\nor highly specialized. Regarding retrieval capa-\\nbilities, existing benchmarks rarely test systems’\\nability to handle heterogeneous or multimodal con-\\ntent, nor do they systematically evaluate robust-\\nness against noisy, evolving, or conflicting infor-\\nmation within a unified framework for trustworthi-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='tent, nor do they systematically evaluate robust-\\nness against noisy, evolving, or conflicting infor-\\nmation within a unified framework for trustworthi-\\nness. In terms of reasoning capabilities, current\\nbenchmarks primarily assess deductive reasoning,\\nleaving underexplored more complex forms such\\nas deep causal reasoning, counterfactual thinking,\\ndecision-oriented reasoning, or analogical reason-\\ning in specialized domains. Moreover, there is a\\nlack of standardized benchmarks and metrics for\\nevaluating the entire reasoning-retrieval trajectory,\\nincluding the efficiency of retrieval steps, the qual-\\nity of intermediate queries, and the logical consis-\\ntency of multi-step reasoning chains.\\nB\\nDeep Research Implementations\\nIn this section, we extend the discussion of the\\nagentic paradigm introduced in Section 5.2, in\\nwhich RAG systems adopt the role of active re-\\nsearchers who plan multistep queries, interleave\\nretrieval with reasoning, and coordinate specialized'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='agentic paradigm introduced in Section 5.2, in\\nwhich RAG systems adopt the role of active re-\\nsearchers who plan multistep queries, interleave\\nretrieval with reasoning, and coordinate specialized\\ntools or agents. These characteristics collectively\\ndefine what we refer to as deep research, represent-\\ning the ability of a system to autonomously break\\ndown complex questions, iteratively gather diverse\\nevidence, and synthesize information through mul-\\ntiple reasoning steps. This paradigm seeks to en-\\nhance autonomy, reduce hallucinations, and im-\\nprove factual accuracy in open-domain tasks.\\nSuch deep research systems can be realized\\nthrough either single-agent or multi-agent archi-\\ntectures. Single-agent systems rely on a single\\nmodel to manage the entire process of question\\ndecomposition, retrieval, and synthesis, offering\\nsimplicity and shared context but facing limita-\\ntions in handling highly specialized or multi-modal\\ntasks. In contrast, multi-agent systems distribute'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 23}, page_content='simplicity and shared context but facing limita-\\ntions in handling highly specialized or multi-modal\\ntasks. In contrast, multi-agent systems distribute\\nthese responsibilities among specialized agents, en-\\nabling modularity and potentially greater robust-\\nness. However, this collaborative design introduces\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Name\\nBase Model\\nOptimizationReward\\nRetriever\\nAgent\\nArchitecture\\nTrain Data\\nEvaluation Data\\nLink\\nAgentic Reasoning\\n(Wu et al., 2025c)\\nN/A\\nPrompting\\nN/A\\nWeb Search\\nCentralized\\nN/A\\nGPQA\\nLink\\ngpt-researcher\\nPrompting\\nN/A\\nWeb Search,\\nLocal Retrieval\\nCentralized\\nN/A\\nN/A\\nLink\\ndeep-searcher\\nDeepseek, , Claude,\\nGemini, Qwen\\nPrompting\\nN/A\\nWeb Search\\nHierarchical\\nN/A\\nN/A\\nLink\\nSearch-R1 (Jin\\net al., 2025)\\nQwen2.5-7B-Instruct,\\nQwen2.5-7B-Base,\\nQwen-2.5-3B-Instruct,\\nQwen-2.5-3B-Base\\nGRPO,\\nPPO\\nExact\\nMatch\\nWeb Search\\nSingle\\nNQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nZeroSearch (Sun\\net al., 2025a)\\nQwen2.5-3B-Base,\\nQwen2.5-7B-Base,\\nQwen2.5-7B-Instruct,\\nQwen2.5-3B-Instruct,\\nLLaMA3.2-3B-Instruct,\\nLLaMA3.2-3B-Base\\nGRPO,\\nPPO,\\nReinforce\\nExact\\nMatch\\nWeb Search\\nSingle\\nNQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nWebthinker (Li\\net al., 2025c)\\nGPT-o1, GPT-o3,\\nDeepseek-R1, QwQ-32B,\\nQwen2.5-32B-Instruct\\nDPO\\nPreference\\nPairs'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='NQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nWebthinker (Li\\net al., 2025c)\\nGPT-o1, GPT-o3,\\nDeepseek-R1, QwQ-32B,\\nQwen2.5-32B-Instruct\\nDPO\\nPreference\\nPairs\\nWeb Search\\nSingle\\nSuperGPQA,\\nWebWalkerQA,\\nOpenThoughts,\\nNaturalReasoning,\\nNuminaMath\\nGPQA, GAIA, WebWalkerQA,\\nHumanity’s Last Exam\\nLink\\nnanoDeepResearch\\nOpenAI series, Claude\\nPrompting\\nN/A\\nWeb Search\\nCentralized\\nN/A\\nN/A\\nLink\\nDeerFlow\\nQwen,\\nPrompting\\nN/A\\nWeb Search\\nDecentralized\\nN/A\\nN/A\\nLink\\ndeep-research\\nDeepseek,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nopen-deep-research\\nOpenAI series, Deepseek,\\nClaude, Gemini\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nDeepResearcher\\n(Zheng et al., 2025)\\nQwen2.5-7B-Instruct\\nGRPO\\nFormat\\nWeb Search\\nDecentralized\\nNQ, TQ, HotpotQA,\\n2WikiMultiHopQA\\nMuSiQue, Bamboogle, PopQA, NQ,\\nTQ, HotpotQA, 2WikiMultiHopQA\\nLink\\nR1-Searcher (Song\\net al., 2025)\\nQwen2.5-7B-Base,\\nLlama3.1-8B-Instruct\\nGRPO, Re-\\ninforce++,\\nSFT\\nRetrieval,\\nFormat\\nWeb Search,\\nLocal Retrieval'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='TQ, HotpotQA, 2WikiMultiHopQA\\nLink\\nR1-Searcher (Song\\net al., 2025)\\nQwen2.5-7B-Base,\\nLlama3.1-8B-Instruct\\nGRPO, Re-\\ninforce++,\\nSFT\\nRetrieval,\\nFormat\\nWeb Search,\\nLocal Retrieval\\nSingle\\nHotpotQA,\\n2WikiMultiHopQA\\nHotpotQA, 2WikiMultiHopQA,\\nMuSiQue, Bamboogle\\nLink\\nReSearch (Chen\\net al., 2025a)\\nQwen2.5-7B-Instruct,\\nQwen2.5-32B-Instruct\\nGRPO\\nFormat,\\nAnswer\\nWeb Search\\nSingle\\nMuSiQue\\nHotpotQA, 2WikiMultiHopQA,\\nMuSiQue, Bamboogle\\nLink\\nSearch-o1 (Li et al.,\\n2025b)\\nQwQ-32B-Preview\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nGPQA, MATH500, AMC2023,\\nAIME2024, LiveCodeBench, Natural\\nQuestions, TriviaQA, HotpotQA,\\n2Wiki, MuSiQue, Bamboogle\\nLink\\nr1-reasoning-rag\\nDeepseek\\nPrompting\\nN/A\\nLocal Retrieval,\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nOpen Deep Search\\n(Alzubi et al., 2025)\\nLlama3.1-70B,\\nDeepseek-R1\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nSimpleQA, FRAME\\nLink\\nnode-DeepResearch\\nGemini,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\ndeep-research\\nGemini, OpenAI series,\\nDeepseek, Claude, Grok\\nPrompt\\nN/A\\nLocal Retrieval,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='Single\\nN/A\\nSimpleQA, FRAME\\nLink\\nnode-DeepResearch\\nGemini,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\ndeep-research\\nGemini, OpenAI series,\\nDeepseek, Claude, Grok\\nPrompt\\nN/A\\nLocal Retrieval,\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nTable 5: Overview of deep research implementations.\\nadditional complexity in coordination and commu-\\nnication, as well as higher computational costs.\\nAlongside these developments in agent orches-\\ntration, the nature of retrievers used in deep re-\\nsearch has also evolved significantly. Early RAG\\nsystems relied on sparse keyword-based retrieval,\\nlater surpassed by dense retrievers employing bi-\\nencoder architectures for semantic matching. More\\nrecent deep research systems increasingly integrate\\nweb search-based retrievers, allowing real-time ac-\\ncess to open-domain information. Some retrievers\\nhave also been transformed into LLM-callable tools\\nfor flexible invocation. This evolution of retrievers\\nhas played a crucial role in enabling the sophisti-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='have also been transformed into LLM-callable tools\\nfor flexible invocation. This evolution of retrievers\\nhas played a crucial role in enabling the sophisti-\\ncated information-gathering processes required for\\ndeep research.\\nC\\nComparison of Reasoning Workflows\\nand Agent Orchestration Strategies\\nTable 6 summarizes the diverse reasoning work-\\nflows and agent orchestration strategies employed\\nin Synergized RAG-Reasoning systems, highlight-\\ning their respective strengths, limitations, and suit-\\nable application scenarios. Reasoning workflows\\nvary from linear chain-based approaches, which\\nare efficient but vulnerable to error propagation, to\\nmore complex tree-based and graph-based methods\\nthat offer higher recall and transparency at the cost\\nof increased computational overhead. Similarly,\\nagent orchestration strategies range from single-\\nagent setups to multi-agent systems that distribute\\nspecialized roles among agents, enhancing robust-\\nness and scalability. However, these advanced de-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 24}, page_content='agent setups to multi-agent systems that distribute\\nspecialized roles among agents, enhancing robust-\\nness and scalability. However, these advanced de-\\nsigns often introduce additional communication\\noverhead and complexity in conflict resolution.\\nThis comparison illustrates the trade-offs inherent\\nin choosing particular workflows or orchestration\\narchitectures and underscores the need for adaptive\\nsystems that can dynamically balance efficiency,\\naccuracy, and resource constraints in real-world\\napplications.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Category\\nSub-category\\nStrengths\\nLimitations\\nSuitable Scenarios\\nReasoning\\nWorkflow\\nChain-based\\nOne retrieval per reasoning step; low\\nlatency and token cost. Easy to cache\\nand monitor.\\nAn early wrong sub-query propagates;\\ncontext grows fast on long chains.\\nSingle-hop or short multi-hop QA\\nwhere each intermediate fact is easy to\\naccess.\\nTree-based (ToT)\\nHigh recall: explores multiple\\nbranches in parallel, hedges against\\nearly errors. Transparent what-if traces.\\nQuadratic cost; tree branches require\\nmany retrieval calls.\\nAmbiguous or “multiple plausible\\npaths” tasks (e.g., HotpotQA, legal\\nreasoning) where missing one clue kills\\naccuracy.\\nTree-based\\n(MCTS)\\nBudget-aware exploration: focuses\\ncalls on promising branches; graceful\\nanytime stopping.\\nTuning-heavy and may converge to a\\nsuboptimal subtree.\\nDeep-search problems under tight\\nAPI-call or token budgets (e.g.,\\nbiomedical QA).\\nGraph-based\\n(Walk-on-Graph)\\nEfficient in explicit KG/document\\ngraphs; short reasoning paths on KGs.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Deep-search problems under tight\\nAPI-call or token budgets (e.g.,\\nbiomedical QA).\\nGraph-based\\n(Walk-on-Graph)\\nEfficient in explicit KG/document\\ngraphs; short reasoning paths on KGs.\\nRequires high-quality KGs; fails if\\ngraphs lack explicit edges; less flexible\\nfor open-web contexts.\\nEnterprise or domain-specific QA\\nwhere a curated KG exists (e.g.,\\nproduct catalogs).\\nGraph-based\\n(Think-on-Graph)\\nAdaptive and verifiable; LLM updates\\na live evidence graph, allowing\\nnode-level citation checks and high\\nfactual accuracy.\\nHigher latency; many micro-tool calls;\\nsearch space can explode without\\npruning.\\nOpen-domain “deep research” or\\nfact-dense synthesis tasks (e.g.,\\nBrowseComp, systematic reviews).\\nAgent\\nOrchestration\\nSingle-agent\\n(Prompt-only)\\nSimple implementation via a ReAct\\nloop; low resource overhead.\\nConstrained by prompt engineering\\nand system design flexibility.\\nPrototyping demos and small-scale\\napplications where simplicity\\noutweighs performance.\\nSingle-agent\\n(SFT)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='loop; low resource overhead.\\nConstrained by prompt engineering\\nand system design flexibility.\\nPrototyping demos and small-scale\\napplications where simplicity\\noutweighs performance.\\nSingle-agent\\n(SFT)\\nClear, well-defined RAG and\\nreasoning patterns; higher precision\\nthan prompt-only approaches.\\nRequires large synthetic data; may\\noverfit tool schemas, reducing\\nout-of-domain generalization.\\nProduction chatbots with stable APIs\\nand predictable query formats (e.g.,\\ninternal customer support).\\nSingle-agent (RL)\\nAdaptive RAG and reasoning yields\\nhigh recall and accuracy; learns when\\nto retrieve and reason.\\nChallenging to define suitable reward\\nsignals; computationally expensive to\\ntrain.\\nOpen-domain research or long-form\\nQA where call costs are high and\\noptimal stop conditions matter.\\nMulti-agent\\n(Decentralized)\\nHigh recall via parallel domain\\nexperts; robustness to noisy or diverse\\ncorpora.\\nHigh communication and consensus\\noverhead; conflicting answers require\\nresolution.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'source': '../data/pdf/towards agentic rag.pdf', 'file_path': '../data/pdf/towards agentic rag.pdf', 'total_pages': 26, 'format': 'PDF 1.5', 'title': 'Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs', 'author': 'Yangning Li; Weizhi Zhang; Yuyao Yang; Wei-Chieh Huang; Yaozu Wu; Junyu Luo; Yuanchen Bei; Henry Peng Zou; Xiao Luo; Yusheng Zhao; Chunkit Chan; Yankai Chen; Zhongfen Deng; Yinghui Li; Hai-Tao Zheng; Dongyuan Li; Renhe Jiang; Ming Zhang; Yangqiu Song; Philip S. Yu', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 25}, page_content='Multi-agent\\n(Decentralized)\\nHigh recall via parallel domain\\nexperts; robustness to noisy or diverse\\ncorpora.\\nHigh communication and consensus\\noverhead; conflicting answers require\\nresolution.\\nLarge-scale evidence aggregation\\nacross heterogeneous sources (e.g.,\\nmeta-analysis, news tracking).\\nMulti-agent\\n(Central-\\nized/Hierarchical)\\nBudget-efficient: manager avoids\\nduplicate searches and ensures a clear\\nprovenance chain. Scales horizontally\\nwithout exponential cost growth.\\nManager prompts or policies can\\nbecome a single-point bottleneck,\\nlimiting performance.\\nComplex tasks requiring coordinated\\nsubtasks under strict API-call budgets.\\nTable 6: Comparison of reasoning workflows and agent orchestration in Synergized RAG-Reasoning systems.\\n26'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 0}, page_content='1 \\nSample PDF \\n \\nCreated for testing PDFObject \\n \\nThis PDF is three pages long. Three long pages. Or three short pages if \\nyou’re optimistic. Is it the same as saying “three long minutes”, knowing \\nthat all minutes are the same duration, and one cannot possibly be longer \\nthan the other? If these pages are all the same size, can one possibly be \\nlonger than the other? \\n \\nI digress. Here’s some Latin. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec \\nodio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum \\nimperdiet. Duis sagittis ipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris \\nmassa. Vestibulum lacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per \\nconubia nostra, per inceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 0}, page_content='conubia nostra, per inceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem \\nat dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. \\nMorbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia \\naliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh.  \\n \\nQuisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia \\nnostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque \\nadipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 0}, page_content='Nunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet. Donec lacus nunc, viverra nec, blandit \\nvel, egestas et, augue. Vestibulum tincidunt malesuada tellus. Ut ultrices ultrices enim. Curabitur \\nsit amet mauris.  \\n \\nMorbi in dui quis est pulvinar ullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras \\nmetus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique,'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 0}, page_content='metus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique, \\ndignissim in, ultrices sit amet, augue. Proin sodales libero eget ante. Nulla quam. Aenean laoreet. \\nVestibulum nisi lectus, commodo ac, facilisis ac, ultricies eu, pede. Ut orci risus, accumsan \\nporttitor, cursus quis, aliquet eget, justo. Sed pretium blandit orci.  \\n \\nUt eu diam at pede suscipit sodales. Aenean lectus elit, fermentum non, convallis id, sagittis at, \\nneque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 1}, page_content='2 \\nipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante \\ndapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. \\nPraesent mauris.  \\n \\nFusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class \\naptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur \\nsodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean \\nquam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem.  \\n \\nProin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, \\nmassa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper \\nvel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia,'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 1}, page_content='vel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, \\nurna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla \\nfacilisi. Ut fringilla. Suspendisse potenti.  \\n \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet.  \\n \\nDonec lacus nunc, viverra nec, blandit vel, egestas et, augue. Vestibulum tincidunt malesuada'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 1}, page_content='elementum. Morbi in ipsum sit amet pede facilisis laoreet.  \\n \\nDonec lacus nunc, viverra nec, blandit vel, egestas et, augue. Vestibulum tincidunt malesuada \\ntellus. Ut ultrices ultrices enim. Curabitur sit amet mauris. Morbi in dui quis est pulvinar \\nullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras metus. Sed aliquet risus a \\ntortor. Integer id quam. Morbi mi.  \\n \\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed \\ncursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis \\nipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris massa. Vestibulum \\nlacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per \\ninceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 1}, page_content='inceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem \\nat dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. \\nMorbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia \\naliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh.  \\n \\nQuisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia \\nnostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque \\nadipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna.'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 1}, page_content='Nunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 2}, page_content='3 \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet. Donec lacus nunc, viverra nec, blandit \\nvel, egestas et, augue. Vestibulum tincidunt malesuada tellus. Ut ultrices ultrices enim. Curabitur \\nsit amet mauris.  \\n \\nMorbi in dui quis est pulvinar ullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras \\nmetus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique, \\ndignissim in, ultrices sit amet, augue. Proin sodales libero eget ante. Nulla quam. Aenean laoreet. \\nVestibulum nisi lectus, commodo ac, facilisis ac, ultricies eu, pede. Ut orci risus, accumsan \\nporttitor, cursus quis, aliquet eget, justo. Sed pretium blandit orci.  \\n \\nUt eu diam at pede suscipit sodales. Aenean lectus elit, fermentum non, convallis id, sagittis at, \\nneque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 2}, page_content='neque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem \\nipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante \\ndapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. \\nPraesent mauris.  \\n \\nFusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class \\naptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur \\nsodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean \\nquam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem.  \\n \\nProin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, \\nmassa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper'),\n",
       " Document(metadata={'producer': 'Mac OS X 10.11.3 Quartz PDFContext', 'creator': 'Word', 'creationdate': \"D:20160319061844Z00'00'\", 'source': '../data/pdf/sample-local-pdf.pdf', 'file_path': '../data/pdf/sample-local-pdf.pdf', 'total_pages': 3, 'format': 'PDF 1.3', 'title': 'Sample PDF', 'author': 'Philip Hutchison', 'subject': '', 'keywords': '', 'moddate': \"D:20160319061844Z00'00'\", 'trapped': '', 'modDate': \"D:20160319061844Z00'00'\", 'creationDate': \"D:20160319061844Z00'00'\", 'page': 2}, page_content='massa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper \\nvel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, \\nurna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla \\nfacilisi. Ut fringilla. Suspendisse potenti.  \\n \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nRAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nZirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang∗\\nThe University of Hong Kong\\nzrguo101@hku.hk\\nxubinrencs@gmail.com\\nchaohuang75@gmail.com\\nABSTRACT\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\\nfor expanding Large Language Models beyond their static training limitations.\\nHowever, a critical misalignment exists between current RAG capabilities and\\nreal-world information environments. Modern knowledge repositories are inher-\\nently multimodal, containing rich combinations of textual content, visual elements,\\nstructured tables, and mathematical expressions. Yet existing RAG frameworks are\\nlimited to textual content, creating fundamental gaps when processing multimodal\\ndocuments. We present RAG-Anything, a unified framework that enables compre-\\nhensive knowledge retrieval across all modalities. Our approach reconceptualizes'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='documents. We present RAG-Anything, a unified framework that enables compre-\\nhensive knowledge retrieval across all modalities. Our approach reconceptualizes\\nmultimodal content as interconnected knowledge entities rather than isolated data\\ntypes. The framework introduces dual-graph construction to capture both cross-\\nmodal relationships and textual semantics within a unified representation. We\\ndevelop cross-modal hybrid retrieval that combines structural knowledge naviga-\\ntion with semantic matching. This enables effective reasoning over heterogeneous\\ncontent where relevant evidence spans multiple modalities. RAG-Anything demon-\\nstrates superior performance on challenging multimodal benchmarks, achieving\\nsignificant improvements over state-of-the-art methods. Performance gains become\\nparticularly pronounced on long documents where traditional approaches fail. Our\\nframework establishes a new paradigm for multimodal knowledge access, eliminat-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='particularly pronounced on long documents where traditional approaches fail. Our\\nframework establishes a new paradigm for multimodal knowledge access, eliminat-\\ning the architectural fragmentation that constrains current systems. Our framework\\nis open-sourced at: https://github.com/HKUDS/RAG-Anything.\\n1\\nINTRODUCTION\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding\\nthe knowledge boundaries of Large Language Models (LLM) beyond their static training limita-\\ntions Zhang et al. (2025). By enabling dynamic retrieval and incorporation of external knowledge\\nduring inference, RAG systems transform static language models into adaptive, knowledge-aware\\nsystems. This capability has proven essential for applications requiring up-to-date information,\\ndomain-specific knowledge, or factual grounding that extends beyond pre-training corpora.\\nHowever, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='domain-specific knowledge, or factual grounding that extends beyond pre-training corpora.\\nHowever, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the\\nrich multimodal information present in real-world documents. This limitation fundamentally mis-\\naligns with how information exists in authentic environments. Real-world knowledge repositories are\\ninherently heterogeneous and multimodal Abootorabi et al. (2025). They contain rich combinations\\nof textual content, visual elements, structured tables, and mathematical expressions across diverse\\ndocument formats. This textual assumption forces existing RAG systems to either discard non-textual\\ninformation entirely or flatten complex multimodal content into inadequate textual approximations.\\nThe consequences of this limitation become particularly severe in document-intensive domains\\nwhere multimodal content carries essential meaning. Academic research, financial analysis, and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='The consequences of this limitation become particularly severe in document-intensive domains\\nwhere multimodal content carries essential meaning. Academic research, financial analysis, and\\ntechnical documentation represent prime examples of knowledge-rich environments. These domains\\nfundamentally depend on visual and structured information. Critical insights are often encoded\\nexclusively in non-textual formats. Such formats resist meaningful conversion to plain text.\\nThe consequences of this limitation become particularly severe in knowledge-intensive domains where\\nmultimodal content carries essential meaning. Three representative scenarios illustrate the critical\\n∗Corresponding Author: Chao Huang\\n1\\narXiv:2510.12323v1  [cs.AI]  14 Oct 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nneed for multimodal RAG capabilities. In Scientific Research, experimental results are primarily\\ncommunicated through plots, diagrams, and statistical visualizations. These contain core discoveries\\nthat remain invisible to text-only systems. Financial Analysis relies heavily on market charts,\\ncorrelation matrices, and performance tables. Investment insights are encoded in visual patterns\\nrather than textual descriptions. Additionally, Medical Literature Analysis depends on radiological\\nimages, diagnostic charts, and clinical data tables. These contain life-critical information essential for\\naccurate diagnosis and treatment decisions. Current RAG frameworks systematically exclude these\\nvital knowledge sources across all three scenarios. This creates fundamental gaps that render them\\ninadequate for real-world applications requiring comprehensive information understanding. Therefore,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='vital knowledge sources across all three scenarios. This creates fundamental gaps that render them\\ninadequate for real-world applications requiring comprehensive information understanding. Therefore,\\nmultimodal RAG emerges as a critical advancement. It is necessary to bridge these knowledge gaps\\nand enable truly comprehensive intelligence across all modalities of human knowledge representation.\\nAddressing multimodal RAG presents three fundamental technical challenges that demand principled\\nsolutions. This makes it significantly more complex than traditional text-only approaches. The naive\\nsolution of converting all multimodal content to textual descriptions introduces severe information\\nloss. Visual elements such as charts, diagrams, and spatial layouts contain semantic richness that\\ncannot be adequately captured through text alone. These inherent limitations necessitate the design of\\neffective technical components. Such components must be specifically designed to handle multimodal'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='effective technical components. Such components must be specifically designed to handle multimodal\\ncomplexity and preserve the full spectrum of information contained within diverse content types.\\nTechnical Challenges. • First, the unified multimodal representation challenge requires seam-\\nlessly integrating diverse information types. The system must preserve their unique characteristics\\nand cross-modal relationships. This demands advanced multimodal encoders that can capture both\\nintra-modal and inter-modal dependencies without losing essential visual semantics. • Second, the\\nstructure-aware decomposition challenge demands intelligent parsing of complex layouts. The\\nsystem must maintain spatial and hierarchical relationships crucial for understanding. This requires\\nspecialized layout-aware parsing modules that can interpret document structure and preserve contex-\\ntual positioning of multimodal elements. • Third, the cross-modal retrieval challenge necessitates'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='specialized layout-aware parsing modules that can interpret document structure and preserve contex-\\ntual positioning of multimodal elements. • Third, the cross-modal retrieval challenge necessitates\\nsophisticated mechanisms that can navigate between different modalities. These mechanisms must\\nreason over their interconnections during retrieval. This calls for cross-modal alignment systems\\ncapable of understanding semantic correspondences across text, images, and structured data. These\\nchallenges are amplified in long-context scenarios. Relevant evidence is dispersed across multiple\\nmodalities and sections, requiring coordinated reasoning across heterogeneous information sources.\\nOur Contributions. To address these challenges, we introduce RAG-Anything, a unified framework\\nthat fundamentally reimagines multimodal knowledge representation and retrieval. Our approach\\nemploys a dual-graph construction strategy that elegantly bridges the gap between cross-modal'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='that fundamentally reimagines multimodal knowledge representation and retrieval. Our approach\\nemploys a dual-graph construction strategy that elegantly bridges the gap between cross-modal\\nunderstanding and fine-grained textual semantics. Rather than forcing diverse modalities into text-\\ncentric pipelines, RAG-Anything constructs complementary knowledge graphs that preserve both\\nmultimodal contextual relationships and detailed textual knowledge. This design enables seamless\\nintegration of visual elements, structured data, and mathematical expressions within a unified retrieval\\nframework. The system maintains semantic integrity across modalities while ensuring efficient\\ncross-modal reasoning capabilities throughout the process.\\nOur cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav-\\nigation with semantic similarity matching. This architecture addresses the fundamental limita-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='Our cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav-\\nigation with semantic similarity matching. This architecture addresses the fundamental limita-\\ntion of existing approaches that rely solely on embedding-based retrieval or keyword matching.\\nRAG-Anything leverages explicit graph relationships to capture multi-hop reasoning patterns. It\\nsimultaneously employs dense vector representations to identify semantically relevant content that\\nlacks direct structural connections. The framework introduces modality-aware query processing\\nand cross-modal alignment systems. These enable textual queries to effectively access visual and\\nstructured information. This unified approach eliminates the architectural fragmentation that plagues\\ncurrent multimodal RAG systems. It delivers superior performance particularly on long-context\\ndocuments where relevant evidence spans multiple modalities and document sections.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content='current multimodal RAG systems. It delivers superior performance particularly on long-context\\ndocuments where relevant evidence spans multiple modalities and document sections.\\nExperimental Validation. To validate the effectiveness of our proposed approach, we conduct com-\\nprehensive experiments on two challenging multimodal benchmarks: DocBench and MMLongBench.\\nOur evaluation demonstrates that RAG-Anything achieves superior performance across diverse do-\\nmains. The framework represents substantial improvements over state-of-the-art baselines. Notably,\\nour performance gains become increasingly significant as content length increases. We observe\\nparticularly pronounced advantages on long-context materials. This validates our core hypothesis\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nthat dual-graph construction and cross-modal hybrid retrieval are essential for handling complex\\nmultimodal materials. Our ablation studies reveal that graph-based knowledge representation provides\\nthe primary performance gains. Traditional chunk-based approaches fail to capture the structural\\nrelationships critical for multimodal reasoning. Case studies further demonstrate that our framework\\nexcels at precise localization within complex layouts. The system effectively disambiguates similar\\nterminology and navigates multi-panel visualizations through structure-aware retrieval mechanisms.\\n2\\nTHE RAG-ANYTHING FRAMEWORK\\n2.1\\nPRELIMINARY\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for dynamically\\nexpanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning\\ncapabilities, their knowledge remains static and bounded by training data cutoffs. This creates an'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='expanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning\\ncapabilities, their knowledge remains static and bounded by training data cutoffs. This creates an\\never-widening gap with the rapidly evolving information landscape. RAG systems address this critical\\nlimitation by enabling LLMs to retrieve and incorporate external knowledge sources during inference.\\nThis transforms them from static repositories into adaptive, knowledge-aware systems.\\nThe Multimodal Reality: Beyond Text-Only RAG. Current RAG systems face a critical limitation\\nthat severely restricts their real-world deployment. Existing frameworks operate under the restrictive\\nassumption that knowledge corpus consists exclusively of plain textual documents. This assump-\\ntion fundamentally misaligns with how information exists in authentic environments. Real-world\\nknowledge repositories are inherently heterogeneous and multimodal, containing rich combinations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='tion fundamentally misaligns with how information exists in authentic environments. Real-world\\nknowledge repositories are inherently heterogeneous and multimodal, containing rich combinations\\nof textual content, visual elements, structured data, and mathematical expressions. These diverse\\nknowledge sources span multiple document formats and presentation mediums, from research papers\\nand technical slides to web pages and interactive documents.\\n2.1.1\\nMOTIVATING RAG-ANYTHING\\nThis multimodal reality introduces fundamental technical challenges that expose the inadequacy of\\ncurrent text-only RAG approaches. Effective multimodal RAG requires unified indexing strategies\\nthat can handle disparate data types, cross-modal retrieval mechanisms that preserve semantic\\nrelationships across modalities, and sophisticated synthesis techniques that can coherently integrate\\ndiverse information sources. These challenges demand a fundamentally different architectural'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='relationships across modalities, and sophisticated synthesis techniques that can coherently integrate\\ndiverse information sources. These challenges demand a fundamentally different architectural\\napproach rather than incremental improvements to existing systems.\\nThe RAG-Anything framework introduces a unified approach for retrieving and processing knowl-\\nedge from heterogeneous multimodal information sources. Our system addresses the fundamental\\nchallenge of handling diverse data modalities and document formats within a retrieval pipeline.\\nThe framework comprises three core components: universal indexing for multimodal knowledge,\\ncross-modal adaptive retrieval, and knowledge-enhanced response generation. This integrated design\\nenables effective knowledge utilization across modalities while maintaining computational efficiency.\\n2.2\\nUNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\\nA key requirement for universal knowledge access is the ability to represent heterogeneous multimodal'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='2.2\\nUNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\\nA key requirement for universal knowledge access is the ability to represent heterogeneous multimodal\\ncontent in a unified, retrieval-oriented abstraction. Unlike existing pipelines that simply parse\\ndocuments into text segments, RAG-Anything introduces Multimodal Knowledge Unification. This\\nprocess decomposes raw inputs into atomic knowledge units while preserving their structural context\\nand semantic alignment. For instance, RAG-Anything ensures that figures remain grounded in their\\ncaptions, equations remain linked to surrounding definitions, and tables stay connected to explanatory\\nnarratives. This transforms heterogeneous files into a coherent substrate for cross-modal retrieval.\\nFormally, each knowledge source ki ∈K (e.g., a web page) is decomposed into atomic content units:\\nki\\nDecompose\\n−−−−−−−→{cj = (tj, xj)}ni\\nj=1,\\n(1)\\nwhere each unit cj consists of a modality type tj ∈text, image, table, equation, . . . and its corre-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='ki\\nDecompose\\n−−−−−−−→{cj = (tj, xj)}ni\\nj=1,\\n(1)\\nwhere each unit cj consists of a modality type tj ∈text, image, table, equation, . . . and its corre-\\nsponding raw content xj. The content xj represents the extracted information from the original\\nknowledge source, processed in a modality-aware manner to preserve semantic integrity.\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nParallel\\nParser\\nMultimodal Knowledge Unification\\n...\\nImage Info\\n...\\nEquation Info\\nText Info\\n...\\nTable Info\\nHierarchical Text \\nExtraction\\nImage Caption & \\nMetadata Extraction\\nLaTeX Equation \\nRecognition\\nTable Structure & \\nContent Parsing\\nDual-Graph Construction for Multimodal Knowledge \\n(each document) \\nStructured Content List\\nText Encoder\\nMulti-modal \\nProcessors\\nEntity & Relation\\nExtraction\\nBeekeeper\\nBee\\nObserve\\nKnowledge Graph\\nMerged Node\\nChild Node\\nParent Node: Multi-\\nmodal Instance\\nVLM/LLM\\nMerged\\n...\\nTextual Multi-modal Info\\nCross-Modal Knowledge\\nGraph\\nText-Based Knowledge Graph\\nKG over All Documents\\nMerged\\nText VDB\\nMulti-modal VDB\\nVDB over All\\nDocuments\\nQuery\\nCould you share insights \\non the experimental \\nresults and data tables?\\nResponse\\nBased on the experimental \\ndata, the results revealed...\\nSemantic Similarity \\nMatching\\nStructural Knowledge \\nNavigation\\nQuery High-/Low-level Keys Extraction\\nHybrid Retrieved Info\\n...'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='Response\\nBased on the experimental \\ndata, the results revealed...\\nSemantic Similarity \\nMatching\\nStructural Knowledge \\nNavigation\\nQuery High-/Low-level Keys Extraction\\nHybrid Retrieved Info\\n...\\nMulti-modal Info Processing\\nText Info Processing\\nLLM\\nText Encoder\\nVector Database\\nFigure 1: Overview of our proposed universal RAG framework RAG-Anything.\\nTo ensure high-fidelity extraction, RAG-Anything leverages specialized parsers for different content\\ntypes. Text is segmented into coherent paragraphs or list items. Figures are extracted with associated\\nmetadata such as captions and cross-references. Tables are parsed into structured cells with headers\\nand values. Mathematical expressions are converted into symbolic representations. The resulting xj\\npreserves both content and structural context within the source. This provides a faithful, modality-\\nconsistent representation. The decomposition abstracts diverse file formats into atomic units while'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='consistent representation. The decomposition abstracts diverse file formats into atomic units while\\nmaintaining their hierarchical order and contextual relationships. This canonicalization enables\\nuniform processing, indexing, and retrieval of multimodal content within our framework.\\n2.2.1\\nDUAL-GRAPH CONSTRUCTION FOR MULTIMODAL KNOWLEDGE\\nWhile multimodal knowledge unification provides a uniform abstraction across modalities, directly\\nconstructing a single unified graph often risks overlooking modality-specific structural signals. The\\nproposed RAG-Anything addresses this challenge through a dual-graph construction strategy. The\\nsystem first builds a cross-modal knowledge graph that faithfully grounds non-textual modalities\\nwithin their contextual environment. It then constructs a text-based knowledge graph using es-\\ntablished text-centric extraction pipelines. These complementary graphs are merged through entity'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='within their contextual environment. It then constructs a text-based knowledge graph using es-\\ntablished text-centric extraction pipelines. These complementary graphs are merged through entity\\nalignment. This design ensures accurate cross-modal grounding and comprehensive coverage of\\ntextual semantics, enabling richer knowledge representation and robust retrieval.\\n• Cross-Modal Knowledge Graph: Non-textual content like images, tables, and equations contains\\nrich semantic information that traditional text-only approaches often overlook. To preserve this\\nknowledge, RAG-Anything constructs a multimodal knowledge graph where non-text atomic\\nunits are transformed into structured graph entities. RAG-Anything leverages multimodal large\\nlanguage models to derive two complementary textual representations from each atomic content\\nunit. The first is a detailed description dchunk\\nj\\noptimized for cross-modal retrieval. The second is\\nan entity summary eentity\\nj'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content='unit. The first is a detailed description dchunk\\nj\\noptimized for cross-modal retrieval. The second is\\nan entity summary eentity\\nj\\ncontaining key attributes such as entity name, type, and description for\\ngraph construction. The generation process is context-aware, processing each unit with its local\\nneighborhood Cj = {ck | |k −j| ≤δ}, where δ controls the contextual window size. This ensures\\nrepresentations accurately reflect each unit’s role within the broader document structure.\\nBuilding on these textual representations, RAG-Anything constructs the graph structure using non-\\ntext units as anchor points. For each non-text unit cj, the graph extraction routine R(·) processes\\nits description dchunk\\nj\\nto identify fine-grained entities and relations:\\n(Vj, Ej) = R(dchunk\\nj\\n),\\n(2)\\nwhere Vj and Ej denote the sets of intra-chunk entities and their relations, respectively. Each\\natomic non-text unit is associated with a multimodal entity node vmm\\nj\\nthat serves as an anchor for\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nits intra-chunk entities through explicit belongs_to edges:\\n˜V = {vmm\\nj\\n}j ∪\\n[\\nj\\nVj,\\n(3)\\n˜E =\\n[\\nj\\nEj ∪\\n[\\nj\\n{(u\\nbelongs_to\\n−−−−−−−→vmm\\nj\\n) : u ∈Vj}.\\n(4)\\nThis construction preserves modality-specific grounding while ensuring non-textual content is con-\\ntextualized by its textual neighborhood. This enables reliable cross-modal retrieval and reasoning.\\n• Text-based Knowledge Graph: For text modality chunks, we construct a traditional text-based\\nknowledge graph following established methodologies similar to LightRAG (Guo et al., 2024)\\nand GraphRAG (Edge et al., 2024). The extraction process operates directly on textual content xj\\nwhere tj = text, leveraging named entity recognition and relation extraction techniques to identify\\nentities and their semantic relationships. Given the rich semantic information inherent in textual\\ncontent, multimodal context integration is not required for this component. The resulting text-based'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='entities and their semantic relationships. Given the rich semantic information inherent in textual\\ncontent, multimodal context integration is not required for this component. The resulting text-based\\nknowledge graph captures explicit knowledge and semantic connections present in textual portions\\nof documents, complementing the multimodal graph’s cross-modal grounding capabilities.\\n2.2.2\\nGRAPH FUSION AND INDEX CREATION\\nThe separate cross-modal and text-based knowledge graphs capture complementary aspects of\\ndocument semantics. Integrating them creates a unified representation leveraging visual-textual\\nassociations and fine-grained textual relationships for enhanced retrieval.\\n• (i) Entity Alignment and Graph Fusion. To create a unified knowledge representation, we\\nmerge the multimodal knowledge graph ( ˜V , ˜E) and text-based knowledge graph through entity align-\\nment. This process uses entity names as primary matching keys to identify semantically equivalent'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='merge the multimodal knowledge graph ( ˜V , ˜E) and text-based knowledge graph through entity align-\\nment. This process uses entity names as primary matching keys to identify semantically equivalent\\nentities across both graph structures. The integration consolidates their representations, creating\\na comprehensive knowledge graph G = (V, E). This graph captures both multimodal contextual\\nrelationships and text-based semantic connections. The merged graph provides a holistic view of the\\ndocument collection. This enables effective retrieval by leveraging visual-textual associations from\\nthe multimodal graph and fine-grained textual knowledge relationships from the text-based graph.\\n• (ii) Dense Representation Generation. To enable efficient similarity-based retrieval, we construct\\na comprehensive embedding table T that encompasses all components generated during the indexing\\nprocess. We encode dense representations for all graph entities, relationships, and atomic content'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='a comprehensive embedding table T that encompasses all components generated during the indexing\\nprocess. We encode dense representations for all graph entities, relationships, and atomic content\\nchunks across modalities using an appropriate encoder. This creates a unified embedding space where\\neach component s ∈entities, relations, chunks is mapped to its corresponding dense representation:\\nT = emb(s) : s ∈V ∪E ∪cjj,\\n(5)\\nwhere emb(·) denotes the embedding function tailored for each component type. Together, the\\nunified knowledge graph G and the embedding table T constitute the complete retrieval index\\nI = (G, T ). This provides both structural knowledge representation and dense vector space for\\nefficient cross-modal similarity search during the subsequent retrieval stage.\\n2.3\\nCROSS-MODAL HYBRID RETRIEVAL\\nThe retrieval stage operates on the index I = (G, T ) to identify relevant knowledge components for a'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='2.3\\nCROSS-MODAL HYBRID RETRIEVAL\\nThe retrieval stage operates on the index I = (G, T ) to identify relevant knowledge components for a\\ngiven user query. Traditional RAG methods face significant limitations when dealing with multimodal\\ndocuments. They typically rely on semantic similarity within single modalities and fail to capture the\\nrich interconnections between visual, mathematical, tabular, and textual elements. To address these\\nchallenges, our framework introduces a cross-modal hybrid retrieval mechanism. This mechanism\\nleverages structural knowledge and semantic representations across heterogeneous modalities.\\nModality-Aware Query Encoding. Given a user query q, we first perform modality-aware query\\nanalysis to extract lexical cues and potential modality preferences embedded within the query.\\nFor instance, queries containing terms such as \"figure,\" \"chart,\" \"table,\" or \"equation\" provide'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content='analysis to extract lexical cues and potential modality preferences embedded within the query.\\nFor instance, queries containing terms such as \"figure,\" \"chart,\" \"table,\" or \"equation\" provide\\nexplicit signals about the expected modality of relevant information. We then compute a unified text\\nembedding eq using the same encoder employed during indexing, ensuring consistency between\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nquery and knowledge representations. This embedding-based approach enables cross-modal retrieval\\ncapabilities where textual queries can effectively access multimodal content through their shared\\nrepresentations, maintaining retrieval consistency while preserving cross-modal accessibility.\\nHybrid Knowledge Retrieval Architecture. Recognizing that knowledge relevance manifests\\nthrough both explicit structural connections and implicit semantic relationships, we design a hybrid\\nretrieval architecture that strategically combines two complementary mechanisms.\\n• (i) Structural Knowledge Navigation. This mechanism addresses the challenge of capturing\\nexplicit relationships and multi-hop reasoning patterns. Traditional keyword-based retrieval often\\nfails to identify knowledge connected through intermediate entities or cross-modal relationships. To\\novercome this limitation, we exploit the structural properties encoded within our unified knowledge'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='overcome this limitation, we exploit the structural properties encoded within our unified knowledge\\ngraph G. We employ keyword matching and entity recognition to locate relevant graph components.\\nThe retrieval process begins with exact entity matching against query terms.\\nWe then perform strategic neighborhood expansion to include related entities and relationships within\\na specified hop distance. This structural approach proves particularly effective at uncovering high-\\nlevel semantic connections and entity-relation patterns that span multiple modalities. It capitalizes\\non the rich cross-modal linkages established in our multimodal knowledge graph. The structural\\nnavigation yields candidate set Cstru(q) containing relevant entities, relationships, and their associated\\ncontent chunks that provide comprehensive contextual information.\\n• (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='content chunks that provide comprehensive contextual information.\\n• (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying\\nsemantically relevant knowledge that lacks explicit structural connections. While structural navigation\\nexcels at following explicit relationships, it may miss relevant content that is semantically related but\\nnot directly connected in the graph topology. To bridge this gap, we conduct dense vector similarity\\nsearch between the query embedding eq and all components stored in embedding table T .\\nThis approach encompasses atomic content chunks across all modalities, graph entities, and relation-\\nship representations, enabling fine-grained semantic matching that can surface relevant knowledge\\neven when traditional lexical or structural signals are absent. The learned embedding space captures\\nnuanced semantic relationships and contextual similarities that complement the explicit structural'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='even when traditional lexical or structural signals are absent. The learned embedding space captures\\nnuanced semantic relationships and contextual similarities that complement the explicit structural\\nsignals from the navigation mechanism. This retrieval pathway returns the top-k most semantically\\nsimilar chunks Cseman(q) ranked by cosine similarity scores, ensuring comprehensive coverage of\\nboth structurally and semantically relevant knowledge.\\nCandidate Pool Unification. Both retrieval pathways may return overlapping candidates with\\ndiffering relevance signals. This necessitates a principled approach to unify and rank results. Retrieval\\ncandidates from both pathways are unified into a comprehensive candidate pool: C(q) = Cstru(q) ∪\\nCseman(q). Simply merging candidates would ignore distinct evidence each pathway provides. It\\nwould fail to account for redundancy between retrieved content.\\n• (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='would fail to account for redundancy between retrieved content.\\n• (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion\\nscoring mechanism integrating multiple complementary relevance signals. These include structural\\nimportance derived from graph topology, semantic similarity scores from embedding space, and query-\\ninferred modality preferences obtained through lexical analysis. This multi-faceted scoring approach\\nensures that final ranked candidates C⋆(q) effectively balance structural knowledge relationships with\\nsemantic relevance while appropriately weighting different modalities based on query characteristics.\\n• (ii) Hybrid Retrieval Integration. The resulting hybrid retrieval mechanism enables our framework\\nto leverage the complementary strengths of both knowledge graphs and dense representations. This\\nprovides comprehensive coverage of relevant multimodal knowledge for response generation.\\n2.4\\nFROM RETRIEVAL TO SYNTHESIS'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content='provides comprehensive coverage of relevant multimodal knowledge for response generation.\\n2.4\\nFROM RETRIEVAL TO SYNTHESIS\\nEffective multimodal question answering requires preserving rich visual semantics while maintaining\\ncoherent grounding across heterogeneous knowledge sources. Simple text-only approaches lose\\ncrucial visual information, while naive multimodal methods struggle with coherent cross-modal\\nintegration. Our synthesis stage addresses these challenges by systematically combining retrieved\\nmultimodal knowledge into comprehensive, evidence-grounded responses.\\n• (i) Building Textual Context. Given the top-ranked retrieval candidates C⋆(q), we construct a\\nstructured textual context. We concatenate textual representations of all retrieved components, includ-\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nTable 1: Statistics of Experimental Datasets.\\nDataset\\n# Documents\\n# Avg. Pages\\n# Avg. Tokens\\n# Doc Types\\n# Questions\\nDocBench\\n229\\n66\\n46377\\n5\\n1102\\nMMLongBench\\n135\\n47.5\\n21214\\n7\\n1082\\ning entity summaries, relationship descriptions, and chunk contents. The concatenation incorporates\\nappropriate delimiters to indicate modality types and hierarchical origins. This approach ensures the\\nlanguage model can effectively parse and reason over heterogeneous knowledge components.\\n• (ii) Recovering Visual Content. For multimodal chunks corresponding to visual artifacts, we\\nperform dereferencing to recover original visual content, creating V⋆(q). This design maintains con-\\nsistency with our unified embedding strategy. Textual proxies enable efficient retrieval while authentic\\nvisual content provides rich semantics necessary for sophisticated reasoning during synthesis.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='sistency with our unified embedding strategy. Textual proxies enable efficient retrieval while authentic\\nvisual content provides rich semantics necessary for sophisticated reasoning during synthesis.\\nThe synthesis process jointly conditions on both the assembled comprehensive textual context and\\ndereferenced visual artifacts using a vision-language model:\\nResponse = VLM(q, P(q), V⋆(q)),\\n(6)\\nwhere the VLM integrates information from query, textual context, and visual content. This unified\\nconditioning enables sophisticated visual interpretation while maintaining grounding in retrieved\\nevidence. The resulting responses are both visually informed and factually grounded.\\n3\\nEVALUATION\\n3.1\\nEXPERIMENTAL SETTINGS\\nEvaluation Datasets. We conduct comprehensive evaluations on two challenging multimodal\\nDocument Question Answering (DQA) benchmarks that reflect real-world complexity and diversity.\\nDocBench (Zou et al., 2024) provides a rigorous testbed with 229 multimodal documents spanning'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='Document Question Answering (DQA) benchmarks that reflect real-world complexity and diversity.\\nDocBench (Zou et al., 2024) provides a rigorous testbed with 229 multimodal documents spanning\\nfive critical domains: Academia, Finance, Government, Laws, and News. The dataset includes 1,102\\nexpert-crafted question-answer pairs. These documents are notably extensive, averaging 66 pages and\\napproximately 46,377 tokens, which presents substantial challenges for long-context understanding.\\nMMLongBench (Ma et al., 2024) complements this evaluation by focusing specifically on long-\\ncontext multimodal document comprehension. It features 135 documents across 7 diverse document\\ntypes with 1,082 expert-annotated questions. Together, these benchmarks provide comprehensive\\ncoverage of the multimodal document understanding challenges that RAG-Anything aims to address.\\nThey ensure our evaluation captures both breadth across domains and depth in document complexity.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='coverage of the multimodal document understanding challenges that RAG-Anything aims to address.\\nThey ensure our evaluation captures both breadth across domains and depth in document complexity.\\nDetailed dataset statistics and characteristics are provided in Appendix A.1.\\nBaselines. We compare RAG-Anything against the following methods for performance evaluation:\\n• GPT-4o-mini: A powerful multimodal language model with native text and image understanding\\ncapabilities. Its 128K token context window enables direct processing of entire documents. We\\nevaluate this model as a strong baseline for long-context multimodal understanding.\\n• LightRAG (Guo et al., 2024): A graph-enhanced RAG system that integrates structured knowledge\\nrepresentation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation\\ninformation and broader semantic context, improving retrieval precision and response coherence.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='representation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation\\ninformation and broader semantic context, improving retrieval precision and response coherence.\\n• MMGraphRAG (Wan & Yu, 2025): A multimodal retrieval framework that constructs unified\\nknowledge graphs spanning textual and visual content. This method employs spectral clustering\\nfor multimodal entity analysis and retrieves context along reasoning paths to guide generation.\\nExperimental Settings. In our experiments, we implement all baselines using GPT-4o-mini as\\nthe backbone LLM. Documents are parsed using MinerU (Wang et al., 2024) to extract text, im-\\nages, tables, and equations for downstream RAG processing. For the retrieval pipeline, we em-\\nploy the text-embedding-3-large model with 3072-dimensional embeddings. We use the\\nbge-reranker-v2-m3 model for reranking. For graph-based RAG methods, we enforce a com-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content='ploy the text-embedding-3-large model with 3072-dimensional embeddings. We use the\\nbge-reranker-v2-m3 model for reranking. For graph-based RAG methods, we enforce a com-\\nbined entity-and-relation token limit of 20,000 tokens and a chunk token limit of 12,000 tokens.\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nTable 2: Accuracy (%) on DocBench Dataset. Performance results with best scores highlighted in\\ndark blue and second-best in light blue. Domain categories include Academia (Aca.), Finance\\n(Fin.), Government (Gov.), Legal Documents (Law), and News Articles (News). Document types are\\ncategorized as Text-only (Txt.), Multimodal (Mm.), and Unanswerable queries (Una.).\\nMethod\\nDomains\\nTypes\\nOverall\\nAca.\\nFin.\\nGov.\\nLaw.\\nNews\\nTxt.\\nMm.\\nUna.\\nGPT-4o-mini\\n40.3\\n46.9\\n60.3\\n59.2\\n61.0\\n61.0\\n43.8\\n49.6\\n51.2\\nLightRAG\\n53.8\\n56.2\\n59.5\\n61.8\\n65.7\\n85.0\\n59.7\\n46.8\\n58.4\\nMMGraphRAG\\n64.3\\n52.8\\n64.9\\n40.0\\n61.5\\n67.6\\n66.0\\n60.5\\n61.0\\nRAGAnything\\n61.4\\n67.0\\n61.5\\n60.2\\n66.3\\n85.0\\n76.3\\n46.0\\n63.4\\nTable 3: Accuracy (%) on MMLongBench across different domains and overall performance. Best re-\\nsults are highlighted in dark blue and second-best in light blue.. Domain categories include Research\\nReports/Introductions (Res.), Tutorials/Workshops (Tut.), Academic Papers (Acad.), Guidebooks'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='sults are highlighted in dark blue and second-best in light blue.. Domain categories include Research\\nReports/Introductions (Res.), Tutorials/Workshops (Tut.), Academic Papers (Acad.), Guidebooks\\n(Guid.), Brochures (Broch.), Administration/Industry Files (Admin.), and Financial Reports (Fin.).\\nMethod\\nDomains\\nOverall\\nRes.\\nTut.\\nAcad.\\nGuid.\\nBroch.\\nAdmin.\\nFin.\\nGPT-4o-mini\\n35.5\\n44.0\\n24.6\\n33.1\\n29.5\\n46.8\\n31.1\\n33.5\\nLightRAG\\n40.8\\n34.1\\n36.2\\n39.4\\n41.0\\n44.4\\n38.3\\n38.9\\nMMGraphRAG\\n40.8\\n36.5\\n35.7\\n35.8\\n28.2\\n46.9\\n38.5\\n37.7\\nRAGAnything\\n46.6\\n43.5\\n38.7\\n43.9\\n34.0\\n45.7\\n43.6\\n42.8\\nOutputs are constrained to a one-sentence format. For the baseline GPT-4o-mini in our QA scenario,\\ndocuments are concatenated into image form with a maximum of 50 pages per document, rendered at\\n144 dpi. Finally, all query results are evaluated for accuracy by GPT-4o-mini.\\n3.2\\nPERFORMANCE COMPARISON\\nSuperior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='144 dpi. Finally, all query results are evaluated for accuracy by GPT-4o-mini.\\n3.2\\nPERFORMANCE COMPARISON\\nSuperior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior\\noverall performance over baselines through its unified multimodal framework. Unlike LightRAG,\\nwhich is restricted to text-only content processing, RAG-Anything treats text, images, tables, and\\nequations as first-class entities. MMGraphRAG only adds basic image processing while treating\\ntables and equations as plain text, missing crucial structural information. RAG-Anything introduces\\na comprehensive dual-graph construction strategy that preserves structural relationships across all\\nmodalities. This unified approach enables superior performance across both evaluation benchmarks.\\nEnhanced Long-Context Performance. RAG-Anything demonstrates superior performance on\\nlong-context documents. The framework excels where relevant evidence is dispersed across multiple'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='Enhanced Long-Context Performance. RAG-Anything demonstrates superior performance on\\nlong-context documents. The framework excels where relevant evidence is dispersed across multiple\\nmodalities and sections. It achieves the best results in information-dense domains such as Research\\nReports and Financial Reports on MMLongBench. These improvements stem from the structured\\ncontext injection mechanism. This mechanism integrates dual-graph construction for cross-page entity\\nalignment. It combines semantic retrieval with structural navigation. The framework also employs\\nmodality-aware processing for efficient context window utilization. Unlike baselines that cannot\\nuniformly process diverse modalities, RAG-Anything effectively captures scattered multimodal\\nevidence. Its cross-modal hybrid retrieval architecture combines structural knowledge navigation\\nwith semantic similarity matching. This enables the framework to leverage both explicit relationships'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='evidence. Its cross-modal hybrid retrieval architecture combines structural knowledge navigation\\nwith semantic similarity matching. This enables the framework to leverage both explicit relationships\\nand implicit semantic connections across modalities.\\nTo systematically evaluate model performance across varying document lengths, we conducted\\ncomprehensive experiments on both datasets. As illustrated in Figure 2, RAG-Anything and MM-\\nGraphRAG exhibit comparable performance on shorter documents. However, RAG-Anything’s\\nadvantages become increasingly pronounced as document length grows. On DocBench, the perfor-\\nmance gap expands dramatically to over 13 points for documents exceeding 100 pages (68.2% vs.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 2: Performance evaluation across documents of varying lengths.\\nTable 4: Ablation study results on DocBench. The “Chunk-only” variant bypasses dual-graph\\nconstruction and relies solely on traditional chunk-based retrieval, while “w/o Reranker” eliminates\\ncross-modal reranking but preserves the core graph-based architecture.\\nMethod\\nDomains\\nTypes\\nOverall\\nAca.\\nFin.\\nGov.\\nLaw.\\nNews\\nTxt.\\nMm.\\nUna.\\nChunk-only\\n55.8\\n61.5\\n60.1\\n60.7\\n64.0\\n81.6\\n66.2\\n43.5\\n60.0\\nw/o Reranker\\n60.9\\n63.5\\n58.8\\n60.2\\n68.6\\n81.7\\n74.7\\n45.4\\n62.4\\nRAGAnything\\n61.4\\n67.0\\n61.5\\n60.2\\n66.3\\n85.0\\n76.3\\n46.0\\n63.4\\n54.6% for 101–200 pages; 68.8% vs. 55.0% for 200+ pages). On MMLongBench, RAG-Anything\\ndemonstrates consistent improvements across all length categories, achieving accuracy gains of 3.4\\npoints for 11–50 pages, 9.3 points for 51–100 pages, and 7.9 points for 101–200 pages. These\\nfindings confirm that our dual-graph construction and cross-modal hybrid retrieval mechanism is'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='points for 11–50 pages, 9.3 points for 51–100 pages, and 7.9 points for 101–200 pages. These\\nfindings confirm that our dual-graph construction and cross-modal hybrid retrieval mechanism is\\nparticularly effective for long-document reasoning tasks.\\n3.3\\nARCHITECTURAL VALIDATION WITH ABLATION STUDIES\\nTo isolate and quantify the contributions of key architectural components in RAG-Anything, we\\nconducted systematic ablation studies examining two critical design choices. Given that our approach\\nfundamentally differs from existing methods through dual-graph construction and hybrid retrieval,\\nwe specifically evaluated: i) Chunk-only, which bypasses graph construction entirely and relies\\nsolely on traditional chunk-based retrieval, and ii) w/o Reranker, which eliminates the cross-modal\\nreranking component while preserving the core graph-based architecture.\\nAs demonstrated in Table 4, the results validate our architectural design through striking performance'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='reranking component while preserving the core graph-based architecture.\\nAs demonstrated in Table 4, the results validate our architectural design through striking performance\\nvariations. • Graph Construction is Essential. The chunk-only variant achieves merely 60.0%\\naccuracy with substantial cross-domain drops. This demonstrates that traditional chunking fails to\\ncapture structural and cross-modal relationships essential for multimodal documents. • Reranking\\nProvides Marginal Gains. Removing the reranker yields only a modest decline to 62.4%, while the\\nfull model achieves 63.4% accuracy. This indicates that cross-modal reranking provides valuable\\nrefinement, but primary gains stem from our graph-based retrieval and cross-modal integration.\\n3.4\\nCASE STUDIES\\nMultimodal documents contain rich structural information within each modality. Understanding\\nthese intra-modal structures is crucial for accurate reasoning. We analyze two representative cases'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='Multimodal documents contain rich structural information within each modality. Understanding\\nthese intra-modal structures is crucial for accurate reasoning. We analyze two representative cases\\nfrom DocBench to demonstrate how RAG-Anything leverages these structures. These cases highlight\\na key limitation of existing methods. Baselines either rely on superficial textual cues or flatten\\ncomplex visual elements into plain text. In contrast, RAG-Anything builds modality-aware graphs\\nthat preserve essential relationships (e.g., table header↔cell↔unit edges; panel↔caption↔axis\\nedges). This enables precise reasoning over complex document layouts.\\n• Case 1: Multi-panel Figure Interpretation. This case examines a common scenario in academic\\nliterature. Researchers often need to compare results across different experimental conditions. These\\nresults are typically presented in multi-panel visualizations. Figure 3 shows a challenging t-SNE\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content=\"RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nMultimodel Document\\nEvidence figure in the document\\nQuestion: Which model's style space shows a clearer separation\\nbetween different styles according to Figure 2?\\nGPT-4o-mini🤔: \\nAccording to Figure 2, the VAE\\nmodel's style space shows a clearer\\nseparation between different styles.\\nMMGraphRAG🤔: \\nAccording to Figure 2, the model's style space\\nshows a clearer separation between different styles\\nin the Variational Autoencoder (VAE) compared to\\nthe Deterministic Autoencoder (DAE).\\nRAG-Anything(Correct😉):\\nThe DAE model's style space shows a clearer\\nseparation between different styles according to\\nFigure 2.\\nLightRAG🤔：\\nAccording to Figure 2, the Variational Autoencoder\\n(VAE) shows a clearer separation between different\\nstyles \\nin \\nits \\nstyle \\nspace \\ncompared \\nto \\nthe\\nDeterministic Autoencoder (DAE). \\n(DAE shows a clearer seperation than VAE in Style Space)\\nFigure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='to \\nthe\\nDeterministic Autoencoder (DAE). \\n(DAE shows a clearer seperation than VAE in Style Space)\\nFigure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation\\npatterns from the style-space panel, while avoiding confusion from the adjacent content-space panel.\\nvisualization with multiple subpanels. The query requires distinguishing between two related but\\ndistinct panels. RAG-Anything constructs a visual-layout graph where panels, axis titles, legends,\\nand captions become nodes. Key edges encode semantic relationships. Panels contain specific plots.\\nCaptions provide contextual information. Subfigures relate hierarchically. This structure guides the\\nretriever to focus on the style-space panel for comparing cluster separation patterns. The system\\navoids confusion from the adjacent content space panel. This panel shows less clear distinctions.\\nMultimodel Document\\nEvidence table in the document'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content=\"avoids confusion from the adjacent content space panel. This panel shows less clear distinctions.\\nMultimodel Document\\nEvidence table in the document\\nQuestion: What was Novo Nordisk's total amount spent on wages and salaries in 2020?\\nGPT-4o-mini🤔: \\nNovo Nordisk's total amount\\nspent on wages and salaries in\\n2020 was DKK 32,928 million.\\nMMGraphRAG🤔: \\nNovo Nordisk spent a total of\\n11,503 million DKK on wages\\nand salaries in 2020.\\nRAG-Anything(Correct😉):\\nNovo Nordisk's total amount spent on wages\\nand salaries in 2020 was DKK 26,778 million. \\nLightRAG🤔：\\nNovo Nordisk spent DKK 11,503 million\\non wages and salaries in 2020.\\n(Identifying the true evidence is the key)\\nFigure 4: Financial table navigation case. The query involves locating the specific intersection of\\n“Wages and salaries” row and “2020” column amid similar terminological entries.\\n• Case 2: Financial Table Navigation. This case addresses a common challenge in financial\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='“Wages and salaries” row and “2020” column amid similar terminological entries.\\n• Case 2: Financial Table Navigation. This case addresses a common challenge in financial\\ndocument analysis. Analysts must extract specific metrics from tables with similar terminology\\nand multiple time periods. Figure 4 shows this scenario. The query involves resolving ambiguous\\nfinancial terms and selecting the correct column for a specified year.\\nRAG-Anything transforms the financial report table into a structured graph. Each row header, column\\nheader (year), data cell, and unit becomes a node. The edges capture key relationships: row-of,\\ncolumn-of, header-applies-to, and unit-of. This structure enables precise navigation. The retriever\\nfocuses on the row “Wages and salaries” and the column for “2020”. It directs attention to the\\ntarget cell (26,778 million). The system successfully disambiguates nearby entries like “Share-based'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='focuses on the row “Wages and salaries” and the column for “2020”. It directs attention to the\\ntarget cell (26,778 million). The system successfully disambiguates nearby entries like “Share-based\\npayments.” Competing methods treat tables as linear text. They often confuse numerical spans and\\nyears. This leads to significantly inaccurate answers. RAG-Anything explicitly models relationships\\nwithin the table. It achieves precise selection and numeric grounding. This ensures accurate responses.\\n• Key Insights. Both cases demonstrate how RAG-Anything’s structure-aware design delivers\\ntargeted advantages. Our approach transforms documents into explicit graph representations. These\\ngraphs capture intra-modal relationships that traditional methods miss. In figures, connections\\nbetween panels, captions, and axes enable panel-level comparisons. This goes beyond keyword\\nmatching. In tables, row–column–unit graphs ensure accurate identification through modeling.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='between panels, captions, and axes enable panel-level comparisons. This goes beyond keyword\\nmatching. In tables, row–column–unit graphs ensure accurate identification through modeling.\\nThis structure-aware retrieval design reduces confusion from repeated terminology and complex\\nlayouts. Traditional RAG systems struggle with these scenarios due to lack of structural understanding.\\nEven MMGraphRAG fails here because it only considers image modality entities. It ignores other\\nmodality entities like table cells, row headers, and column headers. RAG-Anything’s comprehensive\\ngraph representation captures all modality-specific entities and their relationships. This enables\\nprecise, modality-specific grounding that leads to consistent improvements in document Q&A tasks\\nrequiring fine-grained localization. Additional cases are available in Appendix A.2.\\n4\\nRELATED WORK\\n• Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content='requiring fine-grained localization. Additional cases are available in Appendix A.2.\\n4\\nRELATED WORK\\n• Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with\\nlong-context inputs and multi-hop queries, failing to precisely locate dispersed evidence (Zhang et al.,\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\n2025). Graph structures address this limitation by introducing explicit relational modeling, improving\\nboth retrieval efficiency and reasoning accuracy (Bei et al., 2025).\\nSince GraphRAG (Edge et al., 2024), research has evolved along two complementary directions.\\nFirst, graph construction approaches optimize structures for retrieval efficiency, ranging from Ligh-\\ntRAG’s (Guo et al., 2024) sparsified indices to neural models like GNN-RAG (Mavromatis & Karypis,\\n2024) and memory-augmented variants like HippoRAG (Jimenez Gutierrez et al., 2024). Second,\\nknowledge aggregation approaches integrate information for multi-level reasoning through hier-\\narchical methods like RAPTOR (Sarthi et al., 2024) and ArchRAG (Wang et al., 2025). Despite\\nthese advances, existing systems remain text-centric with homogeneous inputs. This limits their\\napplicability to multimodal documents and constrains robust reasoning over heterogeneous content.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='these advances, existing systems remain text-centric with homogeneous inputs. This limits their\\napplicability to multimodal documents and constrains robust reasoning over heterogeneous content.\\nRAG-Anything addresses this gap by extending GraphRAG to all modalities.\\n• Multimodal Retrieval-Augmented Generation. Multimodal RAG represents a natural evolution\\nfrom text-based RAG systems, addressing the need to integrate external knowledge from diverse\\ndata modalities for comprehensive response generation (Abootorabi et al., 2025). However, current\\napproaches are fundamentally constrained by their reliance on modality-specific architectures. Exist-\\ning methods demonstrate these constraints across domains: VideoRAG (Ren et al., 2025) employs\\ndual-channel architectures for video understanding while MM-VID (Lin et al., 2023) converts videos\\nto text, losing visual information; VisRAG (Yu et al., 2025) preserves document layouts as images'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='dual-channel architectures for video understanding while MM-VID (Lin et al., 2023) converts videos\\nto text, losing visual information; VisRAG (Yu et al., 2025) preserves document layouts as images\\nbut misses granular relationships; MMGraphRAG (Wan & Yu, 2025) links scene graphs with textual\\nrepresentations but suffers from structural blindness—treating tables and formulas as plain text\\nwithout proper entity extraction, losing structural information for reasoning.\\nThe fundamental problem underlying these limitations is architectural fragmentation. Current systems\\nrequire specialized processing pipelines for each modality. This creates poor generalizability as new\\nmodalities demand custom architectures and fusion mechanisms. Such fragmentation introduces\\ncross-modal alignment difficulties, modality biases, and information bottlenecks. These issues\\nsystematically compromise system performance and scalability. RAG-Anything addresses this'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='cross-modal alignment difficulties, modality biases, and information bottlenecks. These issues\\nsystematically compromise system performance and scalability. RAG-Anything addresses this\\nfragmentation through a unified graph-based framework. Our approach processes all modalities with\\nconsistent structured modeling. This eliminates architectural constraints while preserving multimodal\\ninformation integrity. The result is seamless cross-modal reasoning across heterogeneous content.\\n5\\nCONCLUSION\\nRAG-Anything introduces a paradigm shift in multimodal retrieval through its unified graph-based\\nframework. Our core technical innovation is the dual-graph construction strategy that seamlessly\\nintegrates cross-modal and text-based knowledge graphs. Rather than forcing diverse modalities into\\ntext-centric pipelines that lose critical structural information, our approach fundamentally reconcep-\\ntualizes multimodal content as interconnected knowledge entities with rich semantic relationships.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='tualizes multimodal content as interconnected knowledge entities with rich semantic relationships.\\nThe hybrid retrieval mechanism strategically combines structural navigation with semantic matching,\\nenabling precise reasoning over complex document layouts. Comprehensive evaluation demonstrates\\nsuperior performance on long-context documents, particularly those exceeding 100 pages where\\ntraditional methods fail. This work establishes a new foundation for multimodal RAG systems that\\ncan handle the heterogeneous nature of diverse information landscapes.\\nOur analysis in Appendix A.5 reveals critical challenges facing current multimodal RAG systems.\\nTwo fundamental issues emerge through systematic failure case examination. First, systems exhibit\\ntext-centric retrieval bias, preferentially accessing textual sources even when queries explicitly\\nrequire visual information. Second, rigid spatial processing patterns fail to adapt to non-standard'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content='text-centric retrieval bias, preferentially accessing textual sources even when queries explicitly\\nrequire visual information. Second, rigid spatial processing patterns fail to adapt to non-standard\\ndocument layouts. These limitations manifest in cross-modal misalignment scenarios and structurally\\nambiguous tables. The findings highlight the need for adaptive spatial reasoning and layout-aware\\nparsing mechanisms to handle real-world multimodal document complexity.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nREFERENCES\\nMohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammad-\\nkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin\\nAsgari. Ask in any modality: A comprehensive survey on multimodal retrieval-augmented genera-\\ntion. arXiv preprint arXiv:2502.08826, 2025.\\nYuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu,\\nShirui Pan, Yizhou Yu, et al. Graphs meet ai agents: Taxonomy, progress, and future opportunities.\\narXiv preprint arXiv:2506.18019, 2025.\\nDarren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,\\nDasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A\\ngraph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-\\naugmented generation. arXiv preprint arXiv:2410.05779, 2024.\\nBernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neuro-\\nbiologically inspired long-term memory for large language models. NeurIPS, 37:59532–59569,\\n2024.\\nKevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng\\nWang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang. Mm-vid: Advancing video\\nunderstanding with gpt-4v(ision). arXiv preprint arXiv:2310.19773, 2023.\\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information Processing Systems, 37:95963–96010, 2024.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information Processing Systems, 37:95963–96010, 2024.\\nCostas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139, 2024.\\nXubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, and Chao Huang.\\nVide-\\norag:\\nRetrieval-augmented generation with extreme long-context videos.\\narXiv preprint\\narXiv:2502.01549, 2025.\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.\\nRaptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International\\nConference on Learning Representations, 2024.\\nXueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal\\nknowledge graphs. arXiv preprint arXiv:2507.20804, 2025.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Conference on Learning Representations, 2024.\\nXueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal\\nknowledge graphs. arXiv preprint arXiv:2507.20804, 2025.\\nBin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu,\\nYuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content\\nextraction. arXiv preprint arXiv:2409.18839, 2024.\\nShu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. Archrag: Attributed community-\\nbased hierarchical retrieval-augmented generation. arXiv preprint arXiv:2502.09891, 2025.\\nShi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang,\\nXu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on\\nmulti-modality documents. arXiv preprint arXiv:2410.10594, 2025.\\nQinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='multi-modality documents. arXiv preprint arXiv:2410.10594, 2025.\\nQinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen,\\nYilin Xiao, Chuang Zhou, Yi Chang, and Xiao Huang. A survey of graph retrieval-augmented\\ngeneration for customized large language models. arXiv preprint arXiv:2501.13958, 2025.\\nAnni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and\\nDong Yu. Docbench: A benchmark for evaluating llm-based document reading systems. arXiv\\npreprint arXiv:2407.10701, 2024.\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nA\\nAPPENDIX\\nThis appendix provides comprehensive supporting materials for our experimental evaluation and\\nimplementation details. Section A.1 presents detailed dataset statistics for the DocBench and\\nMMLongBench multi-modal benchmarks, including document type distributions and complexity\\nmetrics. Section A.2 showcases additional case studies that demonstrate RAG-Anything’s structure-\\naware capabilities across diverse multimodal content understanding tasks. Section A.3 documents the\\ncomplete set of multimodal analysis prompts for vision, table, and equation processing that enable\\ncontext-aware interpretation. Section A.4 provides the standardized accuracy evaluation prompt used\\nfor consistent response assessment across all experimental conditions.\\nA.1\\nDATASET CHARACTERISTICS AND STATISTICS\\nTable 5: Document type distribution and statistics for the DocBench benchmark.\\nType\\nAcad.\\nFin.\\nGov.\\nLaw.\\nNews\\n# Docs\\n49\\n40\\n44\\n46\\n50\\n# Questions\\n303\\n288\\n148\\n191'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='DATASET CHARACTERISTICS AND STATISTICS\\nTable 5: Document type distribution and statistics for the DocBench benchmark.\\nType\\nAcad.\\nFin.\\nGov.\\nLaw.\\nNews\\n# Docs\\n49\\n40\\n44\\n46\\n50\\n# Questions\\n303\\n288\\n148\\n191\\n172\\nAvg. Pages\\n11\\n192\\n69\\n58\\n1\\nTable 6: Document type distribution and statistics for the MMLongBench benchmark.\\nType\\nRes.\\nTut.\\nAcad.\\nGuid.\\nBroch.\\nAdmin.\\nFin.\\n# Docs\\n34\\n17\\n26\\n22\\n15\\n10\\n11\\n# Questions\\n292\\n138\\n199\\n155\\n100\\n81\\n117\\nAvg. Pages\\n39\\n58\\n35\\n78\\n30\\n17\\n87\\nTables 5 and 6 present the distribution of document types across the DocBench and MMLong-\\nBench benchmarks. • DocBench encompasses medium- to long-length documents spanning various\\ndomains, including legal, governmental, and financial files. Financial reports represent the most\\nextensive category, averaging 192 pages per document, while the News category consists of concise\\nsingle-page newspapers. • MMLongBench demonstrates a broader spectrum of document types and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='extensive category, averaging 192 pages per document, while the News category consists of concise\\nsingle-page newspapers. • MMLongBench demonstrates a broader spectrum of document types and\\nlengths. Research reports, tutorials, and academic papers maintain moderate lengths of 35–58 pages\\non average, while guidebooks extend to approximately 78 pages. Brochures and administrative files\\nremain relatively compact, whereas financial reports again emerge as the longest category.\\nCollectively, these two benchmarks provide comprehensive coverage ranging from brief news arti-\\ncles to extensive technical and financial documentation. This establishes diverse and challenging\\nevaluation contexts for multimodal document understanding tasks.\\nA.2\\nADDITIONAL CASE STUDIES\\nMultimodel Document\\nEvidence figure in the document\\nQuestion: Which GCAN sub-model component removal\\nresulted in the lowest accuracy for Twitter15?\\nGPT-4o-mini🤔: \\nThe removal of the dual co-\\nattention mechanism from the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='Evidence figure in the document\\nQuestion: Which GCAN sub-model component removal\\nresulted in the lowest accuracy for Twitter15?\\nGPT-4o-mini🤔: \\nThe removal of the dual co-\\nattention mechanism from the\\nGCAN sub-model resulted in the\\nlowest accuracy for Twitter15.\\nMMGraphRAG🤔: \\nThe removal of the source tweet from the GCAN\\nmodel resulted in the lowest accuracy for\\nTwitter15, as indicated by significant accuracy\\ndrops in the ablation analysis in Figure 4.\\nRAG-Anything(Correct😉):\\nThe removal of the source tweet embeddings\\nand dual co-attention, indicated as model\\nconfiguration \"-S-A,\" resulted in the lowest\\naccuracy for Twitter15.\\nLightRAG🤔：\\nRemoving the source tweet from the GCAN model\\nresulted in the lowest accuracy for Twitter15, as\\nindicated by a significant drop in performance when\\nusing the sub-model that excluded both source tweet\\nembeddings and dual co-attention. \\nFigure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content='using the sub-model that excluded both source tweet\\nembeddings and dual co-attention. \\nFigure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy\\nconfiguration, while baselines misinterpret spatial relationships.\\n• Demonstrating Visual Reasoning Capabilities. Figure 5 illustrates how RAG-Anything handles\\ncomplex visual reasoning tasks involving chart interpretation. The query asks which GCAN sub-\\nmodel component removal yields the lowest accuracy on Twitter15. Traditional approaches struggle\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nwith spatial relationships between visual elements. RAG-Anything addresses this challenge by\\nconstructing a structured graph representation of the bar plot. Bars, axis labels, and legends become\\ninterconnected nodes. These are linked by semantic relations such as bar-of and label-applies-to.\\nThis graph-based approach enables precise alignment between visual and textual elements. The\\nsystem correctly identifies the bar labeled \"-S-A\" (removing source tweet embeddings and dual\\nco-attention) and its corresponding accuracy value as the lowest performer. Baseline methods that\\nflatten visual information often misinterpret spatial relationships. They frequently conflate nearby\\ncomponents. RAG-Anything’s structured representation preserves critical visual-textual associations.\\nThis leads to accurate query resolution and proper attribution of performance drops to \"-S-A\".\\nMultimodel Document\\nEvidence table in the document'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='This leads to accurate query resolution and proper attribution of performance drops to \"-S-A\".\\nMultimodel Document\\nEvidence table in the document\\nQuestion: Which model combination for the Evidence Inference\\ndataset has the highest AUPRC value?\\nGPT-4o-mini🤔: \\nThe model combination for the Evidence Inference\\ndataset with the highest AUPRC value is the BERT-\\nto-BERT model, achieving an AUPRC of 0.455.\\nMMGraphRAG🤔: \\nThe model combination for the Evidence Inference\\ndataset with the highest AUPRC value is the \"Bert-To-\\nBert\" model, which achieved an AUPRC score of\\n0.708.\\nRAG-Anything(Correct😉):\\nThe model combination for the Evidence\\nInference dataset with the highest AUPRC\\nvalue is GloVe + LSTM - Attention, achieving\\na value of 0.506. \\nLightRAG🤔：\\nThe model combination \"BERT + LSTM -\\nAttention\" has the highest AUPRC value for the\\nEvidence Inference dataset at 0.429. \\n(Table with spatial ambiguity)\\nFigure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='Evidence Inference dataset at 0.429. \\n(Table with spatial ambiguity)\\nFigure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while\\nthe compared approaches struggle with structural ambiguity.\\n• Handling Complex Tabular Structures. Figure 6 showcases RAG-Anything’s ability to navigate\\nintricate tabular data where structural disambiguation is crucial. The query seeks the model combi-\\nnation achieving the highest AUPRC value for the Evidence Inference dataset—a task complicated\\nby repeated row labels across multiple datasets within the same table. This scenario highlights a\\nfundamental limitation of conventional approaches that struggle with structural ambiguity in data.\\nRAG-Anything overcomes this by parsing the table into a comprehensive relational graph where\\nheaders and data cells become nodes connected through explicit row-of and column-of relationships.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='RAG-Anything overcomes this by parsing the table into a comprehensive relational graph where\\nheaders and data cells become nodes connected through explicit row-of and column-of relationships.\\nThis structured representation enables the system to correctly isolate the Evidence Inference dataset\\ncontext and identify \"GloVe + LSTM – Attention\" with a score of 0.506 as the optimal configuration.\\nBy explicitly preserving hierarchical table constraints that other methods often collapse or misinterpret,\\nRAG-Anything ensures reliable reasoning across complex multi-dataset tabular structures.\\nA.3\\nCONTEXT-AWARE MULTIMODAL PROMPTING\\nThese three prompts orchestrate structured, context-aware multimodal analysis with JSON-formatted\\noutputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular,\\nand mathematical content while maintaining explicit alignment with surrounding information.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='outputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular,\\nand mathematical content while maintaining explicit alignment with surrounding information.\\nVision Analysis Prompt. Figure 7 orchestrates comprehensive image-context integration. The\\nprompt directs the model to systematically capture compositional elements, object relationships,\\nvisual attributes, stylistic features, dynamic actions, and technical components (e.g., charts), while es-\\ntablishing explicit connections to accompanying text. This approach transcends superficial description,\\nenabling contextually-grounded interpretations that enhance knowledge retrieval and substantiation.\\nTable Analysis Prompt. Figure 8 structures systematic tabular content decomposition across multiple\\nanalytical dimensions: structural organization, column semantics, critical values, statistical patterns,\\nand contextual relevance. Through precise terminology and numerical accuracy requirements, the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='analytical dimensions: structural organization, column semantics, critical values, statistical patterns,\\nand contextual relevance. Through precise terminology and numerical accuracy requirements, the\\nprompt eliminates ambiguous generalizations and ensures faithful preservation of key indicators\\nwhile maintaining coherent alignment with surrounding discourse.\\nEquation Analysis Prompt. Figure 9 prioritizes semantic interpretation over syntactic restatement\\nof mathematical expressions. The prompt instructs comprehensive analysis of variable definitions,\\noperational logic, theoretical foundations, inter-formula relationships, and practical applications. This\\nmethodology ensures mathematical content becomes integral to broader argumentative frameworks,\\nsupporting enhanced retrieval accuracy, analytical traceability, and reasoning coherence.\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 7: Vision analysis prompt for context-aware image interpretation and knowledge extraction.\\nFigure 8: Table analysis prompt for structured content decomposition and semantic understanding.\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 9: Equation analysis prompt for mathematical expression interpretation and integration.\\nFigure 10: Accuracy evaluation prompt for consistent factual assessment across question types.\\nA.4\\nACCURACY EVALUATION PROMPT DESIGN\\nFigure 10 presents the standardized prompt specifically designed for systematic factual accuracy as-\\nsessment of generated responses across multiple domains. The prompt establishes explicit evaluation\\ncriteria that prioritize content correctness over stylistic considerations, producing binary accuracy\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nclassifications accompanied by concise analytical justifications. All accuracy evaluations throughout\\nour comprehensive experimental framework were conducted using GPT-4o-mini, ensuring consistent\\nand reliable assessment standards across diverse question categories and specialized domains.\\nA.5\\nCHALLENGES AND FUTURE DIRECTIONS FOR MULTI-MODAL RAG\\nWhile current multimodal RAG systems demonstrate promising capabilities, their limitations emerge\\nmost clearly through systematic analysis of failure cases. Understanding where and why these systems\\nbreak down is crucial for advancing the field beyond current performance plateaus. Examining failure\\npatterns helps identify fundamental architectural bottlenecks and design principles for more robust\\nmultimodal systems. Our investigation reveals two critical failure patterns exposing deeper systemic\\nissues in multimodal RAG architectures. These patterns are not merely edge cases but reflect'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='multimodal systems. Our investigation reveals two critical failure patterns exposing deeper systemic\\nissues in multimodal RAG architectures. These patterns are not merely edge cases but reflect\\nfundamental challenges in cross-modal information integration and structural reasoning:\\n• Text-Centric Retrieval Bias: Systems exhibit strong preference for textual sources, even when\\nqueries explicitly demand visual information. This reveals inadequate cross-modal attention.\\n• Document Structure Processing Challenges: Systems struggle with complex layouts and non-\\nlinear information flows. This exposes limitations in spatial reasoning and contextual understanding.\\nThese failure modes illuminate key insights about current multimodal AI. They provide concrete\\ndirections for architectural innovations that could substantially improve system robustness.\\nQuestion: According to Figure 3, what is the sequence followed to generate the\\nfinal labels for Named Entity Recognition?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Question: According to Figure 3, what is the sequence followed to generate the\\nfinal labels for Named Entity Recognition?\\nExpected answer: The sequence is Characters -> Char Embedding -> Char\\nLSTM-B/F -> Word Features -> Word Representation -> Word LSTM-F/B ->\\nWord CRF -> Label.\\nMultimodel Document\\nEvidence figure in the document\\nGPT-4o-mini🤔: \\n...involves inputting words and their representations\\ninto a Word CRF layer, followed by Word LSTM-B,\\nWord LSTM-F, and character-level representations,\\nculminating in the final prediction of entity labels.\\nMMGraphRAG🤔: \\n...involves using a combined representation of word\\nembeddings and character-level convolution, which is\\nthen processed through a Bi-LSTM layer, followed by\\na softmax or CRF layer to produce the final labels.\\nRAG-Anything🤔:\\n...involves processing input characters through\\nbi-directional LSTM layers (Char LSTM-B\\nand Char LSTM-F) followed by a softmax or\\nCRF layer that produces the final labels.\\nLightRAG🤔：'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='RAG-Anything🤔:\\n...involves processing input characters through\\nbi-directional LSTM layers (Char LSTM-B\\nand Char LSTM-F) followed by a softmax or\\nCRF layer that produces the final labels.\\nLightRAG🤔：\\n...a sequence where word representations are first\\nprocessed through a convolution layer, followed by a\\nBi-LSTM layer and then a softmax or CRF layer to\\nproduce the final labels. \\n(The order in the figure is from bottom to top)\\nFigure 11: Cross-modal noise case. All methods fail to retrieve the correct answer from the specified\\nimage, instead retrieving noisy textual evidence that misaligns with the structured visual content.\\nMultimodel Document\\nEvidence table in the document\\nQuestion: Which model\\'s style space shows a clearer separation\\nbetween different styles according to Figure 2?\\nGPT-4o-mini🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved by 6.26% when using the GEM fine-tuning\\nstrategy compared to training from scratch, achieving\\n65.35% versus 59.09%.\\nMMGraphRAG🤔:'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='GPT-4o-mini🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved by 6.26% when using the GEM fine-tuning\\nstrategy compared to training from scratch, achieving\\n65.35% versus 59.09%.\\nMMGraphRAG🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved from 44.24% when training from scratch to\\n59.83% when using the GEM fine-tuning strategy,\\nresulting in an improvement of 15.59%.\\nRAG-Anything🤔:\\nThe Joint Goal Accuracy in the \"Train\" domain\\nimproved from 44.24% with training from\\nscratch to 58.10% using the GEM fine-tuning\\nstrategy, resulting in an improvement of\\n13.86%. \\nLightRAG🤔：\\nThe Joint goal accuracy in the \"Train\" domain\\nimproved from 44.24% using training from scratch to\\n50.51% with GEM fine-tuning, indicating an\\nimprovement of 6.27%. \\n(An irregular table)\\nFigure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table\\nlayout with merged cells and unclear column boundaries, leading to incorrect data extraction.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Figure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table\\nlayout with merged cells and unclear column boundaries, leading to incorrect data extraction.\\nCase 1: Cross-Modal Misalignment. Figure 11 presents a particularly revealing failure scenario\\nwhere all evaluated methods consistently produce incorrect answers despite having access to the\\nnecessary information. This universal failure across different architectures suggests fundamental\\nlimitations in how current systems handle noisy, heterogeneous multimodal data—a critical challenge\\nas real-world applications inevitably involve imperfect, inconsistent information sources. The failure\\nexposes two interconnected systemic issues that compound each other:\\nIssue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward\\ntextual passages. This occurs particularly when visual content lacks exact keyword matches. The'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content='Issue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward\\ntextual passages. This occurs particularly when visual content lacks exact keyword matches. The\\nbias persists even when queries contain explicit instructions to prioritize visual sources. This reveals\\na fundamental weakness in cross-modal attention mechanisms.\\nThe retrieved textual information, while topically related, often operates at a different granularity level\\nthan visual content. Images may contain precise, structured data such as specific numerical values,\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\ndetailed diagrams, or exact spatial relationships. Corresponding text typically provides general,\\nconceptual descriptions. This semantic misalignment introduces noise that actively misleads the\\nreasoning process. The system attempts to reconcile incompatible levels of detail and specificity.\\nIssue 2: Rigid Spatial Processing Patterns. Current visual processing models exhibit fundamental\\nrigidity in spatial interpretation. Most systems default to sequential scanning patterns—top-to-\\nbottom and left-to-right—that mirror natural reading conventions. While effective for simple text\\ndocuments, this approach creates systematic failures with structurally complex real-world content.\\nMany documents require non-conventional processing strategies. Tables demand column-wise\\ninterpretation, technical diagrams follow specific directional flows, and scientific figures embed'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Many documents require non-conventional processing strategies. Tables demand column-wise\\ninterpretation, technical diagrams follow specific directional flows, and scientific figures embed\\ncritical information in unexpectedly positioned annotations. These structural variations are prevalent\\nin professional documents, making adaptive spatial reasoning essential.\\nIn the observed failure case, the correct answer required integrating visual elements in reverse order\\nfrom the model’s default processing sequence. The system’s inability to recognize and adapt to this\\nstructural requirement led to systematic misinterpretation. This represents a fundamental architectural\\nlimitation where spatial reasoning remains static regardless of document context or query intent.\\nWhen spatial processing patterns are misaligned with document structure, the extracted information\\nbecomes not merely incomplete but actively misleading. This structural noise compounds other'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='When spatial processing patterns are misaligned with document structure, the extracted information\\nbecomes not merely incomplete but actively misleading. This structural noise compounds other\\nprocessing errors and can lead to confident but entirely incorrect conclusions.\\nCase 2: Structural Noise in Ambiguous Table Layouts. As shown in Figure 12, all methods failed\\nwhen confronted with a structurally ambiguous table. The primary failure stems from the table’s\\nconfusing design: the GEM row lacks dedicated cell boundaries, and the \"Joint\" and \"Slot\" columns\\nmerge without clear separation. These structural irregularities create parsing ambiguities that system-\\natically mislead extraction algorithms. This failure pattern reveals a critical vulnerability in current\\nRAG systems. When table structures deviate from standard formatting conventions—through merged\\ncells, unclear boundaries, or non-standard layouts—extraction methods consistently misinterpret cell'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '../data/pdf/all-in-one-rag.pdf', 'file_path': '../data/pdf/all-in-one-rag.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='RAG systems. When table structures deviate from standard formatting conventions—through merged\\ncells, unclear boundaries, or non-standard layouts—extraction methods consistently misinterpret cell\\nrelationships and conflate distinct data values. This exposes the brittleness of current approaches\\nwhen faced with real-world document variations that deviate from clean, structured formats.\\nThe case highlights two essential directions for enhancing robustness. RAG systems require layout-\\naware parsing mechanisms that can recognize and adapt to structural irregularities rather than\\nimposing rigid formatting assumptions. Additionally, integrating visual processing capabilities\\ncould significantly improve noise resilience, as visual models can leverage spatial relationships and\\ncontextual design cues that are lost in purely structural representations.\\n18')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks= split_documents(pdf_doc)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51979553",
   "metadata": {},
   "source": [
    "COnvert the text to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81722cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON\\nAGENTIC RAG\\nAditi Singh\\nDepartment of Computer Science\\nCleveland State University\\nCleveland, OH, USA\\na.singh22@csuohio.edu\\nAbul Ehtesham\\nThe Davey Tree Expert Company\\nKent, OH, USA\\nabul.ehtesham@davey.com\\nSaket Kumar\\nThe MathWorks Inc\\nNatick, MA, USA\\nsaketk@mathworks.com\\nTala Talaei Khoei\\nKhoury College of Computer Science\\nRoux Institute at Northeastern University\\nPortland, ME, USA\\nt.talaeikhoei@northeastern.edu\\nABSTRACT\\nLarge Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human-\\nlike text generation and natural language understanding. However, their reliance on static training\\ndata limits their ability to respond to dynamic, real-time queries, resulting in outdated or inaccurate\\noutputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\\nintegrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite',\n",
       " 'outputs. Retrieval-Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by\\nintegrating real-time data retrieval to provide contextually relevant and up-to-date responses. Despite\\nits promise, traditional RAG systems are constrained by static workflows and lack the adaptability\\nrequired for multi-step reasoning and complex task management.\\nAgentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding\\nautonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflec-\\ntion, planning, tool use, and multi-agent collaboration to dynamically manage retrieval strategies,\\niteratively refine contextual understanding, and adapt workflows through clearly defined operational\\nstructures ranging from sequential steps to adaptive collaboration. This integration enables Agentic\\nRAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\\napplications.',\n",
       " 'RAG systems to deliver unparalleled flexibility, scalability, and context-awareness across diverse\\napplications.\\nThis survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational\\nprinciples and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG archi-\\ntectures, highlights key applications in industries such as healthcare, finance, and education, and exam-\\nines practical implementation strategies. Additionally, it addresses challenges in scaling these systems,\\nensuring ethical decision-making, and optimizing performance for real-world applications, while\\nproviding detailed insights into frameworks and tools for implementing Agentic RAG 1. The GitHub\\nlink for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey.\\nKeywords Large Language Models (LLMs) · Artificial Intelligence (AI) · Natural Language Understanding ·',\n",
       " 'link for this survey is available at: https://github.com/asinghcsu/AgenticRAG-Survey.\\nKeywords Large Language Models (LLMs) · Artificial Intelligence (AI) · Natural Language Understanding ·\\nRetrieval-Augmented Generation (RAG) · Agentic RAG · Autonomous AI Agents · Reflection · Planning · Tool\\nUse · Multi-Agent Collaboration · Agentic Patterns · Contextual Understanding · Dynamic Adaptability · Scalability ·\\nReal-Time Data Retrieval · Taxonomy of Agentic RAG · Healthcare Applications · Finance Applications · Educational\\nApplications · Ethical AI Decision-Making · Performance Optimization · Multi-Step Reasoning\\n1GitHub link: https://github.com/asinghcsu/AgenticRAG-Survey\\narXiv:2501.09136v3  [cs.AI]  4 Feb 2025',\n",
       " '1\\nIntroduction\\nLarge Language Models (LLMs) [1, 2] [3], such as OpenAI’s GPT-4, Google’s PaLM, and Meta’s LLaMA, have signifi-\\ncantly transformed artificial intelligence (AI) with their ability to generate human-like text and perform complex natural\\nlanguage processing tasks. These models have driven innovation across diverse domains, including conversational\\nagents [4], automated content creation, and real-time translation. Recent advancements have extended their capabilities\\nto multimodal tasks, such as text-to-image and text-to-video generation [5], enabling the creation and editing of videos\\nand images from detailed prompts [6], which broadens the potential applications of generative AI.\\nDespite these advancements, LLMs face significant limitations due to their reliance on static pre-training data. This\\nreliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world',\n",
       " 'reliance often results in outdated information, hallucinated responses [7], and an inability to adapt to dynamic, real-world\\nscenarios. These challenges emphasize the need for systems that can integrate real-time data and dynamically refine\\nresponses to maintain contextual relevance and accuracy.\\nRetrieval-Augmented Generation (RAG) [8, 9] emerged as a promising solution to these challenges. By combining\\nthe generative capabilities of LLMs with external retrieval mechanisms [10], RAG systems enhance the relevance and\\ntimeliness of responses. These systems retrieve real-time information from sources such as knowledge bases [11], APIs,\\nor the web, effectively bridging the gap between static training data and the demands of dynamic applications. However,\\ntraditional RAG workflows remain limited by their linear and static design, which restricts their ability to perform\\ncomplex multi-step reasoning, integrate deep contextual understanding, and iteratively refine responses.',\n",
       " 'complex multi-step reasoning, integrate deep contextual understanding, and iteratively refine responses.\\nThe evolution of agents [12] has significantly enhanced the capabilities of AI systems. Modern agents, including\\nLLM-powered and mobile agents [13], are intelligent entities capable of perceiving, reasoning, and autonomously\\nexecuting tasks. These agents leverage agentic patterns, such as reflection [14], planning [15], tool use, and multi-agent\\ncollaboration [16], to enhance decision-making and adaptability.\\nFurthermore, these agents employ agentic workflow patterns [12, 13], such as prompt chaining, routing, parallelization,\\norchestrator-worker models, and evaluator-optimizer , to structure and optimize task execution. By integrating these\\npatterns, Agentic RAG systems can efficiently manage dynamic workflows and address complex problem-solving\\nscenarios. The convergence of RAG and agentic intelligence has given rise to Agentic Retrieval-Augmented Generation',\n",
       " 'scenarios. The convergence of RAG and agentic intelligence has given rise to Agentic Retrieval-Augmented Generation\\n(Agentic RAG) [14], a paradigm that integrates agents into the RAG pipeline. Agentic RAG enables dynamic retrieval\\nstrategies, contextual understanding, and iterative refinement [15], allowing for adaptive and efficient information\\nprocessing. Unlike traditional RAG, Agentic RAG employs autonomous agents to orchestrate retrieval, filter relevant\\ninformation, and refine responses, excelling in scenarios requiring precision and adaptability. The overview of Agentic\\nRAG is in figure 1.\\nThis survey explores the foundational principles, taxonomy, and applications of Agentic RAG. It provides a comprehen-\\nsive overview of RAG paradigms, such as Naïve RAG, Modular RAG, and Graph RAG [16], alongside their evolution\\ninto Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications',\n",
       " 'into Agentic RAG systems. Key contributions include a detailed taxonomy of Agentic RAG frameworks, applications\\nacross domains such as healthcare [17, 18], finance, and education [19], and insights into implementation strategies,\\nbenchmarks, and ethical considerations.\\nThe structure of this paper is as follows: Section 2 introduces RAG and its evolution, highlighting the limitations of\\ntraditional approaches. Section 3 elaborates on the principles of agentic intelligence and agentic patterns. Section 4\\nelaborates agentic workflow patterns. Section 5 provides a taxonomy of Agentic RAG systems, including single-agent,\\nmulti-agent, and graph-based frameworks. Section 6 provides comparative analysis of Agentic RAG frameworks.\\nSection 7 examines applications of Agentic RAG, while Section 8 discusses implementation tools and frameworks.\\nSection 9 focuses on benchmarks and dataset, and Section 10 concludes with future directions for Agentic RAG systems.\\n2',\n",
       " 'Section 9 focuses on benchmarks and dataset, and Section 10 concludes with future directions for Agentic RAG systems.\\n2\\nFoundations of Retrieval-Augmented Generation\\n2.1\\nOverview of Retrieval-Augmented Generation (RAG)\\nRetrieval-Augmented Generation (RAG) represents a significant advancement in the field of artificial intelligence,\\ncombining the generative capabilities of Large Language Models (LLMs) with real-time data retrieval. While LLMs\\nhave demonstrated remarkable capabilities in natural language processing, their reliance on static pre-trained data\\noften results in outdated or incomplete responses. RAG addresses this limitation by dynamically retrieving relevant\\ninformation from external sources and incorporating it into the generative process, enabling contextually accurate and\\nup-to-date outputs.\\n2',\n",
       " 'Figure 1: An Overview of Agentic RAG\\n2.2\\nCore Components of RAG\\nThe architecture of RAG systems integrates three primary components (Figure2):\\n• Retrieveal: Responsible for querying external data sources such as knowledge bases, APIs, or vector databases.\\nAdvanced retrievers leverage dense vector search and transformer-based models to improve retrieval precision\\nand semantic relevance.\\n• Augmentation: Processes retrieved data, extracting and summarizing the most relevant information to align\\nwith the query context.\\n• Generation: Combines retrieved information with the LLM’s pre-trained knowledge to generate coherent,\\ncontextually appropriate responses.\\n2.3\\nEvolution of RAG Paradigms\\nThe field of Retrieval-Augmented Generation (RAG) has evolved significantly to address the increasing complexity of\\nreal-world applications, where contextual accuracy, scalability, and multi-step reasoning are critical. What began as',\n",
       " 'real-world applications, where contextual accuracy, scalability, and multi-step reasoning are critical. What began as\\nsimple keyword-based retrieval has transitioned into sophisticated, modular, and adaptive systems capable of integrating\\ndiverse data sources and autonomous decision-making processes. This evolution underscores the growing need for\\nRAG systems to handle complex queries efficiently and effectively.\\nThis section examines the progression of RAG paradigms, presenting key stages of development—Naïve RAG,\\nAdvanced RAG, Modular RAG, Graph RAG, and Agentic RAG alongside their defining characteristics, strengths, and\\n3',\n",
       " 'Figure 2: Core Components of RAG\\nlimitations. By understanding the evolution of these paradigms, readers can appreciate the advancements made in\\nretrieval and generative capabilities and their application in various domains\\n2.3.1\\nNaïve RAG\\nNaïve RAG [20] represents the foundational implementation of retrieval-augmented generation. Figure 3 illustrates the\\nsimple retrieve-read workflow of Naive RAG, focusing on keyword-based retrieval and static datasets.. These systems\\nrely on simple keyword-based retrieval techniques, such as TF-IDF and BM25, to fetch documents from static datasets.\\nThe retrieved documents are then used to augment the language model’s generative capabilities.\\nFigure 3: An Overview of Naive RAG.\\nNaïve RAG is characterized by its simplicity and ease of implementation, making it suitable for tasks involving\\nfact-based queries with minimal contextual complexity. However, it suffers from several limitations:',\n",
       " 'fact-based queries with minimal contextual complexity. However, it suffers from several limitations:\\n• Lack of Contextual Awareness: Retrieved documents often fail to capture the semantic nuances of the query\\ndue to reliance on lexical matching rather than semantic understanding.\\n• Fragmented Outputs: The absence of advanced preprocessing or contextual integration often leads to\\ndisjointed or overly generic responses.\\n• Scalability Issues: Keyword-based retrieval techniques struggle with large datasets, often failing to identify\\nthe most relevant information.\\nDespite these limitations, Naïve RAG systems provided a critical proof-of-concept for integrating retrieval with\\ngeneration, laying the foundation for more sophisticated paradigms.\\n2.3.2\\nAdvanced RAG\\nAdvanced RAG [20] systems build upon the limitations of Naïve RAG by incorporating semantic understanding and\\nenhanced retrieval techniques. Figure 4 highlights the semantic enhancements in retrieval and the iterative, context-',\n",
       " 'enhanced retrieval techniques. Figure 4 highlights the semantic enhancements in retrieval and the iterative, context-\\naware pipeline of Advanced RAG. These systems leverage dense retrieval models, such as Dense Passage Retrieval\\n(DPR), and neural ranking algorithms to improve retrieval precision.\\nKey features of Advanced RAG include:\\n4',\n",
       " 'Figure 4: Overview of Advanced RAG\\n• Dense Vector Search: Queries and documents are represented in high-dimensional vector spaces, enabling\\nbetter semantic alignment between the user query and retrieved documents.\\n• Contextual Re-Ranking: Neural models re-rank retrieved documents to prioritize the most contextually\\nrelevant information.\\n• Iterative Retrieval: Advanced RAG introduces multi-hop retrieval mechanisms, enabling reasoning across\\nmultiple documents for complex queries.\\nThese advancements make Advanced RAG suitable for applications requiring high precision and nuanced understanding,\\nsuch as research synthesis and personalized recommendations. However, challenges such as computational overhead\\nand limited scalability persist, particularly when dealing with large datasets or multi-step queries.\\n2.3.3\\nModular RAG\\nModular RAG [20] represents the latest evolution in RAG paradigms, emphasizing flexibility and customization.',\n",
       " '2.3.3\\nModular RAG\\nModular RAG [20] represents the latest evolution in RAG paradigms, emphasizing flexibility and customization.\\nThese systems decompose the retrieval and generation pipeline into independent, reusable components, enabling\\ndomain-specific optimization and task adaptability. Figure 5 demonstrates the modular architecture, showcasing hybrid\\nretrieval strategies, composable pipelines, and external tool integration.\\nKey innovations in Modular RAG include:\\n• Hybrid Retrieval Strategies: Combining sparse retrieval methods (e.g., a sparse encoder-BM25) with dense\\nretrieval techniques [21] (e.g., DPR - Dense Passage Retrieval ) to maximize accuracy across diverse query\\ntypes.\\n• Tool Integration: Incorporating external APIs, databases, or computational tools to handle specialized tasks,\\nsuch as real-time data analysis or domain-specific computations.\\n• Composable Pipelines: Modular RAG enables retrievers, generators, and other components to be replaced,',\n",
       " 'such as real-time data analysis or domain-specific computations.\\n• Composable Pipelines: Modular RAG enables retrievers, generators, and other components to be replaced,\\nenhanced, or reconfigured independently, allowing high adaptability to specific use cases.\\nFor instance, a Modular RAG system designed for financial analytics might retrieve live stock prices via APIs, analyze\\nhistorical trends using dense retrieval, and generate actionable investment insights through a tailored language model.\\nThis modularity and customization make Modular RAG ideal for complex, multi-domain tasks, offering both scalability\\nand precision.\\n5',\n",
       " 'Figure 5: Overview of Modular RAG\\n2.3.4\\nGraph RAG\\nGraph RAG [16] extends traditional Retrieval-Augmented Generation systems by integrating graph-based data structures\\nas illustrated in Figure 6. These systems leverage the relationships and hierarchies within graph data to enhance multi-\\nhop reasoning and contextual enrichment. By incorporating graph-based retrieval, Graph RAG enables richer and more\\naccurate generative outputs, particularly for tasks requiring relational understanding.\\nGraph RAG is characterized by its ability to:\\n• Node Connectivity: Captures and reasons over relationships between entities.\\n• Hierarchical Knowledge Management: Handles structured and unstructured data through graph-based\\nhierarchies.\\n• Context Enrichment: Adds relational understanding by leveraging graph-based pathways.\\nHowever, Graph RAG has some limitations:\\n• Limited Scalability: The reliance on graph structures can restrict scalability, especially with extensive data\\nsources.',\n",
       " 'However, Graph RAG has some limitations:\\n• Limited Scalability: The reliance on graph structures can restrict scalability, especially with extensive data\\nsources.\\n• Data Dependency: High-quality graph data is essential for meaningful outputs, limiting its applicability in\\nunstructured or poorly annotated datasets.\\n• Complexity of Integration: Integrating graph data with unstructured retrieval systems increases design and\\nimplementation complexity.\\nGraph RAG is well-suited for applications such as healthcare diagnostics, legal research, and other domains where\\nreasoning over structured relationships is crucial.\\n2.3.5\\nAgentic RAG\\nAgentic RAG represents a paradigm shift by introducing autonomous agents capable of dynamic decision-making\\nand workflow optimization. Unlike static systems, Agentic RAG employs iterative refinement and adaptive retrieval\\nstrategies to address complex, real-time, and multi-domain queries. This paradigm leverages the modularity of retrieval',\n",
       " 'strategies to address complex, real-time, and multi-domain queries. This paradigm leverages the modularity of retrieval\\nand generation processes while introducing agent-based autonomy.\\n6',\n",
       " 'Figure 6: Overview of Graph RAG\\nKey characteristics of Agentic RAG include:\\n• Autonomous Decision-Making: Agents independently evaluate and manage retrieval strategies based on\\nquery complexity.\\n• Iterative Refinement: Incorporates feedback loops to improve retrieval accuracy and response relevance.\\n• Workflow Optimization: Dynamically orchestrates tasks, enabling efficiency in real-time applications.\\nDespite its advancements, Agentic RAG faces some challenges:\\n• Coordination Complexity: Managing interactions between agents requires sophisticated orchestration\\nmechanisms.\\n• Computational Overhead: The use of multiple agents increases resource requirements for complex work-\\nflows.\\n• Scalability Limitations: While scalable, the dynamic nature of the system can strain computational resources\\nfor high query volumes.\\nAgentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\\ndynamic adaptability and contextual precision are paramount.',\n",
       " 'for high query volumes.\\nAgentic RAG excels in domains like customer support, financial analytics, and adaptive learning platforms, where\\ndynamic adaptability and contextual precision are paramount.\\n2.4\\nChallenges and Limitations of Traditional RAG Systems\\nTraditional Retrieval-Augmented Generation (RAG) systems have significantly expanded the capabilities of Large\\nLanguage Models (LLMs) by integrating real-time data retrieval. However, these systems still face critical challenges\\nthat hinder their effectiveness in complex, real-world applications. The most notable limitations revolve around\\ncontextual integration, multi-step reasoning, and scalability and latency issues.\\n2.4.1\\nContextual Integration\\nEven when RAG systems successfully retrieve relevant information, they often struggle to seamlessly incorporate it\\ninto generated responses. The static nature of retrieval pipelines and limited contextual awareness lead to fragmented,\\ninconsistent, or overly generic outputs.',\n",
       " 'into generated responses. The static nature of retrieval pipelines and limited contextual awareness lead to fragmented,\\ninconsistent, or overly generic outputs.\\nExample: A query such as, \"What are the latest advancements in Alzheimer’s research and their implications for\\nearly-stage treatment?\" might yield relevant research papers and medical guidelines. However, traditional RAG\\nsystems often fail to synthesize these findings into a coherent explanation that connects the new treatments to specific\\npatient scenarios. Similarly, for a query like, \"What are the best sustainable practices for small-scale agriculture in\\narid regions?\", traditional systems might retrieve documents on general agricultural methods but overlook critical\\nsustainability practices tailored to arid environments.\\n7',\n",
       " 'Table 1: Comparative Analysis of RAG Paradigms\\nParadigm\\nKey Features\\nStrengths\\nNaïve RAG\\n• Keyword-based retrieval (e.g.,\\nTF-IDF, BM25)\\n• Simple and easy to implement\\n• Suitable for fact-based queries\\nAdvanced RAG\\n• Dense retrieval models (e.g.,\\nDPR)\\n• Neural ranking and re-ranking\\n• Multi-hop retrieval\\n• High precision retrieval\\n• Improved contextual relevance\\nModular RAG\\n• Hybrid retrieval (sparse and\\ndense)\\n• Tool and API integration\\n• Composable, domain-specific\\npipelines\\n• High flexibility and customization\\n• Suitable for diverse applications\\n• Scalable\\nGraph RAG\\n• Integration\\nof\\ngraph-based\\nstructures\\n• Multi-hop reasoning\\n• Contextual\\nenrichment\\nvia\\nnodes\\n• Relational reasoning capabilities\\n• Mitigates hallucinations\\n• Ideal for structured data tasks\\nAgentic RAG\\n• Autonomous agents\\n• Dynamic decision-making\\n• Iterative refinement and work-\\nflow optimization\\n• Adaptable to real-time changes\\n• Scalable for multi-domain tasks\\n• High accuracy\\n2.4.2\\nMulti-Step Reasoning',\n",
       " '• Dynamic decision-making\\n• Iterative refinement and work-\\nflow optimization\\n• Adaptable to real-time changes\\n• Scalable for multi-domain tasks\\n• High accuracy\\n2.4.2\\nMulti-Step Reasoning\\nMany real-world queries require iterative or multi-hop reasoning—retrieving and synthesizing information across\\nmultiple steps. Traditional RAG systems are often ill-equipped to refine retrieval based on intermediate insights or user\\nfeedback, resulting in incomplete or disjointed responses.\\nExample: A complex query like, \"What lessons from renewable energy policies in Europe can be applied to developing\\nnations, and what are the potential economic impacts?\" demands the orchestration of multiple types of information,\\nincluding policy data, contextualization for developing regions, and economic analysis. Traditional RAG systems\\ntypically fail to connect these disparate elements into a cohesive response.\\n2.4.3\\nScalability and Latency Issues',\n",
       " 'typically fail to connect these disparate elements into a cohesive response.\\n2.4.3\\nScalability and Latency Issues\\nAs the volume of external data sources grows, querying and ranking large datasets becomes increasingly computationally\\nintensive. This results in significant latency, which undermines the system’s ability to provide timely responses in\\nreal-time applications.\\nExample: In time-sensitive settings such as financial analytics or live customer support, delays caused by querying\\nmultiple databases or processing large document sets can hinder the system’s overall utility. For example, a delay in\\nretrieving market trends during high-frequency trading could result in missed opportunities.\\n2.5\\nAgentic RAG: A Paradigm Shift\\nTraditional RAG systems, with their static workflows and limited adaptability, often struggle to handle dynamic, multi-\\nstep reasoning and complex real-world tasks. These limitations have spurred the integration of agentic intelligence,\\n8',\n",
       " 'resulting in Agentic RAG. By incorporating autonomous agents capable of dynamic decision-making, iterative reasoning,\\nand adaptive retrieval strategies, Agentic RAG builds on the modularity of earlier paradigms while overcoming their\\ninherent constraints. This evolution enables more complex, multi-domain tasks to be addressed with enhanced precision\\nand contextual understanding, positioning Agentic RAG as a cornerstone for next-generation AI applications. In\\nparticular, Agentic RAG systems reduce latency through optimized workflows and refine outputs iteratively, tackling\\nthe very challenges that have historically hindered traditional RAG’s scalability and effectiveness.\\n3\\nCore Principles and Background of Agentic Intelligence\\nAgentic Intelligence forms the foundation of Agentic Retrieval-Augmented Generation (RAG) systems, enabling them\\nto transcend the static and reactive nature of traditional RAG. By integrating autonomous agents capable of dynamic',\n",
       " 'to transcend the static and reactive nature of traditional RAG. By integrating autonomous agents capable of dynamic\\ndecision-making, iterative reasoning, and collaborative workflows, Agentic RAG systems exhibit enhanced adaptability\\nand precision. This section explores the core principles underpinning agentic intelligence.\\nComponents of an AI Agent.\\nIn essence, an AI agent comprises (Figure. 7):\\n• LLM (with defined Role and Task): Serves as the agent’s primary reasoning engine and dialogue interface.\\nIt interprets user queries, generates responses, and maintains coherence.\\n• Memory (Short-Term and Long-Term): Captures context and relevant data across interactions. Short-term\\nmemory [22] tracks immediate conversation state, while long-term memory [22]stores accumulated knowledge\\nand agent experiences.\\n• Planning (Reflection & Self-Critique): Guides the agent’s iterative reasoning process through reflection,',\n",
       " 'and agent experiences.\\n• Planning (Reflection & Self-Critique): Guides the agent’s iterative reasoning process through reflection,\\nquery routing, or self-critique[23], ensuring that complex tasks are broken down effectively [24].\\n• Tools Vector Search, Web Search, APIs, etc.): Expands the agent’s capabilities beyond text generation,\\nenabling access to external resources, real-time data, or specialized computations.\\nFigure 7: An Overview of AI Agents\\nAgentic Patterns [25, 26] provide structured methodologies that guide the behavior of agents in Agentic Retrieval-\\nAugmented Generation (RAG) systems. These patterns enable agents to dynamically adapt, plan, and collaborate,\\nensuring that the system can handle complex, real-world tasks with precision and scalability. Four key patterns underpin\\nagentic workflows:\\n3.1\\nReflection\\nReflection is a foundational design pattern in agentic workflows, enabling agents to iteratively evaluate and refine their',\n",
       " 'agentic workflows:\\n3.1\\nReflection\\nReflection is a foundational design pattern in agentic workflows, enabling agents to iteratively evaluate and refine their\\noutputs. By incorporating self-feedback mechanisms, agents can identify and address errors, inconsistencies, and areas\\nfor improvement, enhancing performance across tasks like code generation, text production, and question answering (\\n9',\n",
       " 'as shown in Figure 8). In practical use, Reflection involves prompting an agent to critique its outputs for correctness,\\nstyle, and efficiency, then incorporating this feedback into subsequent iterations. External tools, such as unit tests or\\nweb searches, can further enhance this process by validating results and highlighting gaps.\\nIn multi-agent systems, Reflection can involve distinct roles, such as one agent generating outputs while another\\ncritiques them, fostering collaborative improvement. For instance, in legal research, agents can iteratively refine\\nresponses by re-evaluating retrieved case law, ensuring accuracy and comprehensiveness. Reflection has demonstrated\\nsignificant performance improvements in studies like Self-Refine [27], Reflexion [28], and CRITIC [23].\\nFigure 8: An Overview of Agentic Self- Reflection\\n3.2\\nPlanning\\nPlanning [24] is a key design pattern in agentic workflows that enables agents to autonomously decompose complex tasks',\n",
       " 'Figure 8: An Overview of Agentic Self- Reflection\\n3.2\\nPlanning\\nPlanning [24] is a key design pattern in agentic workflows that enables agents to autonomously decompose complex tasks\\ninto smaller, manageable subtasks. This capability is essential for multi-hop reasoning and iterative problem-solving in\\ndynamic and uncertain scenarios as shown in Figure 9a.\\nBy leveraging planning, agents can dynamically determine the sequence of steps needed to accomplish a larger objective.\\nThis adaptability allows agents to handle tasks that cannot be predefined, ensuring flexibility in decision-making.\\nWhile powerful, Planning can produce less predictable outcomes compared to deterministic workflows like Reflection.\\nPlanning is particularly suited for tasks that require dynamic adaptation, where predefined workflows are insufficient.\\nAs the technology matures, its potential to drive innovative applications across domains will continue to grow.\\n3.3\\nTool Use',\n",
       " 'As the technology matures, its potential to drive innovative applications across domains will continue to grow.\\n3.3\\nTool Use\\nTool Use enables agents to extend their capabilities by interacting with external tools, APIs, or computational resources\\nas illustrated in 9b. This pattern allows agents to gather information, perform computations, and manipulate data beyond\\ntheir pre-trained knowledge. By dynamically integrating tools into workflows, agents can adapt to complex tasks and\\nprovide more accurate and contextually relevant outputs.\\nModern agentic workflows incorporate tool use for a variety of applications, including information retrieval, computa-\\ntional reasoning, and interfacing with external systems. The implementation of this pattern has evolved significantly\\nwith advancements like GPT-4’s function calling capabilities and systems capable of managing access to numerous',\n",
       " 'with advancements like GPT-4’s function calling capabilities and systems capable of managing access to numerous\\ntools. These developments facilitate sophisticated workflows where agents autonomously select and execute the most\\nrelevant tools for a given task.\\nWhile tool use significantly enhances agentic workflows, challenges remain in optimizing the selection of tools,\\nparticularly in contexts with a large number of available options. Techniques inspired by retrieval-augmented generation\\n(RAG), such as heuristic-based selection, have been proposed to address this issue.\\n3.4\\nMulti-Agent\\nMulti-agent collaboration [29] is a key design pattern in agentic workflows that enables task specialization and parallel\\nprocessing. Agents communicate and share intermediate results, ensuring the overall workflow remains efficient and\\ncoherent. By distributing subtasks among specialized agents, this pattern improves the scalability and adaptability\\n10',\n",
       " '(a) An Overview of Agentic Planning\\n(b) An Overview of Tool Use\\nFigure 9: Overview of Agentic Planning and Tool Use\\nof complex workflows. Multi-agent systems allow developers to decompose intricate tasks into smaller, manageable\\nsubtasks assigned to different agents. This approach not only enhances task performance but also provides a robust\\nframework for managing complex interactions. Each agent operates with its own memory and workflow, which can\\ninclude the use of tools, reflection, or planning, enabling dynamic and collaborative problem-solving (see Figure 10).\\nWhile multi-agent collaboration offers significant potential, it is a less predictable design pattern compared to more\\nmature workflows like Reflection and Tool Use. Nevertheless, emerging frameworks such as AutoGen, Crew AI, and\\nLangGraph are providing new avenues for implementing effective multi-agent solutions.\\nFigure 10: An Overview of MultiAgent',\n",
       " 'LangGraph are providing new avenues for implementing effective multi-agent solutions.\\nFigure 10: An Overview of MultiAgent\\nThese design patterns form the foundation for the success of Agentic RAG systems. By structuring workflows—from\\nsimple, sequential steps to more adaptive, collaborative processes—these patterns enable systems to dynamically\\nadapt their retrieval and generative strategies to the diverse and ever-changing demands of real-world environments.\\nLeveraging these patterns, agents are capable of handling iterative, context-aware tasks that significantly exceed the\\ncapabilities of traditional RAG systems.\\n4\\nAgentic Workflow Patterns: Adaptive Strategies for Dynamic Collaboration\\nAgentic workflow patterns, [12, 13] structure LLM-based applications to optimize performance, accuracy, and efficiency.\\nDifferent approaches are suitable depending on task complexity and processing requirements.\\n11',\n",
       " '4.1\\nPrompt Chaining: Enhancing Accuracy Through Sequential Processing\\nPrompt chaining [12, 13] decomposes a complex task into multiple steps, where each step builds upon the previous\\none. This structured approach improves accuracy by simplifying each subtask before moving forward. However, it may\\nincrease latency due to sequential processing.\\nFigure 11: Illustration of Prompt Chaining Workflow\\nWhen to Use: This workflow is most effective when a task can be broken down into fixed subtasks, each contributing\\nto the final output. It is particularly useful in scenarios where step-by-step reasoning enhances accuracy.\\nExample Applications:\\n• Generating marketing content in one language and then translating it into another while preserving nuances.\\n• Structuring document creation by first generating an outline, verifying its completeness, and then developing\\nthe full text.\\n4.2\\nRouting:Directing Inputs to Specialized Processes',\n",
       " '• Structuring document creation by first generating an outline, verifying its completeness, and then developing\\nthe full text.\\n4.2\\nRouting:Directing Inputs to Specialized Processes\\nRouting [12, 13] involves classifying an input and directing it to an appropriate specialized prompt or process. This\\nmethod ensures distinct queries or tasks are handled separately, improving efficiency and response quality.\\nFigure 12: Illustration Routing Workflow\\nWhen to Use: Ideal for scenarios where different types of input require distinct handling strategies, ensuring optimized\\nperformance for each category.\\nExample Applications:\\n• Directing customer service queries into categories such as technical support, refund requests, or general\\ninquiries.\\n12',\n",
       " '• Assigning simple queries to smaller models for cost efficiency, while complex requests go to advanced models.\\n4.3\\nParallelization: Speeding Up Processing Through Concurrent Execution\\nParallelization [12, 13] divides a task into independent processes that run simultaneously, reducing latency and\\nimproving throughput. It can be categorized into sectioning (independent subtasks) and voting (multiple outputs for\\naccuracy).\\nFigure 13: Illustration of Parallelization Workflow\\nWhen to Use: Useful when tasks can be executed independently to enhance speed or when multiple outputs improve\\nconfidence.\\nExample Applications:\\n• Sectioning: Splitting tasks like content moderation, where one model screens input while another generates a\\nresponse.\\n• Voting: Using multiple models to cross-check code for vulnerabilities or analyze content moderation decisions.\\n4.4\\nOrchestrator-Workers: Dynamic Task Delegation',\n",
       " 'response.\\n• Voting: Using multiple models to cross-check code for vulnerabilities or analyze content moderation decisions.\\n4.4\\nOrchestrator-Workers: Dynamic Task Delegation\\nThis workflow [12, 13] features a central orchestrator model that dynamically breaks tasks into subtasks, assigns them\\nto specialized worker models, and compiles the results. Unlike parallelization, it adapts to varying input complexity.\\nFigure 14: Illustration of Orchestrator-Workers Workflow\\nWhen to Use: Best suited for tasks requiring dynamic decomposition and real-time adaptation, where subtasks are not\\npredefined.\\n13',\n",
       " 'Example Applications:\\n• Automatically modifying multiple files in a codebase based on the nature of requested changes.\\n• Conducting real-time research by gathering and synthesizing relevant information from multiple sources.\\n4.5\\nEvaluator-Optimizer: Refining Output Through Iteration\\nThe evaluator-optimizer [12, 13] workflow iteratively improves content by generating an initial output and refining it\\nbased on feedback from an evaluation model.\\nFigure 15: Illustration of Evaluator-Optimizer Workflow\\nWhen to Use: Effective when iterative refinement significantly enhances response quality, especially when clear\\nevaluation criteria exist.\\nExample Applications:\\n• Improving literary translations through multiple evaluation and refinement cycles.\\n• Conducting multi-round research queries where additional iterations refine search results.\\n5\\nTaxonomy of Agentic RAG Systems\\nAgentic Retrieval-Augmented Generation (RAG) systems can be categorized into distinct architectural frameworks',\n",
       " '5\\nTaxonomy of Agentic RAG Systems\\nAgentic Retrieval-Augmented Generation (RAG) systems can be categorized into distinct architectural frameworks\\nbased on their complexity and design principles. These include single-agent architectures, multi-agent systems, and hi-\\nerarchical agentic architectures. Each framework is tailored to address specific challenges and optimize performance for\\ndiverse applications. This section provides a detailed taxonomy of these architectures, highlighting their characteristics,\\nstrengths, and limitations.\\n5.1\\nSingle-Agent Agentic RAG: Router\\nA Single-Agent Agentic RAG: [30] serves as a centralized decision-making system where a single agent manages the\\nretrieval, routing, and integration of information (as shown in Figure. 16). This architecture simplifies the system by\\nconsolidating these tasks into one unified agent, making it particularly effective for setups with a limited number of\\ntools or data sources.\\nWorkflow',\n",
       " 'consolidating these tasks into one unified agent, making it particularly effective for setups with a limited number of\\ntools or data sources.\\nWorkflow\\n1. Query Submission and Evaluation: The process begins when a user submits a query. A coordinating\\nagent (or master retrieval agent) receives the query and analyzes it to determine the most suitable sources of\\ninformation.\\n2. Knowledge Source Selection: Based on the query’s type, the coordinating agent chooses from a variety of\\nretrieval options:\\n14',\n",
       " '• Structured Databases: For queries requiring tabular data access, the system may use a Text-to-SQL\\nengine that interacts with databases like PostgreSQL or MySQL.\\n• Semantic Search: When dealing with unstructured information, it retrieves relevant documents (e.g.,\\nPDFs, books, organizational records) using vector-based retrieval.\\n• Web Search: For real-time or broad contextual information, the system leverages a web search tool to\\naccess the latest online data.\\n• Recommendation Systems: For personalized or contextual queries, the system taps into recommendation\\nengines that provide tailored suggestions.\\n3. Data Integration and LLM Synthesis: Once the relevant data is retrieved from the chosen sources, it is\\npassed to a Large Language Model (LLM). The LLM synthesizes the gathered information, integrating insights\\nfrom multiple sources into a coherent and contextually relevant response.',\n",
       " 'passed to a Large Language Model (LLM). The LLM synthesizes the gathered information, integrating insights\\nfrom multiple sources into a coherent and contextually relevant response.\\n4. Output Generation: Finally, the system delivers a comprehensive, user-facing answer that addresses the\\noriginal query. This response is presented in an actionable, concise format and may optionally include\\nreferences or citations to the sources used.\\nFigure 16: An Overview of Single Agentic RAG\\nKey Features and Advantages.\\n• Centralized Simplicity: A single agent handles all retrieval and routing tasks, making the architecture\\nstraightforward to design, implement, and maintain.\\n• Efficiency & Resource Optimization: With fewer agents and simpler coordination, the system demands\\nfewer computational resources and can handle queries more quickly.\\n• Dynamic Routing: The agent evaluates each query in real-time, selecting the most appropriate knowledge\\nsource (e.g., structured DB, semantic search, web search).',\n",
       " '• Dynamic Routing: The agent evaluates each query in real-time, selecting the most appropriate knowledge\\nsource (e.g., structured DB, semantic search, web search).\\n• Versatility Across Tools: Supports a variety of data sources and external APIs, enabling both structured and\\nunstructured workflows.\\n• Ideal for Simpler Systems: Suited for applications with well-defined tasks or limited integration requirements\\n(e.g., document retrieval, SQL-based workflows).\\n15',\n",
       " 'Use Case: Customer Support\\nPrompt: Can you tell me the delivery status of my order?\\nSystem Process (Single-Agent Workflow):\\n1. Query Submission and Evaluation:\\n• The user submits the query, which is received by the coordinating agent.\\n• The coordinating agent analyzes the query and determines the most appropriate sources of\\ninformation.\\n2. Knowledge Source Selection:\\n• Retrieves tracking details from the order management database.\\n• Fetches real-time updates from the shipping provider’s API.\\n• Optionally conducts a web search to identify local conditions affecting delivery, such as weather\\nor logistical delays.\\n3. Data Integration and LLM Synthesis:\\n• The relevant data is passed to the LLM, which synthesizes the information into a coherent\\nresponse.\\n4. Output Generation:\\n• The system generates an actionable and concise response, providing live tracking updates and\\npotential alternatives.\\nResponse:',\n",
       " 'response.\\n4. Output Generation:\\n• The system generates an actionable and concise response, providing live tracking updates and\\npotential alternatives.\\nResponse:\\nIntegrated Response: “Your package is currently in transit and expected to arrive tomorrow evening. The live\\ntracking from UPS indicates it is at the regional distribution center.”\\n5.2\\nMulti-Agent Agentic RAG Systems:\\nMulti-Agent RAG [30] represents a modular and scalable evolution of single-agent architectures, designed to handle\\ncomplex workflows and diverse query types by leveraging multiple specialized agents (as shown in Figure 17). Instead\\nof relying on a single agent to manage all tasks—reasoning, retrieval, and response generation—this system distributes\\nresponsibilities across multiple agents, each optimized for a specific role or data source.\\nWorkflow\\n1. Query Submission: The process begins with a user query, which is received by a coordinator agent or master',\n",
       " 'Workflow\\n1. Query Submission: The process begins with a user query, which is received by a coordinator agent or master\\nretrieval agent. This agent acts as the central orchestrator, delegating the query to specialized retrieval agents\\nbased on the query’s requirements.\\n2. Specialized Retrieval Agents: The query is distributed among multiple retrieval agents, each focusing on a\\nspecific type of data source or task. Examples include:\\n• Agent 1: Handles structured queries, such as interacting with SQL-based databases like PostgreSQL or\\nMySQL.\\n• Agent 2: Manages semantic searches for retrieving unstructured data from sources like PDFs, books, or\\ninternal records.\\n• Agent 3: Focuses on retrieving real-time public information from web searches or APIs.\\n• Agent 4: Specializes in recommendation systems, delivering context-aware suggestions based on user\\nbehavior or profiles.\\n3. Tool Access and Data Retrieval: Each agent routes the query to the appropriate tools or data sources within',\n",
       " 'behavior or profiles.\\n3. Tool Access and Data Retrieval: Each agent routes the query to the appropriate tools or data sources within\\nits domain, such as:\\n• Vector Search: For semantic relevance.\\n• Text-to-SQL: For structured data.\\n• Web Search: For real-time public information.\\n• APIs: For accessing external services or proprietary systems.\\nThe retrieval process is executed in parallel, allowing for efficient processing of diverse query types.\\n16',\n",
       " 'Figure 17: An Overview of Multi-Agent Agentic RAG Systems\\n4. Data Integration and LLM Synthesis: Once retrieval is complete, the data from all agents is passed to a\\nLarge Language Model (LLM). The LLM synthesizes the retrieved information into a coherent and contextually\\nrelevant response, integrating insights from multiple sources seamlessly.\\n5. Output Generation: The system generates a comprehensive response, which is delivered back to the user in\\nan actionable and concise format.\\nKey Features and Advantages.\\n• Modularity: Each agent operates independently, allowing for seamless addition or removal of agents based on\\nsystem requirements.\\n• Scalability: Parallel processing by multiple agents enables the system to handle high query volumes efficiently.\\n• Task Specialization: Each agent is optimized for a specific type of query or data source, improving accuracy\\nand retrieval relevance.',\n",
       " '• Task Specialization: Each agent is optimized for a specific type of query or data source, improving accuracy\\nand retrieval relevance.\\n• Efficiency: By distributing tasks across specialized agents, the system minimizes bottlenecks and enhances\\nperformance for complex workflows.\\n• Versatility: Suitable for applications spanning multiple domains, including research, analytics, decision-\\nmaking, and customer support.\\nChallenges\\n• Coordination Complexity: Managing inter-agent communication and task delegation requires sophisticated\\norchestration mechanisms.\\n• Computational Overhead: Parallel processing of multiple agents can increase resource usage.\\n• Data Integration: Synthesizing outputs from diverse sources into a cohesive response is non-trivial and\\nrequires advanced LLM capabilities.\\n17',\n",
       " 'Use Case: Multi-Domain Research Assistant\\nPrompt: What are the economic and environmental impacts of renewable energy adoption in Europe?\\nSystem Process (Multi-Agent Workflow):\\n• Agent 1: Retrieves statistical data from economic databases using SQL-based queries.\\n• Agent 2: Searches for relevant academic papers using semantic search tools.\\n• Agent 3: Performs a web search for recent news and policy updates on renewable energy.\\n• Agent 4: Consults a recommendation system to suggest related content, such as reports or expert\\ncommentary.\\nResponse:\\nIntegrated Response: “Adopting renewable energy in Europe has led to a 20% reduction in greenhouse gas\\nemissions over the past decade, according to EU policy reports. Economically, renewable energy investments\\nhave generated approximately 1.2 million jobs, with significant growth in solar and wind sectors. Recent\\nacademic studies also highlight potential trade-offs in grid stability and energy storage costs.”\\n5.3\\nHierarchical Agentic RAG Systems',\n",
       " 'academic studies also highlight potential trade-offs in grid stability and energy storage costs.”\\n5.3\\nHierarchical Agentic RAG Systems\\nHierarchical Agentic RAG: [14] systems employ a structured, multi-tiered approach to information retrieval and\\nprocessing, enhancing both efficiency and strategic decision-making as shown in Figure 18. Agents are organized in\\na hierarchy, with higher-level agents overseeing and directing lower-level agents. This structure enables multi-level\\ndecision-making, ensuring that queries are handled by the most appropriate resources.\\nFigure 18: An illustration of Hierarchical Agentic RAG\\nWorkflow\\n1. Query Reception: A user submits a query, received by a top-tier agent responsible for initial assessment and\\ndelegation.\\n2. Strategic Decision-Making: The top-tier agent evaluates the query’s complexity and decides which subor-\\ndinate agents or data sources to prioritize. Certain databases, APIs, or retrieval tools may be deemed more',\n",
       " 'dinate agents or data sources to prioritize. Certain databases, APIs, or retrieval tools may be deemed more\\nreliable or relevant based on the query’s domain.\\n3. Delegation to Subordinate Agents: The top-tier agent assigns tasks to lower-level agents specialized in\\nparticular retrieval methods (e.g., SQL databases, web search, or proprietary systems). These agents execute\\ntheir assigned tasks independently.\\n18',\n",
       " '4. Aggregation and Synthesis: The results from subordinate agents are collected and integrated by the higher-\\nlevel agent, which synthesizes the information into a coherent response.\\n5. Response Delivery: The final, synthesized answer is returned to the user, ensuring that the response is both\\ncomprehensive and contextually relevant.\\nKey Features and Advantages.\\n• Strategic Prioritization: Top-tier agents can prioritize data sources or tasks based on query complexity,\\nreliability, or context.\\n• Scalability: Distributing tasks across multiple agent tiers enables handling of highly complex or multi-faceted\\nqueries.\\n• Enhanced Decision-Making: Higher-level agents apply strategic oversight to improve overall accuracy and\\ncoherence of responses.\\nChallenges\\n• Coordination Complexity: Maintaining robust inter-agent communication across multiple levels can increase\\norchestration overhead.\\n• Resource Allocation: Efficiently distributing tasks among tiers to avoid bottlenecks is non-trivial.',\n",
       " 'orchestration overhead.\\n• Resource Allocation: Efficiently distributing tasks among tiers to avoid bottlenecks is non-trivial.\\nUse Case: Financial Analysis System\\nPrompt: What are the best investment options given the current market trends in renewable energy?\\nSystem Process (Hierarchical Agentic Workflow):\\n1. Top-Tier Agent: Assesses the query’s complexity and prioritizes reliable financial databases and\\neconomic indicators over less validated data sources.\\n2. Mid-Level Agent: Retrieves real-time market data (e.g., stock prices, sector performance) from\\nproprietary APIs and structured SQL databases.\\n3. Lower-Level Agent(s): Conducts web searches for recent policy announcements and consults recom-\\nmendation systems that track expert opinions and news analytics.\\n4. Aggregation and Synthesis: The top-tier agent compiles the results, integrating quantitative data with\\npolicy insights.\\nResponse:',\n",
       " 'mendation systems that track expert opinions and news analytics.\\n4. Aggregation and Synthesis: The top-tier agent compiles the results, integrating quantitative data with\\npolicy insights.\\nResponse:\\nIntegrated Response: “Based on current market data, renewable energy stocks have shown a 15% growth over\\nthe past quarter, driven by supportive government policies and heightened investor interest. Analysts suggest\\nthat wind and solar sectors, in particular, may experience continued momentum, while emerging technologies\\nlike green hydrogen present moderate risk but potentially high returns.”\\n5.4\\nAgentic Corrective RAG\\nCorrective RAG : introduces mechanisms to self-correct retrieval results, enhancing document utilization and improving\\nresponse generation quality as demonstrated in Figure 19. By embedding intelligent agents into the workflow, Corrective\\nRAG [31] [32] ensures iterative refinement of context documents and responses, minimizing errors and maximizing\\nrelevance.',\n",
       " 'RAG [31] [32] ensures iterative refinement of context documents and responses, minimizing errors and maximizing\\nrelevance.\\nKey Idea of Corrective RAG:\\nThe core principle of Corrective RAG lies in its ability to evaluate retrieved documents\\ndynamically, perform corrective actions, and refine queries to enhance the quality of generated responses. Corrective\\nRAG adjusts its approach as follows:\\n• Document Relevance Evaluation: Retrieved documents are assessed for relevance by the Relevance Evalua-\\ntion Agent. Documents below the relevance threshold trigger corrective steps.\\n• Query Refinement and Augmentation: Queries are refined by the Query Refinement Agent, which leverages\\nsemantic understanding to optimize retrieval for better results.\\n19',\n",
       " 'Figure 19: Overview of Agentic Corrective RAG\\n• Dynamic Retrieval from External Sources: When context is insufficient, the External Knowledge Retrieval\\nAgent performs web searches or accesses alternative data sources to supplement the retrieved documents.\\n• Response Synthesis: All validated and refined information is passed to the Response Synthesis Agent for final\\nresponse generation.\\nWorkflow:\\nThe Corrective RAG system is built on five key agents:\\n1. Context Retrieval Agent: Responsible for retrieving initial context documents from a vector database.\\n2. Relevance Evaluation Agent: Assesses the retrieved documents for relevance and flags any irrelevant or\\nambiguous documents for corrective actions.\\n3. Query Refinement Agent: Rewrites queries to improve retrieval, leveraging semantic understanding to\\noptimize results.\\n4. External Knowledge Retrieval Agent: Performs web searches or accesses alternative data sources when the\\ncontext documents are insufficient.',\n",
       " 'optimize results.\\n4. External Knowledge Retrieval Agent: Performs web searches or accesses alternative data sources when the\\ncontext documents are insufficient.\\n5. Response Synthesis Agent: Synthesizes all validated information into a coherent and accurate response.\\nKey Features and Advantages:\\n• Iterative Correction: Ensures high response accuracy by dynamically identifying and correcting irrelevant or\\nambiguous retrieval results.\\n• Dynamic Adaptability: Incorporates real-time web searches and query refinement for enhanced retrieval\\nprecision.\\n• Agentic Modularity: Each agent performs specialized tasks, ensuring efficient and scalable operation.\\n• Factuality Assurance: By validating all retrieved and generated content, Corrective RAG minimizes the risk\\nof hallucination or misinformation.\\n20',\n",
       " 'Use Case: Academic Research Assistant\\nPrompt: What are the latest findings in generative AI research?\\nSystem Process (Corrective RAG Workflow):\\n1. Query Submission: A user submits the query to the system.\\n2. Context Retrieval:\\n• The Context Retrieval Agent retrieves initial documents from a database of published papers on\\ngenerative AI.\\n• The retrieved documents are passed to the next step for evaluation.\\n3. Relevance Evaluation:\\n• The Relevance Evaluation Agent assesses the documents for alignment with the query.\\n• Documents are classified into relevant, ambiguous, or irrelevant categories. Irrelevant documents\\nare flagged for corrective actions.\\n4. Corrective Actions (if needed):\\n• The Query Refinement Agent rewrites the query to improve specificity and relevance.\\n• The External Knowledge Retrieval Agent performs web searches to fetch additional papers and\\nreports from external sources.\\n5. Response Synthesis:',\n",
       " '• The External Knowledge Retrieval Agent performs web searches to fetch additional papers and\\nreports from external sources.\\n5. Response Synthesis:\\n• The Response Synthesis Agent integrates validated documents into a coherent and comprehensive\\nsummary.\\nResponse:\\nIntegrated Response: “Recent findings in generative AI highlight advancements in diffusion models, reinforce-\\nment learning for text-to-video tasks, and optimization techniques for large-scale model training. For more\\ndetails, refer to studies published in NeurIPS 2024 and AAAI 2025.”\\n5.5\\nAdaptive Agentic RAG\\nAdaptive Retrieval-Augmented Generation (Adaptive RAG) [33] enhances the flexibility and efficiency of large\\nlanguage models (LLMs) by dynamically adjusting query handling strategies based on the complexity of the incoming\\nquery. Unlike static retrieval workflows, Adaptive RAG [34] employs a classifier to assess query complexity and',\n",
       " 'query. Unlike static retrieval workflows, Adaptive RAG [34] employs a classifier to assess query complexity and\\ndetermine the most appropriate approach, ranging from single-step retrieval to multi-step reasoning, or even bypassing\\nretrieval altogether for straightforward queries as illustrated in Figure 20.\\nFigure 20: An Overview of Adaptive Agentic RAG\\nKey Idea of Adaptive RAG\\nThe core principle of Adaptive RAG lies in its ability to dynamically tailor retrieval\\nstrategies based on the complexity of the query. Adaptive RAG adjusts its approach as follows:\\n21',\n",
       " '• Straightforward Queries: For fact-based questions that require no additional retrieval (e.g., \"What is the\\nboiling point of water?\"), the system directly generates an answer using pre-existing knowledge.\\n• Simple Queries: For moderately complex tasks requiring minimal context (e.g., \"What is the status of my\\nlatest electricity bill?\"), the system performs a single-step retrieval to fetch the relevant details.\\n• Complex Queries: For multi-layered queries requiring iterative reasoning (e.g., \"How has the population of\\nCity X changed over the past decade, and what are the contributing factors?\"), the system employs multi-step\\nretrieval, progressively refining intermediate results to provide a comprehensive answer.\\nWorkflow:\\nThe Adaptive RAG system is built on three primary components:\\n1. Classifier Role:\\n• A smaller language model analyzes the query to predict its complexity.\\n• The classifier is trained using automatically labeled datasets, derived from past model outcomes and',\n",
       " '1. Classifier Role:\\n• A smaller language model analyzes the query to predict its complexity.\\n• The classifier is trained using automatically labeled datasets, derived from past model outcomes and\\nquery patterns.\\n2. Dynamic Strategy Selection:\\n• For straightforward queries, the system avoids unnecessary retrieval, directly leveraging the LLM for\\nresponse generation.\\n• For simple queries, it employs a single-step retrieval process to fetch relevant context.\\n• For complex queries, it activates multi-step retrieval to ensure iterative refinement and enhanced reasoning.\\n3. LLM Integration:\\n• The LLM synthesizes retrieved information into a coherent response.\\n• Iterative interactions between the LLM and the classifier enable refinement for complex queries.\\nKey Features and Advantages\\n• Dynamic Adaptability: Adjusts retrieval strategies based on query complexity, optimizing both computational\\nefficiency and response accuracy.',\n",
       " 'Key Features and Advantages\\n• Dynamic Adaptability: Adjusts retrieval strategies based on query complexity, optimizing both computational\\nefficiency and response accuracy.\\n• Resource Efficiency: Minimizes unnecessary overhead for simple queries while ensuring thorough processing\\nfor complex ones.\\n• Enhanced Accuracy: Iterative refinement ensures that complex queries are resolved with high precision.\\n• Flexibility: Can be extended to incorporate additional pathways, such as domain-specific tools or external\\nAPIs.\\n22',\n",
       " 'Use Case: Customer Support Assistant\\nPrompt: Why is my package delayed, and what alternatives do I have?\\nSystem Process (Adaptive RAG Workflow):\\n1. Query Classification:\\n• The classifier analyzes the query and determines it to be complex, requiring multi-step reasoning.\\n2. Dynamic Strategy Selection:\\n• The system activates a multi-step retrieval process based on the complexity classification.\\n3. Multi-Step Retrieval:\\n• Retrieves tracking details from the order database.\\n• Fetches real-time status updates from the shipping provider API.\\n• Conducts a web search for external factors such as weather conditions or local disruptions.\\n4. Response Synthesis:\\n• The LLM integrates all retrieved information, synthesizing a comprehensive and actionable\\nresponse.\\nResponse:\\nIntegrated Response: “Your package is delayed due to severe weather conditions in your region. It is currently\\nat the local distribution center and will be delivered in 2 days. Alternatively, you may opt for a local pickup',\n",
       " 'at the local distribution center and will be delivered in 2 days. Alternatively, you may opt for a local pickup\\nfrom the facility.”\\n5.6\\nGraph-Based Agentic RAG\\n5.6.1\\nAgent-G: Agentic Framework for Graph RAG\\nAgent-G [8]: introduces a novel agentic architecture that integrates graph knowledge bases with unstructured document\\nretrieval. By combining structured and unstructured data sources, this framework enhances retrieval-augmented\\ngeneration (RAG) systems with improved reasoning and retrieval accuracy. It employs modular retriever banks,\\ndynamic agent interaction, and feedback loops to ensure high-quality outputs as shown in Figure 21.\\nFigure 21: An Overview of Agent-G: Agentic Framework for Graph RAG [8]\\n23',\n",
       " 'Key Idea of Agent-G\\nThe core principle of Agent-G lies in its ability to dynamically assign retrieval tasks to\\nspecialized agents, leveraging both graph knowledge bases and textual documents. Agent-G adjusts its retrieval strategy\\nas follows:\\n• Graph Knowledge Bases: Structured data is used to extract relationships, hierarchies, and connections (e.g.,\\ndisease-to-symptom mappings in healthcare).\\n• Unstructured Documents: Traditional text retrieval systems provide contextual information to complement\\ngraph data.\\n• Critic Module: Evaluates the relevance and quality of retrieved information, ensuring alignment with the\\nquery.\\n• Feedback Loops: Refines retrieval and synthesis through iterative validation and re-querying.\\nWorkflow:\\nThe Agent-G system is built on four primary components:\\n1. Retriever Bank:\\n• A modular set of agents specializes in retrieving graph-based or unstructured data.\\n• Agents dynamically select relevant sources based on the query’s requirements.\\n2. Critic Module:',\n",
       " '• A modular set of agents specializes in retrieving graph-based or unstructured data.\\n• Agents dynamically select relevant sources based on the query’s requirements.\\n2. Critic Module:\\n• Validates retrieved data for relevance and quality.\\n• Flags low-confidence results for re-retrieval or refinement.\\n3. Dynamic Agent Interaction:\\n• Task-specific agents collaborate to integrate diverse data types.\\n• Ensures cohesive retrieval and synthesis across graph and text sources.\\n4. LLM Integration:\\n• Synthesizes validated data into a coherent response.\\n• Iterative feedback from the critic ensures alignment with the query’s intent.\\nKey Features and Advantages\\n• Enhanced Reasoning: Combines structured relationships from graphs with contextual information from\\nunstructured documents.\\n• Dynamic Adaptability: Adjusts retrieval strategies dynamically based on query requirements.\\n• Improved Accuracy: Critic module reduces the risk of irrelevant or low-quality data in responses.',\n",
       " '• Dynamic Adaptability: Adjusts retrieval strategies dynamically based on query requirements.\\n• Improved Accuracy: Critic module reduces the risk of irrelevant or low-quality data in responses.\\n• Scalable Modularity: Supports the addition of new agents for specialized tasks, enhancing scalability.\\n24',\n",
       " 'Use Case: Healthcare Diagnostics\\nPrompt: What are the common symptoms of Type 2 Diabetes, and how are they related to heart disease?\\nSystem Process (Agent-G Workflow):\\n1. Query Reception and Assignment: The system receives the query and identifies the need for both\\ngraph-structured and unstructured data to answer the question comprehensively.\\n2. Graph Retriever:\\n• Extracts relationships between Type 2 Diabetes and heart disease from a medical knowledge\\ngraph.\\n• Identifies shared risk factors such as obesity and high blood pressure by exploring graph hierar-\\nchies and relationships.\\n3. Document Retriever:\\n• Retrieves descriptions of Type 2 Diabetes symptoms (e.g., increased thirst, frequent urination,\\nfatigue) from medical literature.\\n• Adds contextual information to complement the graph-based insights.\\n4. Critic Module:\\n• Evaluates the relevance and quality of the retrieved graph data and document data.\\n• Flags low-confidence results for refinement or re-querying.',\n",
       " '4. Critic Module:\\n• Evaluates the relevance and quality of the retrieved graph data and document data.\\n• Flags low-confidence results for refinement or re-querying.\\n5. Response Synthesis: The LLM integrates validated data from the Graph Retriever and Document\\nRetriever into a coherent response, ensuring alignment with the query’s intent.\\nResponse:\\nIntegrated Response: “Type 2 Diabetes symptoms include increased thirst, frequent urination, and fatigue.\\nStudies show a 50% correlation between diabetes and heart disease, primarily through shared risk factors such\\nas obesity and high blood pressure.”\\n5.6.2\\nGeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation\\nGeAR [35]: introduces an agentic framework that enhances traditional Retrieval-Augmented Generation (RAG) systems\\nby incorporating graph-based retrieval mechanisms. By leveraging graph expansion techniques and an agent-based',\n",
       " 'by incorporating graph-based retrieval mechanisms. By leveraging graph expansion techniques and an agent-based\\narchitecture, GeAR addresses challenges in multi-hop retrieval scenarios, improving the system’s ability to handle\\ncomplex queries as shown in Figure 22.\\nKey Idea of GeAR\\nGeAR advances RAG performance through two primary innovations:\\n• Graph Expansion: Enhances conventional base retrievers (e.g., BM25) by expanding the retrieval process to\\ninclude graph-structured data, enabling the system to capture complex relationships and dependencies between\\nentities.\\n• Agent Framework: Incorporates an agent-based architecture that utilizes graph expansion to manage retrieval\\ntasks more effectively, allowing for dynamic and autonomous decision-making in the retrieval process.\\nWorkflow:\\nThe GeAR system operates through the following components:\\n1. Graph Expansion Module:\\n• Integrates graph-based data into the retrieval process, allowing the system to consider relationships',\n",
       " 'Workflow:\\nThe GeAR system operates through the following components:\\n1. Graph Expansion Module:\\n• Integrates graph-based data into the retrieval process, allowing the system to consider relationships\\nbetween entities during retrieval.\\n• Enhances the base retriever’s ability to handle multi-hop queries by expanding the search space to include\\nconnected entities.\\n2. Agent-Based Retrieval:\\n• Employs an agent framework to manage the retrieval process, enabling dynamic selection and combination\\nof retrieval strategies based on the query’s complexity.\\n• Agents can autonomously decide to utilize graph-expanded retrieval paths to improve the relevance and\\naccuracy of retrieved information.\\n25',\n",
       " '3. LLM Integration:\\n• Combines the retrieved information, enriched by graph expansion, with the capabilities of a Large\\nLanguage Model (LLM) to generate coherent and contextually relevant responses.\\n• The integration ensures that the generative process is informed by both unstructured documents and\\nstructured graph data.\\nFigure 22: An Overview of GeAR: Graph-Enhanced Agent for Retrieval-Augmented Generation[35]\\nKey Features and Advantages\\n• Enhanced Multi-Hop Retrieval: GeAR’s graph expansion allows the system to handle complex queries that\\nrequire reasoning over multiple interconnected pieces of information.\\n• Agentic Decision-Making: The agent framework enables dynamic and autonomous selection of retrieval\\nstrategies, improving efficiency and relevance.\\n• Improved Accuracy: By incorporating structured graph data, GeAR enhances the precision of retrieved\\ninformation, leading to more accurate and contextually appropriate responses.',\n",
       " '• Improved Accuracy: By incorporating structured graph data, GeAR enhances the precision of retrieved\\ninformation, leading to more accurate and contextually appropriate responses.\\n• Scalability: The modular nature of the agent framework allows for the integration of additional retrieval\\nstrategies and data sources as needed.\\n26',\n",
       " 'Use Case: Multi-Hop Question Answering\\nPrompt: Which author influenced the mentor of J.K. Rowling?\\nSystem Process (GeAR Workflow):\\n1. Top-Tier Agent: Evaluates the query’s multi-hop nature and determines that a combination of graph\\nexpansion and document retrieval is necessary to answer the question.\\n2. Graph Expansion Module:\\n• Identifies that J.K. Rowling’s mentor is a key entity in the query.\\n• Traces the literary influences on that mentor by exploring graph-structured data on literary\\nrelationships.\\n3. Agent-Based Retrieval:\\n• An agent autonomously selects the graph-expanded retrieval path to gather relevant information\\nabout the mentor’s influences.\\n• Integrates additional context by querying textual data sources for unstructured details about the\\nmentor and their influences.\\n4. Response Synthesis: Combines insights from the graph and document retrieval processes using the\\nLLM to generate a response that accurately reflects the complex relationships in the query.\\nResponse:',\n",
       " 'LLM to generate a response that accurately reflects the complex relationships in the query.\\nResponse:\\nIntegrated Response: “J.K. Rowling’s mentor, [Mentor Name], was heavily influenced by [Author Name],\\nknown for their [notable works or genre]. This connection highlights the layered relationships in literary history,\\nwhere influential ideas often pass through multiple generations of authors.”\\n5.7\\nAgentic Document Workflows in Agentic RAG\\nAgentic Document Workflows (ADW) [36] extend traditional Retrieval-Augmented Generation (RAG) paradigms by\\nenabling end-to-end knowledge work automation. These workflows orchestrate complex document-centric processes,\\nintegrating document parsing, retrieval, reasoning, and structured outputs with intelligent agents (see Figure 23). ADW\\nsystems address limitations of Intelligent Document Processing (IDP) and RAG by maintaining state, coordinating\\nmulti-step workflows, and applying domain-specific logic to documents.\\nWorkflow',\n",
       " 'systems address limitations of Intelligent Document Processing (IDP) and RAG by maintaining state, coordinating\\nmulti-step workflows, and applying domain-specific logic to documents.\\nWorkflow\\n1. Document Parsing and Information Structuring:\\n• Documents are parsed using enterprise-grade tools (e.g., LlamaParse) to extract relevant data fields such\\nas invoice numbers, dates, vendor information, line items, and payment terms.\\n• Structured data is organized for downstream processing.\\n2. State Maintenance Across Processes:\\n• The system maintains state about document context, ensuring consistency and relevance across multi-step\\nworkflows.\\n• Tracks the progression of the document through various processing stages.\\n3. Knowledge Retrieval:\\n• Relevant references are retrieved from external knowledge bases (e.g., LlamaCloud) or vector indexes.\\n• Retrieves real-time, domain-specific guidelines for enhanced decision-making.\\n4. Agentic Orchestration:',\n",
       " '• Retrieves real-time, domain-specific guidelines for enhanced decision-making.\\n4. Agentic Orchestration:\\n• Intelligent agents apply business rules, perform multi-hop reasoning, and generate actionable recommen-\\ndations.\\n• Orchestrates components such as parsers, retrievers, and external APIs for seamless integration.\\n5. Actionable Output Generation:\\n• Outputs are presented in structured formats, tailored to specific use cases.\\n• Recommendations and extracted insights are synthesized into concise and actionable reports.\\n27',\n",
       " 'Figure 23: An Overview of Agentic Document Workflows (ADW)\\n[36]\\nUse Case: Invoice Payments Workflow\\nPrompt: Generate a payment recommendation report based on the submitted invoice and associated vendor\\ncontract terms.\\nSystem Process (ADW Workflow):\\n1. Parse the invoice to extract key details such as invoice number, date, vendor information, line items,\\nand payment terms.\\n2. Retrieve the corresponding vendor contract to verify payment terms and identify any applicable\\ndiscounts or compliance requirements.\\n3. Generate a payment recommendation report that includes original amount due, potential early payment\\ndiscounts, budget impact analysis, and strategic payment actions.\\nResponse: Integrated Response: \"Invoice INV-2025-045 for $15,000.00 has been processed. An early payment\\ndiscount of 2% is available if paid by 2025-04-10, reducing the amount due to $14,700.00. A bulk order discount',\n",
       " 'discount of 2% is available if paid by 2025-04-10, reducing the amount due to $14,700.00. A bulk order discount\\nof 5% was applied as the subtotal exceeded $10,000.00. It is recommended to approve early payment to save\\n2% and ensure timely fund allocation for upcoming project phases.\"\\nKey Features and Advantages\\n• State Maintenance: Tracks document context and workflow stage, ensuring consistency across processes.\\n• Multi-Step Orchestration: Handles complex workflows involving multiple components and external tools.\\n• Domain-Specific Intelligence: Applies tailored business rules and guidelines for precise recommendations.\\n• Scalability: Supports large-scale document processing with modular and dynamic agent integration.\\n• Enhanced Productivity: Automates repetitive tasks while augmenting human expertise in decision-making.\\n28',\n",
       " '6\\nComparative Analysis of Agentic RAG Frameworks\\nTable 2 provides a comprehensive comparative analysis of the three architectural frameworks: Traditional RAG, Agentic\\nRAG, and Agentic Document Workflows (ADW). This analysis highlights their respective strengths, weaknesses, and\\nbest-fit scenarios, offering valuable insights into their applicability across diverse use cases.\\nTable 2: Comparative Analysis: Traditional RAG vs Agentic RAG vs Agentic Document Workflows (ADW)\\nFeature\\nTraditional RAG\\nAgentic RAG\\nAgentic Document\\nWorkflows (ADW)\\nFocus\\nIsolated retrieval and\\ngeneration tasks\\nMulti-agent\\ncollaboration and\\nreasoning\\nDocument-centric\\nend-to-end workflows\\nContext Maintenance\\nLimited\\nEnabled through\\nmemory modules\\nMaintains state across\\nmulti-step workflows\\nDynamic Adaptability\\nMinimal\\nHigh\\nTailored to document\\nworkflows\\nWorkflow\\nOrchestration\\nAbsent\\nOrchestrates multi-agent\\ntasks\\nIntegrates multi-step\\ndocument processing\\nUse of External\\nTools/APIs\\nBasic integration (e.g.,',\n",
       " 'Minimal\\nHigh\\nTailored to document\\nworkflows\\nWorkflow\\nOrchestration\\nAbsent\\nOrchestrates multi-agent\\ntasks\\nIntegrates multi-step\\ndocument processing\\nUse of External\\nTools/APIs\\nBasic integration (e.g.,\\nretrieval tools)\\nExtends via tools like\\nAPIs and knowledge\\nbases\\nDeeply integrates business\\nrules and domain-specific\\ntools\\nScalability\\nLimited to small\\ndatasets or queries\\nScalable for multi-agent\\nsystems\\nScales for multi-domain\\nenterprise workflows\\nComplex Reasoning\\nBasic (e.g., simple\\nQ&A)\\nMulti-step reasoning\\nwith agents\\nStructured reasoning across\\ndocuments\\nPrimary Applications\\nQA systems, knowledge\\nretrieval\\nMulti-domain\\nknowledge and\\nreasoning\\nContract review, invoice\\nprocessing, claims analysis\\nStrengths\\nSimplicity, quick setup\\nHigh accuracy,\\ncollaborative reasoning\\nEnd-to-end automation,\\ndomain-specific intelligence\\nChallenges\\nPoor contextual\\nunderstanding\\nCoordination\\ncomplexity\\nResource overhead, domain\\nstandardization',\n",
       " 'High accuracy,\\ncollaborative reasoning\\nEnd-to-end automation,\\ndomain-specific intelligence\\nChallenges\\nPoor contextual\\nunderstanding\\nCoordination\\ncomplexity\\nResource overhead, domain\\nstandardization\\nThe comparative analysis underscores the evolutionary trajectory from Traditional RAG to Agentic RAG and further to\\nAgentic Document Workflows (ADW). While Traditional RAG offers simplicity and ease of deployment for basic tasks,\\nAgentic RAG introduces enhanced reasoning and scalability through multi-agent collaboration. ADW builds upon these\\nadvancements by providing robust, document-centric workflows that facilitate end-to-end automation and integration\\nwith domain-specific processes. Understanding the strengths and limitations of each framework is crucial for selecting\\nthe most appropriate architecture to meet specific application requirements and operational demands.\\n7\\nApplications of Agentic RAG',\n",
       " 'the most appropriate architecture to meet specific application requirements and operational demands.\\n7\\nApplications of Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems have demonstrated transformative potential across a variety\\nof domains. By combining real-time data retrieval, generative capabilities, and autonomous decision-making, these\\nsystems address complex, dynamic, and multi-modal challenges. This section explores the key applications of Agentic\\nRAG, providing detailed insights into how these systems are shaping industries such as customer support, healthcare,\\nfinance, education, legal workflows, and creative industries.\\n7.1\\nCustomer Support and Virtual Assistants\\nAgentic RAG systems are revolutionizing customer support by enabling real-time, context-aware query resolution.\\nTraditional chatbots and virtual assistants often rely on static knowledge bases, leading to generic or outdated responses.\\n29',\n",
       " 'By contrast, Agentic RAG systems dynamically retrieve the most relevant information, adapt to the user’s context, and\\ngenerate personalized responses.\\nUse Case: Twitch Ad Sales Enhancement [37]\\nFor instance, Twitch leveraged an agentic workflow with RAG on Amazon Bedrock to streamline ad sales. The system\\ndynamically retrieved advertiser data, historical campaign performance, and audience demographics to generate detailed\\nad proposals, significantly boosting operational efficiency.\\nKey Benefits:\\n• Improved Response Quality: Personalized and context-aware replies enhance user engagement.\\n• Operational Efficiency: Reduces the workload on human support agents by automating complex queries.\\n• Real-Time Adaptability: Dynamically integrates evolving data, such as live service outages or pricing\\nupdates.\\n7.2\\nHealthcare and Personalized Medicine\\nIn healthcare, the integration of patient-specific data with the latest medical research is critical for informed decision-',\n",
       " 'updates.\\n7.2\\nHealthcare and Personalized Medicine\\nIn healthcare, the integration of patient-specific data with the latest medical research is critical for informed decision-\\nmaking. Agentic RAG systems enable this by retrieving real-time clinical guidelines, medical literature, and patient\\nhistory to assist clinicians in diagnostics and treatment planning.\\nUse Case: Patient Case Summary [38]\\nAgentic RAG systems have been applied in generating patient case summaries. For example, by integrating electronic\\nhealth records (EHR) and up-to-date medical literature, the system generates comprehensive summaries for clinicians\\nto make faster and more informed decisions.\\nKey Benefits:\\n• Personalized Care: Tailors recommendations to individual patient needs.\\n• Time Efficiency: Streamlines the retrieval of relevant research, saving valuable time for healthcare providers.\\n• Accuracy: Ensures recommendations are based on the latest evidence and patient-specific parameters.\\n7.3',\n",
       " '• Accuracy: Ensures recommendations are based on the latest evidence and patient-specific parameters.\\n7.3\\nLegal and Contract Analysis\\nAgentic RAG systems are redefining how legal workflows are conducted, offering tools for rapid document analysis and\\ndecision-making.\\nUse Case: Contract Review [39]\\nA legal agentic RAG system can analyze contracts, extract critical clauses, and identify potential risks. By combining\\nsemantic search capabilities with legal knowledge graphs, it automates the tedious process of contract review, ensuring\\ncompliance and mitigating risks.\\nKey Benefits:\\n• Risk Identification: Automatically flags clauses that deviate from standard terms.\\n• Efficiency: Reduces the time spent on contract review processes.\\n• Scalability: Handles large volumes of contracts simultaneously.\\n7.4\\nFinance and Risk Analysis\\nAgentic RAG systems are transforming the finance industry by providing real-time insights for investment decisions,',\n",
       " '7.4\\nFinance and Risk Analysis\\nAgentic RAG systems are transforming the finance industry by providing real-time insights for investment decisions,\\nmarket analysis, and risk management. These systems integrate live data streams, historical trends, and predictive\\nmodeling to generate actionable outputs.\\nUse Case: Auto Insurance Claims Processing [40]\\nIn auto insurance, Agentic RAG can automate claim processing. For example, by retrieving policy details and combining\\nthem with accident data, it generates claim recommendations while ensuring compliance with regulatory requirements.\\nKey Benefits:\\n• Real-Time Analytics: Delivers insights based on live market data.\\n30',\n",
       " '• Risk Mitigation: Identifies potential risks using predictive analysis and multi-step reasoning.\\n• Enhanced Decision-Making: Combines historical and live data for comprehensive strategies.\\n7.5\\nEducation and Personalized Learning\\nEducation is another domain where Agentic RAG systems are making significant strides. These systems enable adaptive\\nlearning by generating explanations, study materials, and feedback tailored to the learner’s progress and preferences.\\nUse Case: Research Paper Generation [41]\\nIn higher education, Agentic RAG has been used to assist researchers by synthesizing key findings from multiple\\nsources. For instance, a researcher querying, “What are the latest advancements in quantum computing?” receives a\\nconcise summary enriched with references, enhancing the quality and efficiency of their work.\\nKey Benefits:\\n• Tailored Learning Paths: Adapts content to individual student needs and performance levels.',\n",
       " 'Key Benefits:\\n• Tailored Learning Paths: Adapts content to individual student needs and performance levels.\\n• Engaging Interactions: Provides interactive explanations and personalized feedback.\\n• Scalability: Supports large-scale deployments for diverse educational environments.\\n7.6\\nGraph-Enhanced Applications in Multimodal Workflows\\nGraph-Enhanced Agentic RAG (GEAR) combines graph structures with retrieval mechanisms, making it particularly\\neffective in multimodal workflows where interconnected data sources are essential.\\nUse Case: Market Survey Generation\\nGEAR enables the synthesis of text, images, and videos for marketing campaigns. For example, querying, “What\\nare the emerging trends in eco-friendly products?” generates a detailed report enriched with customer preferences,\\ncompetitor analysis, and multimedia content.\\nKey Benefits:\\n• Multi-Modal Capabilities: Integrates text, image, and video data for comprehensive outputs.',\n",
       " 'competitor analysis, and multimedia content.\\nKey Benefits:\\n• Multi-Modal Capabilities: Integrates text, image, and video data for comprehensive outputs.\\n• Enhanced Creativity: Generates innovative ideas and solutions for marketing and entertainment.\\n• Dynamic Adaptability: Adapts to evolving market trends and customer needs.\\nThe applications of Agentic RAG systems span a wide range of industries, showcasing their versatility and transformative\\npotential. From personalized customer support to adaptive education and graph-enhanced multimodal workflows, these\\nsystems address complex, dynamic, and knowledge-intensive challenges. By integrating retrieval, generation, and\\nagentic intelligence, Agentic RAG systems are paving the way for next-generation AI applications.\\n8\\nTools and Frameworks for Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant evolution in combining retrieval,',\n",
       " '8\\nTools and Frameworks for Agentic RAG\\nAgentic Retrieval-Augmented Generation (RAG) systems represent a significant evolution in combining retrieval,\\ngeneration, and agentic intelligence. These systems extend the capabilities of traditional RAG by integrating decision-\\nmaking, query reformulation, and adaptive workflows. The following tools and frameworks provide robust support for\\ndeveloping Agentic RAG systems, addressing the complex requirements of real-world applications.\\nKey Tools and Frameworks:\\n• LangChain and LangGraph: LangChain [42] provides modular components for building RAG pipelines,\\nseamlessly integrating retrievers, generators, and external tools. LangGraph complements this by introducing\\ngraph-based workflows that support loops, state persistence, and human-in-the-loop interactions, enabling\\nsophisticated orchestration and self-correction mechanisms in agentic systems.\\n• LlamaIndex: LlamaIndex’s [43] Agentic Document Workflows (ADW) enable end-to-end automation of',\n",
       " 'sophisticated orchestration and self-correction mechanisms in agentic systems.\\n• LlamaIndex: LlamaIndex’s [43] Agentic Document Workflows (ADW) enable end-to-end automation of\\ndocument processing, retrieval, and structured reasoning. It introduces a meta-agent architecture where\\nsub-agents manage smaller document sets, coordinating through a top-level agent for tasks such as compliance\\nanalysis and contextual understanding.\\n• Hugging Face Transformers and Qdrant: Hugging Face [44] offers pre-trained models for embedding and\\ngeneration tasks, while Qdrant [45] enhances retrieval workflows with adaptive vector search capabilities,\\nallowing agents to optimize performance by dynamically switching between sparse and dense vector methods.\\n31',\n",
       " '• CrewAI and AutoGen: These frameworks emphasize multi-agent architectures. CrewAI [46] supports\\nhierarchical and sequential processes, robust memory systems, and tool integrations. AG2 [47] (formerly\\nknows as AutoGen [48, 49]) excels in multi-agent collaboration with advanced support for code generation,\\ntool execution, and decision-making.\\n• OpenAI Swarm Framework: An educational framework designed for ergonomic, lightweight multi-agent\\norchestration [50], emphasizing agent autonomy and structured collaboration.\\n• Agentic RAG with Vertex AI: Developed by Google, Vertex AI [51] integrates seamlessly with Agentic\\nRetrieval-Augmented Generation (RAG), providing a platform to build, deploy, and scale machine learning\\nmodels while leveraging advanced AI capabilities for robust, contextually aware retrieval and decision-making\\nworkflows.\\n• Semantic Kernel: Semantic Kernel [52, 53] is an open-source SDK by Microsoft that integrates large language',\n",
       " 'workflows.\\n• Semantic Kernel: Semantic Kernel [52, 53] is an open-source SDK by Microsoft that integrates large language\\nmodels (LLMs) into applications. It supports agentic patterns, enabling the creation of autonomous AI agents\\nfor natural language understanding, task automation, and decision-making. It has been used in scenarios like\\nServiceNow’s P1 incident management to facilitate real-time collaboration, automate task execution, and\\nretrieve contextual information seamlessly\\n• Amazon Bedrock for Agentic RAG: Amazon Bedrock [37] provides a robust platform for implementing\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n• IBM Watson and Agentic RAG: IBM’s watsonx.ai [54] supports building Agentic RAG systems, exemplified\\nby using the Granite-3-8B-Instruct model to answer complex queries by integrating external information and\\nenhancing response accuracy.\\n• Neo4j and Vector Databases: Neo4j, a prominent open-source graph database, excels in handling complex',\n",
       " 'enhancing response accuracy.\\n• Neo4j and Vector Databases: Neo4j, a prominent open-source graph database, excels in handling complex\\nrelationships and semantic queries. Alongside Neo4j, vector databases like Weaviate, Pinecone, Milvus, and\\nQdrant provide efficient similarity search and retrieval capabilities, forming the backbone of high-performance\\nAgentic Retrieval-Augmented Generation (RAG) workflows.\\n9\\nBenchmarks and Datasets\\nCurrent benchmarks and datasets provide valuable insights into evaluating Retrieval-Augmented Generation (RAG)\\nsystems, including those with agentic and graph-based enhancements. While some are explicitly designed for RAG,\\nothers are adapted to test retrieval, reasoning, and generation capabilities in diverse scenarios. Datasets are crucial for\\ntesting the retrieval, reasoning, and generation components of RAG systems. Table 3 discusses some key datasets based\\non the dowstream task for RAG Evaluation.',\n",
       " 'testing the retrieval, reasoning, and generation components of RAG systems. Table 3 discusses some key datasets based\\non the dowstream task for RAG Evaluation.\\nBenchmarks play a critical role in standardizing the evaluation of RAG systems by providing structured tasks and\\nmetrics. The following benchmarks are particularly relevant:\\n• BEIR (Benchmarking Information Retrieval): A versatile benchmark designed for evaluating embedding\\nmodels on a variety of information retrieval tasks, encompassing 17 datasets across diverse domains like\\nbioinformatics, finance, and question answering [55].\\n• MS MARCO (Microsoft Machine Reading Comprehension): Focused on passage ranking and question\\nanswering, this benchmark is widely used for dense retrieval tasks in RAG systems [56].\\n• TREC (Text REtrieval Conference, Deep Learning Track): Provides datasets for passage and document\\nretrieval, emphasizing the quality of ranking models in retrieval pipelines [57].',\n",
       " '• TREC (Text REtrieval Conference, Deep Learning Track): Provides datasets for passage and document\\nretrieval, emphasizing the quality of ranking models in retrieval pipelines [57].\\n• MuSiQue (Multihop Sequential Questioning): A benchmark for multihop reasoning across multiple\\ndocuments, emphasizing the importance of retrieving and synthesizing information from disconnected contexts\\n[58].\\n• 2WikiMultihopQA: A dataset designed for multihop QA tasks over two Wikipedia articles, focusing on the\\nability to connect knowledge across multiple sources [59].\\n• AgentG (Agentic RAG for Knowledge Fusion): Tailored for agentic RAG tasks, this benchmark assesses\\ndynamic information synthesis across multiple knowledge bases [8].\\n• HotpotQA: A multi-hop QA benchmark requiring retrieval and reasoning over interconnected contexts, ideal\\nfor evaluating complex RAG workflows[60].\\n• RAGBench: A large-scale, explainable benchmark featuring 100,000 examples across industry domains, with',\n",
       " 'for evaluating complex RAG workflows[60].\\n• RAGBench: A large-scale, explainable benchmark featuring 100,000 examples across industry domains, with\\na TRACe evaluation framework for actionable RAG metrics [61].\\n32',\n",
       " '• BERGEN (Benchmarking Retrieval-Augmented Generation): A library for systematically benchmarking\\nRAG systems with standardized experiments [62].\\n• FlashRAG Toolkit: Implements 12 RAG methods and includes 32 benchmark datasets to support efficient\\nand standardized RAG evaluation [63].\\n• GNN-RAG: This benchmark evaluates graph-based RAG systems on tasks like node-level and edge-level\\npredictions, focusing on retrieval quality and reasoning performance in Knowledge Graph Question Answering\\n(KGQA) [64].\\nTable 3: Downstream Tasks and Datasets for RAG Evaluation (Adapted from [20]\\nCategory\\nTask Type\\nDatasets and References\\nQA\\nSingle-hop QA\\nNatural Questions (NQ) [65], TriviaQA [66], SQuAD [67],\\nWeb Questions (WebQ) [68], PopQA [69], MS MARCO\\n[56]\\nMulti-hop QA\\nHotpotQA [60], 2WikiMultiHopQA [59], MuSiQue [58]\\nLong-form QA\\nELI5 [70], NarrativeQA (NQA) [71], ASQA [72], QM-\\nSum [73]\\nDomain-specific QA\\nQasper [74], COVID-QA [75], CMB/MMCU Medical\\n[76]\\nMulti-choice QA',\n",
       " 'Long-form QA\\nELI5 [70], NarrativeQA (NQA) [71], ASQA [72], QM-\\nSum [73]\\nDomain-specific QA\\nQasper [74], COVID-QA [75], CMB/MMCU Medical\\n[76]\\nMulti-choice QA\\nQuALITY [77], ARC (No reference available), Common-\\nsenseQA [78]\\nGraph-based QA\\nGraph QA\\nGraphQA [79]\\nEvent Argument Extraction\\nWikiEvent [80], RAMS [81]\\nDialog\\nOpen-domain Dialog\\nWizard of Wikipedia (WoW) [82]\\nPersonalized Dialog\\nKBP [83], DuleMon [84]\\nTask-oriented Dialog\\nCamRest [85]\\nRecommendation\\nPersonalized Content\\nAmazon Datasets (Toys, Sports, Beauty) [86]\\nReasoning\\nCommonsense Reasoning\\nHellaSwag [87], CommonsenseQA [78]\\nCoT Reasoning\\nCoT Reasoning [88]\\nComplex Reasoning\\nCSQA [89]\\nOthers\\nLanguage Understanding\\nMMLU (No reference available), WikiText-103 [65]\\nFact Checking/Verification\\nFEVER [90], PubHealth [91]\\nStrategy QA\\nStrategyQA [92]\\nSummarization\\nText Summarization\\nWikiASP [93], XSum [94]\\nLong-form Summarization\\nNarrativeQA (NQA) [71], QMSum [73]\\nText Generation\\nBiography\\nBiography Dataset (No reference available)',\n",
       " 'Summarization\\nText Summarization\\nWikiASP [93], XSum [94]\\nLong-form Summarization\\nNarrativeQA (NQA) [71], QMSum [73]\\nText Generation\\nBiography\\nBiography Dataset (No reference available)\\nText Classification\\nSentiment Analysis\\nSST-2 [95]\\nGeneral Classification\\nVioLens[96], TREC [57]\\nCode Search\\nProgramming Search\\nCodeSearchNet [97]\\nRobustness\\nRetrieval Robustness\\nNoMIRACL [98]\\nLanguage Modeling Robustness\\nWikiText-103 [99]\\nMath\\nMath Reasoning\\nGSM8K [100]\\nMachine Translation\\nTranslation Tasks\\nJRC-Acquis [101]\\n10\\nConclusion\\nAgentic Retrieval-Augmented Generation (RAG) represents a transformative advancement in artificial intelligence,\\naddressing the limitations of traditional RAG systems through the integration of autonomous agents. By leveraging\\n33',\n",
       " 'agentic intelligence, these systems introduce capabilities such as dynamic decision-making, iterative reasoning, and\\ncollaborative workflows, enabling them to tackle complex, real-world tasks with enhanced precision and adaptability.\\nThis survey explored the evolution of RAG systems, from their initial implementations to advanced paradigms like\\nModular RAG, highlighting the contributions and limitations of each. The integration of agents into the RAG pipeline\\nhas emerged as a pivotal development, resulting in Agentic RAG systems that overcome static workflows and limited\\ncontextual adaptability. Applications across healthcare, finance, education, and creative industries demonstrate the\\ntransformative potential of these systems, showcasing their ability to deliver personalized, real-time, and context-aware\\nsolutions.\\nDespite their promise, Agentic RAG systems face challenges that require further research and innovation. Coordination',\n",
       " 'solutions.\\nDespite their promise, Agentic RAG systems face challenges that require further research and innovation. Coordination\\ncomplexity in multi-agent architectures, scalability, and latency issues, as well as ethical considerations, must be\\naddressed to ensure robust and responsible deployment. Additionally, the lack of specialized benchmarks and datasets\\ntailored to evaluate agentic capabilities poses a significant hurdle. Developing evaluation methodologies that capture\\nthe unique aspects of Agentic RAG, such as multi-agent collaboration and dynamic adaptability, will be crucial for\\nadvancing the field.\\nLooking ahead, the convergence of retrieval-augmented generation and agentic intelligence has the potential to redefine\\nAI’s role in dynamic and complex environments. By addressing these challenges and exploring future directions,\\nresearchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for transformative',\n",
       " 'researchers and practitioners can unlock the full potential of Agentic RAG systems, paving the way for transformative\\napplications across industries and domains. As AI systems continue to evolve, Agentic RAG stands as a cornerstone for\\ncreating adaptive, context-aware, and impactful solutions that meet the demands of a rapidly changing world.\\nReferences\\n[1] Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and\\nJianfeng Gao. Large language models: A survey, 2024.\\n[2] Aditi Singh. Exploring language models: A comprehensive survey and analysis. In 2023 International Con-\\nference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication\\nEngineering (RMKMATE), pages 1–4, 2023.\\n[3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\\nJunjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,',\n",
       " 'Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren,\\nYifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language\\nmodels, 2024.\\n[4] Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. A complete survey on llm-based ai\\nchatbots, 2024.\\n[5] Aditi Singh. A survey of ai text-to-image and ai text-to-video generators. In 2023 4th International Conference\\non Artificial Intelligence, Robotics and Control (AIRC), pages 32–36, 2023.\\n[6] Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and Tala Talaei Khoei.\\nExploring prompt engineering: A systematic review with swot analysis, 2024.\\n[7] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua\\nPeng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles,',\n",
       " 'Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles,\\ntaxonomy, challenges, and open questions. ACM Transactions on Information Systems, November 2024.\\n[8] Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala,\\nand Christos Faloutsos. Agent-g: An agentic framework for graph retrieval augmented generation, 2024.\\n[9] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao\\nZhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: A survey, 2024.\\n[10] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan,\\nand Graham Neubig. Active retrieval augmented generation, 2023.\\n[11] Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n[12] Anthropic.\\nBuilding\\neffective\\nagents,\\n2024.',\n",
       " '[11] Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval\\ntechnique, challenge, 2023.\\n[12] Anthropic.\\nBuilding\\neffective\\nagents,\\n2024.\\nhttps://www.anthropic.com/research/\\nbuilding-effective-agents. Accessed: February 2, 2025.\\n[13] LangChain.\\nLanggraph workflows tutorial, 2025.\\nhttps://langchain-ai.github.io/langgraph/\\ntutorials/workflows/. Accessed: February 2, 2025.\\n34',\n",
       " '[14] Chidaksh Ravuru, Sagar Srinivas Sakhinana, and Venkataramana Runkana.\\nAgentic retrieval-augmented\\ngeneration for time series analysis, 2024.\\n[15] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey, 2023.\\n[16] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang.\\nGraph retrieval-augmented generation: A survey, 2024.\\n[17] Aditi Singh, Abul Ehtesham, Saifuddin Mahmud, and Jong-Hoon Kim. Revolutionizing mental health care\\nthrough langchain: A journey with a large language model. In 2024 IEEE 14th Annual Computing and\\nCommunication Workshop and Conference (CCWC), pages 0073–0078, 2024.\\n[18] Gaurav Kumar Gupta, Aditi Singh, Sijo Valayakkad Manikandan, and Abul Ehtesham. Digital diagnostics: The\\npotential of large language models in recognizing symptoms of common illnesses. AI, 6(1), 2025.\\n[19] Aditi Singh, Abul Ehtesham, Saket Kumar, Gaurav Kumar Gupta, and Tala Talaei Khoei. Encouraging responsible',\n",
       " '[19] Aditi Singh, Abul Ehtesham, Saket Kumar, Gaurav Kumar Gupta, and Tala Talaei Khoei. Encouraging responsible\\nuse of generative ai in education: A reward-based learning approach. In Tim Schlippe, Eric C. K. Cheng, and\\nTianchong Wang, editors, Artificial Intelligence in Education Technologies: New Development and Innovative\\nPractices, pages 404–413, Singapore, 2025. Springer Nature Singapore.\\n[20] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and\\nHaofen Wang. Retrieval-augmented generation for large language models: A survey, 2024.\\n[21] Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen\\ntau Yih. Dense passage retrieval for open-domain question answering, 2020.\\n[22] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong\\nWen. A survey on the memory mechanism of large language model based agents, 2024.',\n",
       " '[22] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong\\nWen. A survey on the memory mechanism of large language model based agents, 2024.\\n[23] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large\\nlanguage models can self-correct with tool-interactive critiquing, 2024.\\n[24] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang,\\nand Enhong Chen. Understanding the planning of llm agents: A survey, 2024.\\n[25] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Enhancing ai systems with agentic workflows\\npatterns in large language model. In 2024 IEEE World AI IoT Congress (AIIoT), pages 527–532, 2024.\\n[26] DeepLearning.AI. How agents can improve llm performance. https://www.deeplearning.ai/the-batch/\\nhow-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io,\\n2024.\\nAc-\\ncessed: 2025-01-13.',\n",
       " 'how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io,\\n2024.\\nAc-\\ncessed: 2025-01-13.\\n[27] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\\nDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann,\\nSean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback, 2023.\\n[28] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\\nReflexion: Language agents with verbal reinforcement learning, 2023.\\n[29] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and\\nXiangliang Zhang. Large language model based multi-agents: A survey of progress and challenges, 2024.\\n[30] Weaviate Blog. What is agentic rag? https://weaviate.io/blog/what-is-agentic-rag#:~:text=is%\\n20Agentic%20RAG%3F-,%E2%80%8B,of%20the%20non%2Dagentic%20pipeline. Accessed: 2025-01-14.',\n",
       " '[30] Weaviate Blog. What is agentic rag? https://weaviate.io/blog/what-is-agentic-rag#:~:text=is%\\n20Agentic%20RAG%3F-,%E2%80%8B,of%20the%20non%2Dagentic%20pipeline. Accessed: 2025-01-14.\\n[31] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation, 2024.\\n[32] LangGraph CRAG Tutorial. Langgraph crag: Contextualized retrieval-augmented generation tutorial. https:\\n//langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/. Accessed: 2025-01-14.\\n[33] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. Adaptive-rag: Learning to adapt\\nretrieval-augmented large language models through question complexity, 2024.\\n[34] LangGraph Adaptive RAG Tutorial. Langgraph adaptive rag: Adaptive retrieval-augmented generation tu-\\ntorial.\\nhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/.\\nAccessed: 2025-01-14.',\n",
       " 'torial.\\nhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/.\\nAccessed: 2025-01-14.\\n[35] Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Damien Graux, Dandan\\nTu, Zeren Jiang, Ruofei Lai, Yang Ren, and Jeff Z. Pan. Gear: Graph-enhanced agent for retrieval-augmented\\ngeneration, 2024.\\n[36] LlamaIndex.\\nIntroducing agentic\\ndocument\\nworkflows.\\nhttps://www.llamaindex.ai/blog/\\nintroducing-agentic-document-workflows, 2025. Accessed: 2025-01-13.\\n35',\n",
       " '[37] AWS\\nMachine\\nLearning\\nBlog.\\nHow\\ntwitch\\nused\\nagentic\\nworkflow\\nwith\\nrag\\non\\namazon\\nbedrock\\nto\\nsupercharge\\nad\\nsales.\\nhttps://aws.amazon.com/blogs/machine-learning/\\nhow-twitch-used-agentic-workflow-with-rag-on-amazon-bedrock-to-supercharge-ad-sales/,\\n2025. Accessed: 2025-01-13.\\n[38] LlamaCloud Demo Repository.\\nPatient case summary workflow using llamacloud.\\nhttps:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/\\npatient_case_summary/patient_case_summary.ipynb, 2025. Accessed: 2025-01-13.\\n[39] LlamaCloud Demo Repository.\\nContract review workflow using llamacloud.\\nhttps://github.com/\\nrun-llama/llamacloud-demo/blob/main/examples/document_workflows/contract_review/\\ncontract_review.ipynb, 2025. Accessed: 2025-01-13.\\n[40] LlamaCloud Demo Repository.\\nAuto insurance claims workflow using llamacloud.\\nhttps:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/auto_\\ninsurance_claims/auto_insurance_claims.ipynb, 2025. Accessed: 2025-01-13.',\n",
       " 'https:\\n//github.com/run-llama/llamacloud-demo/blob/main/examples/document_workflows/auto_\\ninsurance_claims/auto_insurance_claims.ipynb, 2025. Accessed: 2025-01-13.\\n[41] LlamaCloud Demo Repository.\\nResearch paper report generation workflow using llamacloud.\\nhttps://github.com/run-llama/llamacloud-demo/blob/main/examples/report_generation/\\nresearch_paper_report_generation.ipynb, 2025. Accessed: 2025-01-13.\\n[42] LangGraph Agentic RAG Tutorial. Langgraph agentic rag: Nodes and edges tutorial. https://langchain-ai.\\ngithub.io/langgraph/tutorials/rag/langgraph_agentic_rag/#nodes-and-edges.\\nAccessed:\\n2025-01-14.\\n[43] LlamaIndex\\nBlog.\\nAgentic\\nrag\\nwith\\nllamaindex.\\nhttps://www.llamaindex.ai/blog/\\nagentic-rag-with-llamaindex-2721b8a49ff6. Accessed: 2025-01-14.\\n[44] Hugging Face Cookbook. Agentic rag: Turbocharge your retrieval-augmented generation with query reformula-\\ntion and self-query. https://huggingface.co/learn/cookbook/en/agent_rag. Accessed: 2025-01-14.',\n",
       " 'tion and self-query. https://huggingface.co/learn/cookbook/en/agent_rag. Accessed: 2025-01-14.\\n[45] Qdrant Blog. Agentic rag: Combining rag with agents for enhanced information retrieval. https://qdrant.\\ntech/articles/agentic-rag/. Accessed: 2025-01-14.\\n[46] crewAI Inc. crewai: A github repository for ai projects. https://github.com/crewAIInc/crewAI, 2025.\\nAccessed: 2025-01-15.\\n[47] AG2AI Contributors. Ag2: A github repository for advanced generative ai research. https://github.com/\\nag2ai/ag2, 2025. Accessed: 2025-01-15.\\n[48] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun\\nZhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and Chi Wang. Autogen: Enabling\\nnext-gen llm applications via multi-agent conversation framework. 2023.\\n[49] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Training\\nlanguage model agents without modifying language models. ICML’24, 2024.',\n",
       " '[49] Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Training\\nlanguage model agents without modifying language models. ICML’24, 2024.\\n[50] OpenAI. Swarm: Lightweight multi-agent orchestration framework. https://github.com/openai/swarm.\\nAccessed: 2025-01-14.\\n[51] LlamaIndex Documentation. Agentic rag using vertex ai. https://docs.llamaindex.ai/en/stable/\\nexamples/agent/agentic_rag_using_vertex_ai/. Accessed: 2025-01-14.\\n[52] Microsoft. Semantic kernel overview, 2025. https://learn.microsoft.com/en-us/semantic-kernel/\\noverview/. Accessed: February 2, 2025.\\n[53] Microsoft. Semantic kernel github repository, 2025. https://github.com/microsoft/semantic-kernel.\\nAccessed: February 2, 2025.\\n[54] IBM Granite Community.\\nAgentic rag: Ai agents with ibm granite models.\\nhttps://github.com/\\nibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_\\nRAG.ipynb. Accessed: 2025-01-14.',\n",
       " 'Agentic rag: Ai agents with ibm granite models.\\nhttps://github.com/\\nibm-granite-community/granite-snack-cookbook/blob/main/recipes/AI-Agents/Agentic_\\nRAG.ipynb. Accessed: 2025-01-14.\\n[55] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous\\nbenchmark for zero-shot evaluation of information retrieval models, 2021.\\n[56] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew\\nMcNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong\\nWang. Ms marco: A human generated machine reading comprehension dataset, 2018.\\n[57] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff.\\nOverview of the trec 2022 deep learning track. In Text REtrieval Conference (TREC). NIST, TREC, March 2023.\\n36',\n",
       " '[58] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\\nvia single-hop question composition, 2022.\\n[59] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset\\nfor comprehensive evaluation of reasoning steps, 2020.\\n[60] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christo-\\npher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.\\n[61] Robert Friel, Masha Belyi, and Atindriyo Sanyal. Ragbench: Explainable benchmark for retrieval-augmented\\ngeneration systems, 2024.\\n[62] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and Stéphane\\nClinchant. Bergen: A benchmarking library for retrieval-augmented generation, 2024.\\n[63] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient',\n",
       " 'Clinchant. Bergen: A benchmarking library for retrieval-augmented generation, 2024.\\n[63] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient\\nretrieval-augmented generation research, 2024.\\n[64] Costas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model reasoning,\\n2024.\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei\\nChang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question\\nanswering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019.\\n[66] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\\nchallenge dataset for reading comprehension, 2017.',\n",
       " '[66] Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\\nchallenge dataset for reading comprehension, 2017.\\n[67] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\\ncomprehension of text, 2016.\\n[68] Jonathan Berant, Andrew K. Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-\\nanswer pairs. In Conference on Empirical Methods in Natural Language Processing, 2013.\\n[69] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to\\ntrust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers,\\nJordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada, July 2023. Association\\nfor Computational Linguistics.',\n",
       " 'Computational Linguistics (Volume 1: Long Papers), pages 9802–9822, Toronto, Canada, July 2023. Association\\nfor Computational Linguistics.\\n[70] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5: Long form\\nquestion answering, 2019.\\n[71] Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward\\nGrefenstette. The narrativeqa reading comprehension challenge. 2017.\\n[72] Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form\\nanswers, 2023.\\n[73] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. pages 5905–5921, June 2021.\\n[74] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-',\n",
       " 'multi-domain meeting summarization. pages 5905–5921, June 2021.\\n[74] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-\\nseeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke\\nZettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao\\nZhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, pages 4599–4610, Online, June 2021. Association\\nfor Computational Linguistics.\\n[75] Timo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. COVID-QA: A question answering\\ndataset for COVID-19. In ACL 2020 Workshop on Natural Language Processing for COVID-19 (NLP-COVID),\\n2020.\\n[76] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang,',\n",
       " '2020.\\n[76] Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang,\\nJianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. Cmb: A comprehensive medical benchmark in chinese,\\n2024.\\n[77] Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh\\nPadmakumar, Johnny Ma, Jana Thompson, He He, and Samuel R. Bowman. Quality: Question answering with\\nlong input texts, yes!, 2022.\\n37',\n",
       " '[78] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering\\nchallenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors,\\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149–4158, Minneapolis,\\nMinnesota, June 2019. Association for Computational Linguistics.\\n[79] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V. Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan\\nHooi. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering,\\n2024.\\n[80] Sha Li, Heng Ji, and Jiawei Han. Document-level event argument extraction by conditional generation, 2021.\\n[81] Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. Multi-sentence argument\\nlinking, 2020.',\n",
       " '[81] Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. Multi-sentence argument\\nlinking, 2020.\\n[82] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia:\\nKnowledge-powered conversational agents, 2019.\\n[83] Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan,\\nIrwin King, and Kam-Fai Wong. Large language models as source planner for personalized knowledge-grounded\\ndialogue, 2023.\\n[84] Xinchao Xu, Zhibin Gou, Wenquan Wu, Zheng-Yu Niu, Hua Wu, Haifeng Wang, and Shihang Wang. Long time\\nno see! open-domain conversation with long-term persona memory, 2022.\\n[85] Tsung-Hsien Wen, Milica Gaši´c, Nikola Mrkši´c, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David\\nVandyke, and Steve Young. Conditional generation and snapshot learning in neural dialogue systems. In\\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2153–2162,',\n",
       " 'Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2153–2162,\\nAustin, Texas, November 2016. Association for Computational Linguistics.\\n[86] Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends with one-class\\ncollaborative filtering. In Proceedings of the 25th International Conference on World Wide Web, WWW ’16, page\\n507–517, Republic and Canton of Geneva, CHE, 2016. International World Wide Web Conferences Steering\\nCommittee.\\n[87] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really\\nfinish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Linguistics, pages 4791–4800, Florence, Italy, July 2019.\\nAssociation for Computational Linguistics.\\n[88] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The',\n",
       " 'Association for Computational Linguistics.\\n[88] Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. The\\ncot collection: Improving zero-shot and few-shot learning of language models via chain-of-thought fine-tuning,\\n2023.\\n[89] Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. Complex\\nsequential question answering: Towards learning to converse over linked question answer pairs with a knowledge\\ngraph. 2018.\\n[90] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for\\nfact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018\\nConference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.',\n",
       " 'Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for\\nComputational Linguistics.\\n[91] Neema Kotonya and Francesca Toni. Explainable automated fact-checking for public health claims, 2020.\\n[92] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop?\\na question answering benchmark with implicit reasoning strategies, 2021.\\n[93] Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig. Wikiasp:\\nA dataset for multi-domain aspect-based summarization, 2020.\\n[94] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware\\nconvolutional neural networks for extreme summarization, 2018.\\n[95] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher\\nPotts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky,',\n",
       " 'Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky,\\nTimothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013\\nConference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington,\\nUSA, October 2013. Association for Computational Linguistics.\\n38',\n",
       " '[96] Sourav Saha, Jahedul Alam Junaed, Maryam Saleki, Arnab Sen Sharma, Mohammad Rashidujjaman Rifat,\\nMohamed Rahouti, Syed Ishtiaque Ahmed, Nabeel Mohammed, and Mohammad Ruhul Amin. Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms of communal violence and its evaluation. In\\nFiroj Alam, Sudipta Kar, Shammur Absar Chowdhury, Farig Sadeque, and Ruhul Amin, editors, Proceedings\\nof the First Workshop on Bangla Language Processing (BLP-2023), pages 72–84, Singapore, December 2023.\\nAssociation for Computational Linguistics.\\n[97] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. Codesearchnet\\nchallenge: Evaluating the state of semantic code search, 2020.\\n[98] Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo,\\nXiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, and Jimmy Lin. \"knowing when you don’t know\":',\n",
       " 'Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi Rezagholizadeh, and Jimmy Lin. \"knowing when you don’t know\":\\nA multilingual relevance assessment dataset for robust retrieval-augmented generation, 2024.\\n[99] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\\n[100] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to\\nsolve math word problems, 2021.\\n[101] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaž Erjavec, Dan Tufi¸s, and Dániel Varga.\\nThe JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Nicoletta Calzolari, Khalid\\nChoukri, Aldo Gangemi, Bente Maegaard, Joseph Mariani, Jan Odijk, and Daniel Tapias, editors, Proceedings of\\nthe Fifth International Conference on Language Resources and Evaluation (LREC‘06), Genoa, Italy, May 2006.',\n",
       " 'the Fifth International Conference on Language Resources and Evaluation (LREC‘06), Genoa, Italy, May 2006.\\nEuropean Language Resources Association (ELRA).\\n39',\n",
       " 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly',\n",
       " 'entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder',\n",
       " 'transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and',\n",
       " 'efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.',\n",
       " 'Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in',\n",
       " 'Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building',\n",
       " 'The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions',\n",
       " 'described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate',\n",
       " 'aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:',\n",
       " 'connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2',\n",
       " 'Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This',\n",
       " 'around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3',\n",
       " 'Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is',\n",
       " '1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values h times with different, learned',\n",
       " 'we found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4',\n",
       " 'MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].',\n",
       " 'position in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks',\n",
       " 'of the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-',\n",
       " 'Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5',\n",
       " 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding',\n",
       " 'PE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-',\n",
       " 'during training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to',\n",
       " 'dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with',\n",
       " 'computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n6',\n",
       " 'the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.',\n",
       " 'convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece',\n",
       " 'target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:',\n",
       " '(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\nResidual Dropout\\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7',\n",
       " 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [15]\\n23.75\\nDeep-Att + PosUnk [32]\\n39.2\\n1.0 · 1020\\nGNMT + RL [31]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [8]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [26]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [32]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [31]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [8]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.0\\n2.3 · 1019\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation',\n",
       " 'hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.',\n",
       " 'previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2\\nModel Variations',\n",
       " 'model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8',\n",
       " 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This',\n",
       " '(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based',\n",
       " 'multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful',\n",
       " 'tensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9',\n",
       " 'References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.',\n",
       " 'machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in',\n",
       " 'Recognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.',\n",
       " '2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10',\n",
       " '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.',\n",
       " 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.',\n",
       " 'networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11',\n",
       " 'ScienceDirect\\nAvailable online at www.sciencedirect.com\\nProcedia Computer Science 246 (2024) 3781–3790\\n1877-0509 © 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge \\nBased and Intelligent information and Engineering Systems\\n10.1016/j.procs.2024.09.178\\nKeywords: Large Language Models (LLMs); Natural Language Processing (NLP); Retrieval-Augmented Generation (RAG); Text generation; \\nDigital transformation. \\n1. Introduction \\nDigital transformation signifies the incorporation of digital technology across different facets of a business, \\nreshaping its operations and value delivery to customers [1]. At the forefront of driving such transformative \\npractices are Large Language Models (LLMs), advanced machine learning models trained extensively on textual',\n",
       " 'practices are Large Language Models (LLMs), advanced machine learning models trained extensively on textual \\ndata to comprehend and produce human-like text [1]. LLMs, such as the Generative Pre-training Transformer (GPT) \\n \\n \\n* Corresponding author. Tel.: +33 03 80 39 50 00; fax: +33 03 80 39 50 69.  \\nE-mail address: muhammad.arslan@u-bourgogne.fr \\n28th International Conference on Knowledge-Based and Intelligent Information & Engineering \\nSystems (KES 2024) \\nA Survey on RAG with LLMs \\nMuhammad Arslana*, Hussam Ghanema, Saba Munawarb and Christophe Cruza \\naLaboratoire Interdisciplinaire Carnot de Bourgogne (ICB), Dijon, France \\nbNational University of Computer and Emerging Sciences (NUCES), Islamabad, Pakistan                                                                 \\nAbstract \\nIn the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain',\n",
       " 'Abstract \\nIn the fast-paced realm of digital transformation, businesses are increasingly pressured to innovate and boost efficiency to remain \\ncompetitive and foster growth. Large Language Models (LLMs) have emerged as game-changers across industries, \\nrevolutionizing various sectors by harnessing extensive text data to analyze and generate human-like text. Despite their \\nimpressive capabilities, LLMs often encounter challenges when dealing with domain-specific queries, potentially leading to \\ninaccuracies in their outputs. In response, Retrieval-Augmented Generation (RAG) has emerged as a viable solution. By \\nseamlessly integrating external data retrieval into text generation processes, RAG aims to enhance the accuracy and relevance of \\nthe generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG,',\n",
       " 'the generated content. However, existing literature reviews tend to focus primarily on the technological advancements of RAG, \\noverlooking a comprehensive exploration of its applications. This paper seeks to address this gap by providing a thorough review \\nof RAG applications, encompassing both task-specific and discipline-specific studies, while also outlining potential avenues for \\nfuture research. By shedding light on current RAG research and outlining future directions, this review aims to catalyze further \\nexploration and development in this dynamic field, thereby contributing to ongoing digital transformation efforts. \\n© 2024 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)\\nPeer-review under responsibility of the scientific committee of the 28th International Conference on Knowledge Based and \\nIntelligent information and Engineering Systems',\n",
       " '3782\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nseries [2, 3] and others, have demonstrated remarkable capabilities in NLP tasks [4]. However, these models face \\nchallenges when dealing with domain-specific queries, often generating inaccurate or irrelevant information, \\ncommonly referred to as “hallucinations”, particularly when data is sparse [5]. This limitation makes deploying \\nLLMs in real-world settings impractical, as the generated output may not be reliable [4].  \\nIn the middle of 2020, Lewis et al. [6] introduced RAG, a significant advancement in the field of LLMs for \\nimproving generative tasks (see Fig. 1 (a)). RAG incorporates an initial step where LLMs search an external data \\nsource to retrieve relevant information before producing text or answering questions. RAG addresses these \\nlimitations by integrating external data retrieval into the generative process, thereby enhancing the accuracy and',\n",
       " 'limitations by integrating external data retrieval into the generative process, thereby enhancing the accuracy and \\nrelevance of the generated output. By dynamically retrieving information from knowledge bases during inference, \\nRAG provides a more informed and evidence-based approach to language generation, significantly reducing the risk \\nof hallucinations and improving the overall quality of the generated text [4, 6]. This approach has the potential to \\nmake LLMs more practical for real-world applications, as it ensures that the generated output is grounded in \\nretrieved evidence, leading to more reliable and accurate results. Fig. 1 (b) showcases how real-time business \\nsystems can leverage the RAG with LLM architecture. As an example, without RAG, the system lacks access to \\nreal-time or updated information. However, with RAG integration, leveraging external data sources such as news',\n",
       " 'real-time or updated information. However, with RAG integration, leveraging external data sources such as news \\narticles, the system can respond to current business events, presenting opportunities for business intelligence \\nanalysts. \\n \\n                                  (a)                                                                                             (b) \\nFig. 1. (a) A generic RAG architecture, where users’ queries, potentially in different modalities (e.g., text, code, image, etc.), are inputted into \\nboth the retriever and the generator. The retriever scans for relevant data sources in storage, while the generator engages with the retrieval \\noutcomes, ultimately generating results across various modalities [6]; Fig. 1. (b) illustrates how RAG integration with the LLM handles queries \\nthat fall outside the scope of the LLM’s training data.  \\nWhile the field of RAG has seen substantial growth, several online surveys [4, 7, 8, 9] have explored',\n",
       " 'that fall outside the scope of the LLM’s training data.  \\nWhile the field of RAG has seen substantial growth, several online surveys [4, 7, 8, 9] have explored \\ntechnological advancements in RAG. Although these surveys provide valuable insights and references, they offer \\nonly a limited overview of RAG applications. To address this gap, this paper aims to provide an exhaustive \\noverview of RAG applications, including both task-specific and discipline-specific studies, as well as future \\ndirections. By highlighting the current state of RAG research and its potential future directions, this review aims to \\ninspire further investigation and development in this exciting field. \\nThe paper’s structure is as follows: Section 2 presents the adopted research methodology for this survey. In \\nSection 3, we provide an overview of RAG applications, followed by a detailed discussion in Section 4. The paper \\nconcludes in Section 5, summarizing the key findings and implications of the study.',\n",
       " 'Section 3, we provide an overview of RAG applications, followed by a detailed discussion in Section 4. The paper \\nconcludes in Section 5, summarizing the key findings and implications of the study. \\n2. Background \\nThe research method (see Fig. 2) employed in this paper involves a thorough review and analysis of research \\npublications related to RAG. The main objective is to identify and categorize its applications across various NLP \\ntasks and disciplines. The paper begins by collecting research publications specific to RAG, focusing on their \\napplications. Since the RAG with LLM domain is relatively new and emerging, with many studies available as pre-',\n",
       " 'Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3783\\nprints online, limiting the search to platforms such as Scopus or IEEE would greatly reduce the number of studies. \\nTherefore, Google Scholar was utilized to access the studies on RAG. However, in cases where both pre-print and \\npublished versions of a study were available, the published version was chosen to cover the maximum number of \\npeer-reviewed studies. Each study underwent manual review to assess its comprehensiveness and depth, excluding \\nshort studies. It is important to note that the purpose of the survey is not to cover the most optimal studies, but rather \\nto provide an overview of how this field has attained significant attention in a short period, with researchers \\nexploring diverse application scenarios.  \\nThe keywords used to collect research publications included “retrieval augmented generation”, “RAG',\n",
       " 'exploring diverse application scenarios.  \\nThe keywords used to collect research publications included “retrieval augmented generation”, “RAG \\napplications”, “generative models with retrieval”, “external data retrieval in text generation”, “enhancing text \\ngeneration with retrieval”, “integrating retrieval into generative models”, “external knowledge in text generation”, \\n“retrieval-based text generation”, “information retrieval for text generation”, and “contextualized retrieval in \\nlanguage models”. These publications are then classified into two principal categories: task-based classification and \\ndiscipline-based classification. Task-based classification focuses on categorizing RAG studies according to their \\nexecution of information processing tasks, particularly within NLP. Conversely, discipline-based classification \\ncategorizes studies based on their application to specific domains. Under the task-based classification, the',\n",
       " 'categorizes studies based on their application to specific domains. Under the task-based classification, the \\npublications are further subdivided into categories such as Question Answering (QA), Text Generation and \\nSummarization, Information Retrieval and Extraction, Text Analysis and Processing, Software Development and \\nMaintenance (SDM), Decision Making and Applications, and Other Categories. Similarly, under the discipline-\\nbased classification, the publications are further subdivided into categories such as Medical/Biomedical, Financial, \\nEducational, Technology and Software Development, Social and Communication, Literature, and Other Categories. \\nThese categories are selected based on an understanding of the context of the studies and the underlying problems \\nthey address. Within both classification methods, “software development” stands out as a common category. It \\ninvolves programming information processing tasks under task-based classification and encompasses systems for',\n",
       " 'involves programming information processing tasks under task-based classification and encompasses systems for \\ndeveloping various applications across different domains under discipline-based classification. Figure 3 illustrates \\nthe number of publications related to RAG applications from 2020 to February 2024. Specifically, there was a single \\npublication found in 2020, 6 publications in 2022, 28 publications in 2023, and 16 publications until February 2024, \\nindicating a growing interest and research activity in the field of RAG applications. \\nFig. 2. Research Method',\n",
       " '3784\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 3. Evolution of Research Publications on RAG Applications \\n3. Applications of RAG with LLMs \\nUpon thorough examination of the selected papers focusing on RAG applications, we uncovered a vast array of \\ndiverse applications. These findings are distilled into a comprehensive table format (see Table 1), detailing three \\ncrucial aspects: 1) Use case with RAG, 2) Used datasets/benchmarks, and 3) Application area. Noteworthy \\napplications span various domains, including biomedical, financial, and medical inquiries, alongside text \\nsummarization and book review generation. RAG’s versatility extends to commonsense QA, table-based queries, \\nand clinical decision-making, among others. It further encompasses educational decision making, textbook question \\nanswering, and enterprise search functionalities. RAG is instrumental in sentiments classification, health education,',\n",
       " 'answering, and enterprise search functionalities. RAG is instrumental in sentiments classification, health education, \\nand generating biomedical explanations, while also enhancing user writing accuracy and speed. Its utility spans \\nhumanitarian assistance, generating informative dialogues, crafting realistic images and intricate plotlines, and much \\nmore.  \\nAdditionally, RAG aids in natural language QA, disease identification, and information extraction. It handles \\ndecision-making tasks, hashtag management, hate speech detection, and scientific document classification. RAG \\nexcels in entity description generation, text correction, and SQL translation, while also enhancing open-domain QA \\nand professional knowledge inquiries. Also, it extends the capabilities of machine translation tasks beyond text-to-\\nSQL, such as neural text re-ranking [6]. Moreover, it supports multicultural enterprise queries, e-commerce',\n",
       " 'SQL, such as neural text re-ranking [6]. Moreover, it supports multicultural enterprise queries, e-commerce \\nsearches, and personalized dialogue systems. Furthermore, RAG facilitates event argument extraction, intelligence \\nreport generation, short-form QA, automated transactions, and private data handling. Lastly, it contributes to science \\nQA, clinical writing, and pharmaceutical regulatory compliance inquiries. \\nAfter compiling all the applications of RAG, the subsequent step involves categorizing them based on the \\nspecific nature of the NLP tasks they tackle (see Table 2 and Fig. 4). From the compiled publications, it was \\nobserved that 20 studies were dedicated to QA, 6 to Text Generation and Summarization, 6 to Information Retrieval \\nand Extraction, 5 to Text Analysis and Processing, 4 to SDM, and 5 to Decision Making and Applications, while the \\nremaining 6 studies were classified under \"Other Categories.\" This classification is significant as it helps in',\n",
       " 'remaining 6 studies were classified under \"Other Categories.\" This classification is significant as it helps in \\nunderstanding the distribution and focus of RAG applications across different NLP tasks. Additionally, since RAG \\napplications span various disciplines, further classification (see Table 3 and Fig. 5) reveals that 9 publications were \\nrelated to Medical/Biomedical, 2 to Financial, 2 to Educational, 9 to Technology and Software Development, 7 to \\nSocial and Communication, and 3 to Literature, with the remaining falling into \"Other Categories\". \\nTable 1. Applications of RAG  \\nNo. \\nUse case with RAG \\nUsed datasets / benchmarks \\nApplication area \\n1 \\nMIRAGE: Medical information RAG [10]  \\nMedical QA datasets \\nBiomedical QA \\n2 \\nRAG for improved context accuracy [11]  \\nFinancial reports \\nFinancial QA \\n3 \\nRetrieval-augmented Electrocardiography (ECG) [12]  \\nCardiac symptoms and sleep apnea diagnosis \\nMedical QA \\n4 \\nRepresentative Vector Summarization (RVS) [13]',\n",
       " 'Financial reports \\nFinancial QA \\n3 \\nRetrieval-augmented Electrocardiography (ECG) [12]  \\nCardiac symptoms and sleep apnea diagnosis \\nMedical QA \\n4 \\nRepresentative Vector Summarization (RVS) [13]  \\nPDFs, text documents, spreadsheets, etc.  \\nMedical text summarization \\n5 \\nRetrieval-augmented controllable reviews [14] \\nAmazon book reviews \\nBook review generation',\n",
       " 'Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3785\\n6 \\nRetrieval-augmented knowledge graph reasoning [15] \\nCommonsense QA and OpenBookQA. \\nCommonsense QA \\n7 \\nAnswers from table corpus via RAG [16] \\nWikipedia data \\nTable QA  \\n8 \\nLiVersa: a liver disease specific LLM using RAG [17] \\nLiver Diseases  \\nMedical QA \\n9 \\nAlmanac: RAG for clinical medicine [18] \\nGuidelines and treatment recommendations. \\nClinical decision-making \\n10 \\nAssessment of tutoring practices [19] \\nDialogue transcripts from a middle-school.  \\nEducational decision making \\n11 \\nHandling out of domain scenarios [20] \\nLife science, earth science, etc. lessons.  \\nTextbook QA \\n12 \\nAutomated form filling [21] \\nRequest forms for IT projects \\nEnterprise search \\n13 \\nFinancial sentiment analysis [22]  \\nTwitter financial news and FiQA datasets \\nSentiments classification \\n14 \\nFrontline health worker capacity building [23] \\nPregnancy-related guidelines \\nHealth education QA \\n15',\n",
       " 'Twitter financial news and FiQA datasets \\nSentiments classification \\n14 \\nFrontline health worker capacity building [23] \\nPregnancy-related guidelines \\nHealth education QA \\n15 \\nSelf-BioRAG: a framework for biomedical text [24] \\nBiomedical instruction sets \\nBiomedical Informatics \\n16 \\nHybrid RAG for real-time composition assistance [25] \\nWikiText-103, Enron Emails, etc.  \\nWriting speed and accuracy \\n17 \\nRAG-Fusion to obtain product information [26] \\nProduct datasheets \\nTechnical information QA \\n18 \\nCommit message generation for code intelligence [27]  \\nMCMD dataset  \\nSDM \\n19 \\nFloodBrain: Flood disaster reporting [28] \\nReliefWeb reports \\nHumanitarian assistance \\n20 \\nRich answer encoding [29] \\nMSMARCO QA and WoW dataset. \\nGenerative QA  \\n21 \\nText-to-image generator [30] \\nCOCO and WikiImages datasets. \\nRealistic images generation \\n22 \\nCode completion framework [31] \\nCodeXGLUE and CodeNet datasets. \\nSDM \\n23 \\nComplex story generation framework [32] \\nIMDB movie details dataset',\n",
       " 'Realistic images generation \\n22 \\nCode completion framework [31] \\nCodeXGLUE and CodeNet datasets. \\nSDM \\n23 \\nComplex story generation framework [32] \\nIMDB movie details dataset \\nGenerate stories  \\n24 \\nTRAC: Trustworthy retrieval augmented chatbot [33] \\nNatural Question dataset \\nNatural QA \\n25 \\nClinfo.ai using scientific literature [34] \\nPubMed dataset \\nMedical QA \\n26 \\nRealGen for controllable traffic scenarios [35] \\nnuScenes dataset \\nCritical traffic scenarios \\n27 \\nZero-shot disease phenotyping [36]  \\nClinical notes \\nIdentifying diseases \\n28 \\nRAP-Gen for automatic program repair [37]  \\nTFix, Defects4J, etc. datasets \\nSDM \\n29 \\nCode4UIE : retrieval-augmented code generation [38] \\nACE04, ACE05, CoNLL03, etc. datasets \\nInformation extraction  \\n30 \\nRAP: retrieval-augmented planning [39] \\nALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31 \\nRIGHT for mainstream hashtag recommendation [40] \\nTwitter and Weibo data.  \\nRetrieval-enhanced hashtags \\n32',\n",
       " 'ALFWorld, Webshop, etc. datasets \\nDecision-making  \\n31 \\nRIGHT for mainstream hashtag recommendation [40] \\nTwitter and Weibo data.  \\nRetrieval-enhanced hashtags \\n32 \\nRAUCG for counter narrative generation for hate speech \\n[41] \\nMultitargetCONAN dataset \\nCombating hate speech \\n33 \\nWeakly-supervised scientific document classification \\n[42] \\nAGNews and MeSH datasets.  \\nScientific documents \\nclassification \\n34 \\nrT5 for Chinese entity description generation [43] \\nXunZi and MengZi datasets.  \\nEntity description generation \\n35 \\nRSpell: domain adaptive Chinese spelling check [44]  \\nCSC dataset \\nText error correction \\n36 \\nXRICL: cross-lingual retrieval-augmented in-context \\nlearning for cross-lingual text-to-SQL semantic parsing \\n[45] \\nXSPIDER and XKAGGLE-DBQA datasets. \\nText-to-SQL translation \\n37 \\nSELF-RAG: learning to retrieve, generate, and \\ncritique through self-reflection [46] \\nOpen-Instruct processed data. \\nOpen-domain QA and fact \\nverification \\n38',\n",
       " 'Text-to-SQL translation \\n37 \\nSELF-RAG: learning to retrieve, generate, and \\ncritique through self-reflection [46] \\nOpen-Instruct processed data. \\nOpen-domain QA and fact \\nverification \\n38 \\nChatDOC with enhanced PDF structure recognition [47] \\nAcademic papers, financial reports, \\ntextbooks, and legislative materials \\nProfessional knowledge QA \\n39 \\nG-Retriever for textual graph understanding [48] \\nGraphQA (ExplaGraphs, SceneGraphs and \\nWebQSP)  \\nChat with graphs \\n40 \\nEnhancing multilingual information retrieval in \\nmixed Human Resources (HR) environments [49] \\nHR standard operating procedures and \\nQuality Assurance (QA) documents \\nMulticultural enterprise QA \\n41 \\nDifferentiable RAG [50] \\nUser-clicked logs \\nE-commerce search (query \\nintent classification) \\n42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data \\nSDM \\n43 \\nUniMS-RAG: a unified multi-source RAG [52] \\nDuLeMon and KBP datasets \\nPersonalized dialogue \\nsystems \\n44',\n",
       " '42 \\nRAG to elevate low-code developer skills [51] \\nCaspio and Power automate data \\nSDM \\n43 \\nUniMS-RAG: a unified multi-source RAG [52] \\nDuLeMon and KBP datasets \\nPersonalized dialogue \\nsystems \\n44 \\nRAG QA for event argument extraction [53] \\nACE 2005 and WikiEvent datasets \\nEvent argument (answer) \\nextraction \\n45 \\nFABULA: retrieval-augmented narrative construction \\n[54] \\nOntoNotes and Pile datasets \\nIntelligence report \\ngeneration \\n46 \\nTime-Aware Adaptive Retrieval (TA-ARE) [55] \\nRetrievalQA dataset \\nShort-form open-domain \\nQA \\n47 \\nCash transaction booking via RAG [56] \\nCash Management Software (CMS) \\ntransactions. \\nAutomated cash transaction \\nbooking \\n48 \\nRetrieval-Augmented Thought Process (RATP) [57] \\nBoolq and emrQA datasets. \\nQA with private data \\n49 \\nATLANTIC for interdisciplinary science [58] \\nS2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50 \\nWriting documents for clinical trials [59] \\nFDA guidance database, ClinicalTrials.gov, \\nand AACT database.',\n",
       " 'S2ORC dataset \\nScience QA and scientific \\ndocument classification  \\n50 \\nWriting documents for clinical trials [59] \\nFDA guidance database, ClinicalTrials.gov, \\nand AACT database.  \\nClinical-related writing \\n51 \\nQA RAG model [60] \\nFDA Q&A datasets  \\nPharma industry regulatory \\ncompliance QA',\n",
       " '3786\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n4. Discussion  \\nThe classification of RAG applications according to the specific NLP tasks they target holds significant \\nimportance for several reasons. Firstly, it offers valuable insights into the distribution and focus of RAG applications \\nacross various tasks within the field of NLP. By quantifying the number of studies dedicated to each task, \\nresearchers gain a deeper understanding of where efforts and resources are predominantly concentrated within the \\nRAG domain. By analyzing the distribution of RAG applications, researchers can discern prevailing trends in \\nresearch interest and identify emerging areas of importance. The classification of RAG applications based on \\ndiscipline offers valuable insights into its widespread adoption across various domains. This classification not only \\nprovides a comprehensive understanding of RAG’s applicability but also underscores its potential to revolutionize',\n",
       " 'provides a comprehensive understanding of RAG’s applicability but also underscores its potential to revolutionize \\nvarious domains, thereby contributing significantly to the advancement of NLP technologies.  \\nWhile this survey offers a comprehensive overview of RAG applications across various NLP tasks and \\ndisciplines, it also has its limitations. 1) Given that RAG technology is still emerging, the majority of RAG-based \\nstudies are available in pre-print formats on platforms like arXiv, lacking peer review. This raises questions about \\ntheir authenticity. 2) Additionally, the survey overlooks the technical implementation details and challenges \\nassociated with using RAG technology alongside open-source LLMs. Organizations may find RAG implementation \\ncostly if they do not opt for open-source LLM architectures, especially considering the expense of querying the \\nLLM via Application Programming Interface (API). 3) Furthermore, the performance of RAG concerning the',\n",
       " 'LLM via Application Programming Interface (API). 3) Furthermore, the performance of RAG concerning the \\nvolume and variety of datasets has not been discussed. Deploying RAG with large datasets of varying structures \\n(e.g., structured, semi-structured, or non-structured) may lead to processing delays, warranting further exploration \\nbefore selecting a RAG with LLM integrated solution for organizational deployment.  \\n4) Additionally, this survey did not cover the diverse range of RAG architectures and technologies available for \\nintegration with different LLMs. Future work should delve into these options to discuss how various RAG solutions \\ncan be adapted with LLMs for different NLP tasks and applications. 5) Furthermore, the survey did not address the \\naccuracy of information obtained from RAG with LLM solutions. It is essential to explore the reliability of these \\nsystems and assess the organizations’ dependency on their generated responses. LLMs often generate responses with',\n",
       " 'systems and assess the organizations’ dependency on their generated responses. LLMs often generate responses with \\nhigh confidence, making it challenging to evaluate the accuracy of the information provided. 6) While the survey \\nprimarily focuses on task-based and discipline-based applications of RAG, there is a need for further research to \\nexplore ethical considerations associated with its usage, especially when dealing with sensitive datasets. For \\nexample, in the biomedical domain, RAG has the potential to accidentally expose private information to analysts, \\nraising concerns about data privacy and security. Additionally, in the legal domain, RAG may mistakeably reveal \\nprivileged information during document analysis, potentially violating client confidentiality and attorney-client \\nprivilege. Therefore, future studies should delve deeper into these ethical implications to ensure responsible and \\nethical use of RAG technology across various domains. \\n5. Conclusion',\n",
       " 'privilege. Therefore, future studies should delve deeper into these ethical implications to ensure responsible and \\nethical use of RAG technology across various domains. \\n5. Conclusion  \\nThis article offers a thorough examination of the applications of RAG with LLMs, showcasing their potential to \\ndrive digital transformation across diverse industries. Initially, it gathers the latest publications on RAG from online \\nrepositories. These publications are then classified based on task-oriented and discipline-oriented criteria. A notable \\ntrend observed is the increasing number of research papers on RAG deposited in open-access sources, particularly \\nsince 2023. However, many works remain unpublished or are in the preprint stage, awaiting review by various \\njournals. A significant portion of these studies primarily focus on the task of QA in NLP. Conversely, there is a \\nnoticeable gap in research exploring Entity Linking, an essential NLP task that contributes to knowledge graph',\n",
       " 'noticeable gap in research exploring Entity Linking, an essential NLP task that contributes to knowledge graph \\ndevelopment. Addressing this gap could unlock numerous applications in the realm of linked data. Regarding \\ndisciplines, the majority of research applications are concentrated in the fields of Medical/Biomedical and \\nTechnology and Software Development. In contrast, disciplines such as Business and Agriculture receive \\ncomparatively less attention. Future research endeavors should aim to bridge this gap by addressing the specific \\nneeds of these underrepresented disciplines.',\n",
       " 'Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3787\\nTable 2. Task-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 4. Task-based classification of RAG applications with count of publications. The word cloud is generated based on the publication counts \\nlisted under various headings in Table 2. \\n \\n \\n \\n \\n1) Question Answering (QA)  \\n- Biomedical QA [1] \\n- Financial QA [2] \\n- Medical QA [3] \\n- Commonsense QA [6] \\n- Textbook QA [11] \\n- Health education QA [14] \\n- Technical product information QA [17] \\n- Natural QA [24] \\n- Professional knowledge QA [38] \\n- Multicultural enterprise QA [40] \\n- Open-domain QA and fact verification [46] \\n- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29] \\n- Pharma industry regulatory compliance QA [51]',\n",
       " '- Open-domain QA and fact verification [46] \\n- Short-form open-domain QA [46] \\n- Generative QA and informative conversations [29] \\n- Pharma industry regulatory compliance QA [51] \\n- Science QA and document classification [49] \\n- Clinical-related writing [50] \\n- Personalized dialogue systems [43] \\n2) Text Generation and Summarization \\n- Medical text summarization [4] \\n- Book review generation [5] \\n- Biomedical Informatics [15] \\n- Generate stories with complex plots [23] \\n- Generate realistic and faithful images [21] \\n- Entity description generation [34] \\n \\n3) Information Retrieval and Extraction \\n- Table QA [7] \\n- Enterprise search [12] \\n- Retrieval-enhanced hashtags [31] \\n- Information extraction [29] \\n- Event argument (answer) extraction [44] \\n- E-commerce search (query intent classification) [41] \\n4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Scientific documents classification [33]',\n",
       " '4) Text Analysis and Processing \\n- Sentiments classification [13] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Scientific documents classification [33] \\n- Combating online hate speech [32] \\n \\n5) Software Development and \\nMaintenance \\n- Code intelligence [18] \\n- Code completion [22] \\n- Automatic program repair [28] \\n- Elevate low-code developer skills [42] \\n \\n6) Decision Making and Applications \\n- Clinical decision-making [9] \\n- Educational decision making [10] \\n- Decision-making applications [30] \\n- Automated cash transaction booking [47] \\n- Intelligence report generation [45] \\n \\n7) Other Categories: \\n- Editing and crafting diverse behaviors, \\nincluding critical traffic scenarios [26] \\n- Identifying diseases [27] \\n- Chat with graphs [39] \\nTask: (Count of Publications)  \\nQuestion Answering (QA): (20) \\nText Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6) \\nText Analysis and Processing: (5) \\nSoftware Development and Maintenance: (4)',\n",
       " 'Question Answering (QA): (20) \\nText Generation and Summarization: (6) \\nInformation Retrieval and Extraction: (6) \\nText Analysis and Processing: (5) \\nSoftware Development and Maintenance: (4) \\nDecision Making and Applications: (5) \\nOther Categories: (6)',\n",
       " '3788\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\nTable 3. Discipline-based classification of RAG applications. The detailed categories are derived from the \"Application area\" \\ncolumn of Table 1. These categories are assigned based on a thorough comprehension of the study’s context. \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFig. 5. Discipline-based classification of RAG applications with count of publications. The word cloud is generated based on the publication \\ncounts listed under various headings in Table 3. \\nAcknowledgements \\nThe authors thank the French Government and the National Research Agency (ANR) for their funding. \\nReferences \\n[1] Roumeliotis KI, Tselikas ND, & Nasiopoulos DK. (2024). “LLMs in e-commerce: a comparative analysis of GPT and LLaMA models in \\nproduct review evaluation,” Natural Language Processing Journal:1-6:100056.',\n",
       " 'product review evaluation,” Natural Language Processing Journal:1-6:100056. \\n[2] Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). “Language Models are Few-Shot \\nLearners,” Advances in Neural Information Processing Systems 33 (NeurIPS 2020).  \\n[3] OpenAI, R. (2023). “Gpt-4 technical report,” arxiv 2303.08774. View in Article: 2(5). \\n1) Medical / Biomedical \\n- Biomedical QA [1] \\n- Medical QA [3] \\n- Medical text summarization [4] \\n- Health education QA [14] \\n- Identifying diseases [27] \\n- Clinical decision-making [9] \\n- Clinical-related writing [50] \\n- Science QA and scientific document classification [49] \\n- Pharma industry regulatory compliance QA [51] \\n2) Financial \\n- Financial QA [2] \\n- Automated cash transaction booking [47] \\n3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11] \\n4) Technology and Software Development \\n- Table QA [7] \\n- Technical product information QA [17]',\n",
       " '3) Educational \\n- Educational decision making [10] \\n- Textbook QA [11] \\n4) Technology and Software Development \\n- Table QA [7] \\n- Technical product information QA [17] \\n- Software development and maintenance [18, 22, 28, 42] \\n- Generative QA and informative conversations [20] \\n- Information extraction [29] \\n- Text error correction [35] \\n- Text-to-SQL translation [36] \\n- Personalized dialogue systems [43] \\n- Event argument (answer) extraction [44] \\n5) Social and Communication \\n- Commonsense QA [6] \\n- Sentiments classification [13] \\n- Combating online hate speech [32] \\n- Retrieval-enhanced hashtags [31] \\n- Humanitarian assistance [19] \\n- Chat with graphs [39] \\n- Multicultural enterprise QA [40] \\n6) Literature  \\n- Book review generation guided by reference documents [5] \\n- Enhance user writing speed and accuracy [16] \\n- Generate stories with complex plots [23] \\n7) Other Categories   \\n- Enterprise search [12] \\n- Generate realistic and faithful images [21]',\n",
       " '- Enhance user writing speed and accuracy [16] \\n- Generate stories with complex plots [23] \\n7) Other Categories   \\n- Enterprise search [12] \\n- Generate realistic and faithful images [21] \\n- Decision-making applications [30] \\n- Open-domain question answering and fact verification [37] \\n- Professional knowledge QA [38] \\n- Intelligence report generation [45] \\n- Short-form open-domain QA [46] \\n- Question answering with private data [48] \\n \\nDiscipline: (Count of Publications)  \\nMedical / Biomedical: (9) \\nFinancial: (2) \\nEducational: (2) \\nTechnology and Software Development: (9) \\nSocial and Communication: (7) \\nLiterature (3) \\nOther Categories: (8)',\n",
       " 'Muhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\x08\\n3789\\n[4] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., ... & Wang, H. (2023). “Retrieval-augmented generation for large language models: A \\nsurvey,” arXiv preprint arXiv:2312.10997. \\n[5] Kandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2023). “Large language models struggle to learn long-tail knowledge,” In \\nInternational Conference on Machine Learning. PMLR: 5696-15707. \\n[6] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Kiela, D. (2020). “Retrieval-augmented generation for knowledge-\\nintensive nlp tasks,” Advances in Neural Information Processing Systems 33: 9459-9474. \\n[7] Li, H., Su, Y., Cai, D., Wang, Y., & Liu, L. (2022). “A survey on retrieval-augmented text generation,” arXiv preprint arXiv:2202.01110. \\n[8] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., ... & Scialom, T. (2023). “Augmented language models: a',\n",
       " '[8] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., ... & Scialom, T. (2023). “Augmented language models: a \\nsurvey,” arXiv preprint arXiv:2302.07842. \\n[9] Zhao, R., Chen, H., Wang, W., Jiao, F., Do, X. L., Qin, C., ... & Joty, S. (2023). “Retrieving multimodal information for augmented \\ngeneration: A survey,” arXiv preprint arXiv:2303.10868. \\n[10] Xiong, G., Jin, Q., Lu, Z., & Zhang, A. (2024). “Benchmarking retrieval-augmented generation for medicine,” arXiv preprint \\narXiv:2402.13178. \\n[11] Jimeno Yepes, A., You, Y., Milczek, J., Laverde, S., & Li, L. (2024). “Financial Report Chunking for Effective Retrieval Augmented \\nGeneration,” arXiv e-prints, arXiv-2402. \\n[12] Yu, H., Guo, P., & Sano, A. (2023). “Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation,” \\nIn Machine Learning for Health (ML4H) PMLR: 650-663.',\n",
       " '[12] Yu, H., Guo, P., & Sano, A. (2023). “Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation,” \\nIn Machine Learning for Health (ML4H) PMLR: 650-663. \\n[13] Manathunga, S. S., & Illangasekara, Y. A. (2023). “Retrieval Augmented Generation and Representative Vector Summarization for large \\nunstructured textual data in Medical Education,” arXiv preprint arXiv:2308.00479. \\n[14] Kim, J., Choi, S., Amplayo, R. K., & Hwang, S. W. (2020). “Retrieval-augmented controllable review generation,” In Proceedings of the \\n28th International Conference on Computational Linguistics: 2284-2295. \\n[15] Sha, Y., Feng, Y., He, M., Liu, S., & Ji, Y. (2023). “Retrieval-augmented Knowledge Graph Reasoning for Commonsense Question \\nAnswering,” Mathematics 11(15): 3269; https://doi.org/10.3390/math11153269.  \\n[16] Pan, F., Canim, M., Glass, M., Gliozzo, A., & Hendler, J. (2022). “End-to-End Table Question Answering via Retrieval-Augmented',\n",
       " '[16] Pan, F., Canim, M., Glass, M., Gliozzo, A., & Hendler, J. (2022). “End-to-End Table Question Answering via Retrieval-Augmented \\nGeneration,” arXiv preprint arXiv:2203.16714. \\n[17] Ge, J., Sun, S., Owens, J., Galvez, V., Gologorskaya, O., Lai, J. C., ... & Lai, K. (2023). “Development of a Liver Disease-Specific Large \\nLanguage Model Chat Interface using Retrieval Augmented Generation,” medRxiv. \\n[18] Zakka, C., Shad, R., Chaurasia, A., Dalal, A. R., Kim, J. L., Moor, M., ... & Hiesinger, W. (2024). “Almanac—retrieval-augmented language \\nmodels for clinical medicine,” NEJM AI 1(2), AIoa2300068. \\n[19] Han, Z. FeiFei, Lin, J., Gurung, A., Thomas, D. R., Chen, E., Borchers, C., Gupta, S., & Koedinger, K. R. (2024). “Improving Assessment of \\nTutoring Practices using Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.14594. \\n[20] Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A., & Jamal, A. (2024). “Enhancing Textbook Question Answering Task with',\n",
       " \"[20] Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A., & Jamal, A. (2024). “Enhancing Textbook Question Answering Task with \\nLarge Language Models and Retrieval Augmented Generation,” arXiv preprint arXiv:2402.05128. \\n[21] Bucur, M. (2023). “Exploring Large Language Models and Retrieval Augmented Generation for Automated Form Filling,” (Bachelor's \\nthesis, University of Twente). \\n[22] Zhang, B., Yang, H., Zhou, T., Ali Babar, M., & Liu, X. Y. (2023). “Enhancing financial sentiment analysis via retrieval augmented large \\nlanguage models,” In Proceedings of the Fourth ACM International Conference on AI in Finance: 349-356. \\n[23] Al Ghadban, Y., Lu, H. Y., Adavi, U., Sharma, A., Gara, S., Das, N., ... & Hirst, J. E. (2023). “Transforming healthcare education: \\nHarnessing large language models for frontline health worker capacity building using retrieval-augmented generation,” medRxiv, 2023-12.\",\n",
       " 'Harnessing large language models for frontline health worker capacity building using retrieval-augmented generation,” medRxiv, 2023-12. \\n[24] Jeong, M., Sohn, J., Sung, M., & Kang, J. (2024). “Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-\\nAugmented Large Language Models,” arXiv preprint arXiv:2401.15269. \\n[25] Xia, M., Zhang, X., Couturier, C., Zheng, G., Rajmohan, S., & Ruhle, V. (2023). “Hybrid retrieval-augmented generation for real-time \\ncomposition assistance,” arXiv preprint arXiv:2308.04215. \\n[26] Rackauckas, Z. (2024). “RAG-Fusion: A New Take on Retrieval-Augmented Generation,” arXiv preprint arXiv:2402.03367. \\n[27] Shi, E., Wang, Y., Tao, W., Du, L., Zhang, H., Han, S., ... & Sun, H. (2022). “RACE: Retrieval-Augmented Commit Message \\nGeneration,” arXiv preprint arXiv:2203.02700. \\n[28] Colverd, G., Darm, P., Silverberg, L., & Kasmanoff, N. (2023). “FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented',\n",
       " 'Generation,” arXiv preprint arXiv:2203.02700. \\n[28] Colverd, G., Darm, P., Silverberg, L., & Kasmanoff, N. (2023). “FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented \\nGeneration with an LLM,” arXiv preprint arXiv:2311.02597. \\n[29] Huang, W., Lapata, M., Vougiouklis, P., Papasarantopoulos, N., & Pan, J. (2023). “Retrieval Augmented Generation with Rich Answer \\nEncoding,” In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-\\nPacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers): 1012-1025. \\n[30] Chen, W., Hu, H., Saharia, C., & Cohen, W. W. (2022). “Re-imagen: Retrieval-augmented text-to-image generator,” arXiv preprint \\narXiv:2209.14491. \\n[31] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S. W., & Svyatkovskiy, A. (2022). “Reacc: A retrieval-augmented code completion \\nframework,” arXiv preprint arXiv:2203.07722.',\n",
       " 'arXiv:2209.14491. \\n[31] Lu, S., Duan, N., Han, H., Guo, D., Hwang, S. W., & Svyatkovskiy, A. (2022). “Reacc: A retrieval-augmented code completion \\nframework,” arXiv preprint arXiv:2203.07722. \\n[32] Wen, Z., Tian, Z., Wu, W., Yang, Y., Shi, Y., Huang, Z., & Li, D. (2023). “Grove: a retrieval-augmented complex story generation \\nframework with a forest of evidence,” arXiv preprint arXiv:2310.05388.',\n",
       " '3790\\t\\nMuhammad Arslan  et al. / Procedia Computer Science 246 (2024) 3781–3790\\n[33] Li, S., Park, S., Lee, I., & Bastani, O. (2023). “TRAC: Trustworthy Retrieval Augmented Chatbot,” arXiv preprint arXiv:2307.04642. \\n[34] Lozano, A., Fleming, S. L., Chiang, C. C., & Shah, N. (2023). “Clinfo. ai: An open-source retrieval-augmented large language model system \\nfor answering medical questions using scientific literature,” In Pacific symposium on Biocomputing 2024: 8-23. \\n[35] Ding, W., Cao, Y., Zhao, D., Xiao, C., & Pavone, M. (2023). “RealGen: Retrieval Augmented Generation for Controllable Traffic \\nScenarios,” arXiv preprint arXiv:2312.13303. \\n[36] Thompson, W. E., Vidmar, D. M., De Freitas, J. K., Pfeifer, J. M., Fornwalt, B. K., Chen, R., ... & Miotto, R. (2023). “Large Language \\nModels with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,” arXiv preprint arXiv:2312.06457.',\n",
       " 'Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping,” arXiv preprint arXiv:2312.06457. \\n[37] Wang, W., Wang, Y., Joty, S., & Hoi, S. C. (2023). “Rap-gen: Retrieval-augmented patch generation with codet5 for automatic program \\nrepair,” In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software \\nEngineering: 146-158. \\n[38] Guo, Y., Li, Z., Jin, X., Liu, Y., Zeng, Y., Liu, W., ... & Cheng, X. (2023). “Retrieval-augmented code generation for universal information \\nextraction,” arXiv preprint arXiv:2311.02962. \\n[39] Kagaya, T., Yuan, T. J., Lou, Y., Karlekar, J., Pranata, S., Kinose, A., ... & You, Y. (2024). “RAP: Retrieval-Augmented Planning with \\nContextual Memory for Multimodal LLM Agents,” arXiv preprint arXiv:2402.03610. \\n[40] Fan, R. Z., Fan, Y., Chen, J., Guo, J., Zhang, R., & Cheng, X. (2023). “RIGHT: Retrieval-augmented Generation for Mainstream Hashtag',\n",
       " '[40] Fan, R. Z., Fan, Y., Chen, J., Guo, J., Zhang, R., & Cheng, X. (2023). “RIGHT: Retrieval-augmented Generation for Mainstream Hashtag \\nRecommendation,” arXiv preprint arXiv:2312.10466. \\n[41] Jiang, S., Tang, W., Chen, X., Tanga, R., Wang, H., & Wang, W. (2023). Raucg: Retrieval-augmented unsupervised counter narrative \\ngeneration for hate speech. arXiv preprint arXiv:2310.05650. \\n[42] Xu, R., Yu, Y., Ho, J., & Yang, C. (2023). “Weakly-supervised scientific document classification via retrieval-augmented multi-stage \\ntraining,” In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval: 2501-\\n2505. \\n[43] Hu, M., Zhao, X., Wei, J., Wu, J., Sun, X., Li, Z., ... & Zhang, Y. (2023). “rT5: A Retrieval-Augmented Pre-trained Model for Ancient \\nChinese Entity Description Generation,” In International Conference on NLP and Chinese Computing. Cham: Springer: 736-748.',\n",
       " 'Chinese Entity Description Generation,” In International Conference on NLP and Chinese Computing. Cham: Springer: 736-748. \\n[44] Song, S., Lv, Q., Geng, L., Cao, Z., & Fu, G. (2023). “RSpell: Retrieval-augmented Framework for Domain Adaptive Chinese Spelling \\nCheck,” In CCF International Conference on Natural Language Processing and Chinese Computing. Cham: Springer: 551-562.  \\n[45] Shi, P., Zhang, R., Bai, H., & Lin, J. (2022). “Xricl: Cross-lingual retrieval-augmented in-context learning for cross-lingual text-to-sql \\nsemantic parsing,” arXiv preprint arXiv:2210.13693. \\n[46] Asai, A., Wu, Z., Wang, Y., Sil, A., & Hajishirzi, H. (2023). “Self-rag: Learning to retrieve, generate, and critique through self-\\nreflection,” arXiv preprint arXiv:2310.11511. \\n[47] Lin, D. (2024). “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,” arXiv preprint \\narXiv:2401.12599.',\n",
       " 'reflection,” arXiv preprint arXiv:2310.11511. \\n[47] Lin, D. (2024). “Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition,” arXiv preprint \\narXiv:2401.12599. \\n[48] He, X., Tian, Y., Sun, Y., Chawla, N. V., Laurent, T., LeCun, Y., ... & Hooi, B. (2024). “G-Retriever: Retrieval-Augmented Generation for \\nTextual Graph Understanding and Question Answering,” arXiv preprint arXiv:2402.07630. \\n[49] Ahmad, S. R. (2024). “Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model \\nImplementation for Multicultural Enterprise,” arXiv preprint arXiv:2401.01511. \\n[50] Zhao, C., Jiang, Y., Qiu, Y., Zhang, H., & Yang, W. Y. (2023). “Differentiable Retrieval Augmentation via Generative Language Modeling \\nfor E-commerce Query Intent Classification,” In Proceedings of the 32nd ACM International Conference on Information and Knowledge \\nManagement: 4445-4449.',\n",
       " 'for E-commerce Query Intent Classification,” In Proceedings of the 32nd ACM International Conference on Information and Knowledge \\nManagement: 4445-4449. \\n[51] Nakhod, o. Using retrieval-augmented generation to elevate low-code developer skills. https://doi.org/10.15407/jai2023.03.126 \\n[52] Wang, H., Huang, W., Deng, Y., Wang, R., Wang, Z., Wang, Y., ... & Wong, K. F. (2024). “UniMS-RAG: A Unified Multi-source \\nRetrieval-Augmented Generation for Personalized Dialogue Systems,” arXiv preprint arXiv:2401.13256. \\n[53] Du, X., & Ji, H. (2022). “Retrieval-augmented generative question answering for event argument extraction,” arXiv preprint \\narXiv:2211.07067. \\n[54] Ranade, P., & Joshi, A. (2023). “FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction,” arXiv \\npreprint arXiv:2310.13848. \\n[55] Zhang, Z., Fang, M., & Chen, L. (2024). “RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain',\n",
       " 'preprint arXiv:2310.13848. \\n[55] Zhang, Z., Fang, M., & Chen, L. (2024). “RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain \\nQuestion Answering,” arXiv preprint arXiv:2402.16457. \\n[56] Zhang, S., Yadav, D., & Jin, T. (2023). “Cash transaction booking via retrieval augmented LLM. KDD 2023 Workshop on Robust NLP for \\nFinance (RobustFin),” https://www.amazon.science/publications/cash-transaction-booking-via-retrieval-augmented-llm \\n[57] Pouplin, T., Sun, H., Holt, S., & Van der Schaar, M. (2024). “Retrieval-Augmented Thought Process as Sequential Decision Making,” arXiv \\npreprint arXiv:2402.07812. \\n[58] Munikoti, S., Acharya, A., Wagle, S., & Horawalavithana, S. (2023). “ATLANTIC: Structure-Aware Retrieval-Augmented Language Model \\nfor Interdisciplinary Science,” arXiv preprint arXiv:2311.12289. \\n[59] Markey, N., El-Mansouri, I., Rensonnet, G., van Langen, C., & Meier, C. (2024). “From RAGs to riches: Using large language models to',\n",
       " '[59] Markey, N., El-Mansouri, I., Rensonnet, G., van Langen, C., & Meier, C. (2024). “From RAGs to riches: Using large language models to \\nwrite documents for clinical trials,” arXiv preprint arXiv:2402.16406. \\n[60] Kim, J., & Min, M. (2024). “From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical Regulatory Compliance Process,” arXiv \\npreprint arXiv:2402.01717.',\n",
       " 'Sample PDF\\nThis is a simple PDF ﬁle. Fun fun fun.\\nLorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. \\nCurabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget \\npharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. \\nInteger a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. \\nVestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla \\nerat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. \\nVivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique \\naccumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, \\nhendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. \\nSuspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \\npulvinar quis, nisl.',\n",
       " 'Suspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \\npulvinar quis, nisl.\\nPellentesque sit amet lectus. Praesent pulvinar, nunc quis iaculis sagittis, justo quam \\nlobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. \\nPhasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh \\nante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis \\nsem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin \\nvelit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, \\nfeugiat eu, gravida eu, consequat molestie, ipsum. Nullam vel est ut ipsum volutpat \\nfeugiat. Aenean pellentesque.\\nIn mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, \\nscelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat',\n",
       " 'In mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, \\nscelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat \\nvarius. Donec lacinia, neque a luctus aliquet, pede massa imperdiet ante, at varius lorem \\npede sed sapien. Fusce erat nibh, aliquet in, eleifend eget, commodo eget, erat. Fusce \\nconsectetuer. Cras risus tortor, porttitor nec, tristique sed, convallis semper, eros. Fusce \\nvulputate ipsum a mauris. Phasellus mollis. Curabitur sed urna. Aliquam nec sapien non \\nnibh pulvinar convallis. Vivamus facilisis augue quis quam. Proin cursus aliquet metus. \\nSuspendisse lacinia. Nulla at tellus ac turpis eleifend scelerisque. Maecenas a pede vitae \\nenim commodo interdum. Donec odio. Sed sollicitudin dui vitae justo.\\nMorbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus \\nmolestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque',\n",
       " 'Morbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus \\nmolestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque \\npenatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit \\namet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa \\neget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, \\nquam.',\n",
       " 'Towards Agentic RAG with Deep Reasoning:\\nA Survey of RAG-Reasoning Systems in LLMs\\nYangning Li1*, Weizhi Zhang2*, Yuyao Yang2, Wei-Chieh Huang2, Yaozu Wu3\\nJunyu Luo4, Yuanchen Bei5, Henry Peng Zou2, Xiao Luo6, Yusheng Zhao4\\nChunkit Chan7, Yankai Chen2, Zhongfen Deng2, Yinghui Li1, Hai-Tao Zheng1,\\nDongyuan Li3, Renhe Jiang3, Ming Zhang4, Yangqiu Song7, Philip S. Yu1\\n1Tsinghua University 2University of Illinois Chicago 3The University of Tokyo\\n4Peking University 5University of Illinois Urbana-Champaign\\n6University of California, Los Angeles 7HKUST\\nynli23@mails.tsinghua.edu.cn, wzhan42@uic.edu\\nAbstract\\nRetrieval-Augmented Generation (RAG) lifts\\nthe factuality of Large Language Models\\n(LLMs) by injecting external knowledge, yet\\nit falls short on problems that demand multi-\\nstep inference; conversely, purely reasoning-\\noriented approaches often hallucinate or mis-\\nground facts. This survey synthesizes both\\nstrands under a unified reasoning-retrieval per-',\n",
       " 'step inference; conversely, purely reasoning-\\noriented approaches often hallucinate or mis-\\nground facts. This survey synthesizes both\\nstrands under a unified reasoning-retrieval per-\\nspective. We first map how advanced reason-\\ning optimizes each stage of RAG (Reasoning-\\nEnhanced RAG). Then, we show how re-\\ntrieved knowledge of different type supply\\nmissing premises and expand context for\\ncomplex inference (RAG-Enhanced Reason-\\ning).\\nFinally, we spotlight emerging Syn-\\nergized RAG-Reasoning frameworks, where\\n(agentic) LLMs iteratively interleave search\\nand reasoning to achieve state-of-the-art per-\\nformance across knowledge-intensive bench-\\nmarks.\\nWe categorize methods, datasets,\\nand open challenges, and outline research av-\\nenues toward deeper RAG-Reasoning systems\\nthat are more effective, multimodally-adaptive,\\ntrustworthy, and human-centric. The collec-\\ntion is available at https://github.com/\\nDavidZWZ/Awesome-RAG-Reasoning.\\n1\\nIntroduction',\n",
       " 'that are more effective, multimodally-adaptive,\\ntrustworthy, and human-centric. The collec-\\ntion is available at https://github.com/\\nDavidZWZ/Awesome-RAG-Reasoning.\\n1\\nIntroduction\\nThe remarkable progress in Large Language Mod-\\nels (LLMs) has transformed a wide array of fields,\\nshowcasing unprecedented capabilities across di-\\nverse tasks (Zhao et al., 2023). Despite these ad-\\nvancements, the effectiveness of LLMs remains\\nhindered by two fundamental limitations: knowl-\\nedge hallucinations, due to the static and parametric\\nmanner of their knowledge storage (Huang et al.,\\n2025b); and struggles with complex reasoning, es-\\npecially when tackling real-world problems (Chang\\net al., 2024). These limitations have driven the\\n* Equal Contribution.\\ndevelopment of two major directions: Retrieval-\\nAugmented Generation (RAG) (Fan et al., 2024a),\\nwhich provides LLMs with external knowledge;\\nand various methods aimed at enhancing their in-\\nherent reasoning abilities (Chen et al., 2025c).',\n",
       " 'Augmented Generation (RAG) (Fan et al., 2024a),\\nwhich provides LLMs with external knowledge;\\nand various methods aimed at enhancing their in-\\nherent reasoning abilities (Chen et al., 2025c).\\nThe two limitations are inherently intertwined:\\nmissing knowledge can impede reasoning, and\\nflawed reasoning hinders knowledge utilization\\n(Tonmoy et al., 2024). Naturally, researchers have\\nincreasingly explored combining retrieval with rea-\\nsoning, though early work followed two separate,\\none-way enhancements.\\nThe first, Reasoning-\\nenhanced RAG (Gao et al., 2023b) (Reasoning\\n→RAG), leverages reasoning to improve specific\\nstages of the RAG pipeline. The second path, RAG-\\nenhanced Reasoning (Fan et al., 2024a) (RAG →\\nReasoning), supplies external factual grounding or\\ncontextual cues to bolster LLM reasoning.\\nWhile beneficial, the above methods remain\\nbound to a static Retrieval-Then-Reasoning (RTR)\\nframework, offering only localized improvements\\nto individual components. Several inherent limi-',\n",
       " 'While beneficial, the above methods remain\\nbound to a static Retrieval-Then-Reasoning (RTR)\\nframework, offering only localized improvements\\nto individual components. Several inherent limi-\\ntations persist: (1) Retrieval Adequacy and Accu-\\nracy cannot be guaranteed; Pre-retrieved knowl-\\nedge may fail to align with the actual knowledge\\nneeds that emerge during reasoning, especially in\\ncomplex tasks (Zheng et al., 2025; Li et al., 2025d).\\n(2) Reasoning Depth remains constrained. When\\nretrieved knowledge contains errors or conflicts,\\nit can adversely interfere with the model’s inher-\\nent reasoning capabilities (Li et al., 2025b; Chen\\net al., 2025a). (3) System Adaptability proves in-\\nsufficient. The RTR framework lacks mechanisms\\nfor iterative feedback or dynamic retrieval during\\nreasoning. This rigidity limits its effectiveness in\\nscenarios that require adaptive reasoning, such as\\nopen-domain QA or scientific discovery (Xiong\\net al., 2025; Alzubi et al., 2025).',\n",
       " 'reasoning. This rigidity limits its effectiveness in\\nscenarios that require adaptive reasoning, such as\\nopen-domain QA or scientific discovery (Xiong\\net al., 2025; Alzubi et al., 2025).\\nAs shown in Figure 1, these shortcomings have\\ncatalyzed a paradigm shift toward Synergized Re-\\n1\\narXiv:2507.09477v2  [cs.CL]  16 Jul 2025',\n",
       " 'Query\\nKnowledge\\nSource\\nDense/Sparse Retriever\\nRetrieval\\nIntegration\\nRe-rank \\n& Filter\\nData Fusion\\nGeneration\\nRetrieval Optimization\\n§3 Reasoning Enhanced RAG (Reasoning → RAG) \\nReasoning Enhanced\\n§4 RAG Enhanced Reasoning (RAG → Reasoning) \\nKnowledge Base\\nWeb Retrieval\\nTool Using\\nPrior Experience\\nExample & Training Data\\n§5 Synergized RAG and Reasoning (RAG ⇔ Reasoning) \\n§5.1 Reasoning Workflow\\n(a) Chain Based\\n(b) Tree Based\\n(c) Graph Based\\n§5.2 Agent Orchestration\\n(a) Single Agent\\nRAG Pipeline\\nLLM Reasoning\\nKnowledge 1: he is a South African \\npilot, sailor, consultant, who partly …\\nLet‘s Think Step by Step! …….\\nFind the solution!\\nRAG Enhanced\\n🎯Target: Accurate, and \\nReasoning-aware Retrieval\\n• Retrieve info tailored for \\nreasoning\\n• Filter and fuse evidence \\nlogically\\n• Faithful, logic-grounded output\\n🎯Target: Deeper, and \\nGrounded Reasoning\\n• Grounded by external \\nknowledge\\n• Verified by tools and evidence\\n• Guided by memory or \\nexamples\\n☹Drawback: \\nIrrelevant Knowledge',\n",
       " '🎯Target: Deeper, and \\nGrounded Reasoning\\n• Grounded by external \\nknowledge\\n• Verified by tools and evidence\\n• Guided by memory or \\nexamples\\n☹Drawback: \\nIrrelevant Knowledge \\ndisrupts reasoning accuracy.\\nQuery\\n❌\\nOutput\\n…\\nAction\\nObservation\\nAgent\\nRAG\\n(b) Decentralized \\nMulti Agent\\n(c) Centralized \\nMulti Agent\\nReasoning\\nRAG\\nQuery\\n❌\\n❓\\n✅\\nIntegration Enhancement\\nGeneration\\nEnhancement\\n☹Drawback: \\nShallow Reasoning leads to \\ninaccurate retrieval.\\nAgent 1\\nAgent 2\\nAgent 3\\nAgent 4\\nRAG\\nManager\\nAgent\\nWork Agent 1\\nWork Agent 2\\nWork Agent 3\\nRAG\\nQuery\\nOutput\\n…\\nFigure 1: Overview of the RAG-Reasoning System. The Reasoning-Enhanced RAG methods and RAG-Enhanced\\nReasoning methods represent one-way enhancements. In contrast, the Synergized RAG-Reasoning System performs\\nreasoning and retrieval iteratively, enabling mutual enhancements.\\ntrieval and Reasoning within LLMs (RAG ⇔\\nReasoning). These methods support a dynamic,\\niterative interplay where reasoning actively guides',\n",
       " 'trieval and Reasoning within LLMs (RAG ⇔\\nReasoning). These methods support a dynamic,\\niterative interplay where reasoning actively guides\\nretrieval, and newly retrieved knowledge, in turn,\\ncontinuously refines the reasoning process. This\\ntrend is further exemplified by recent ”Deep Re-\\nsearch” products from OpenAI1, Gemini2, Perplex-\\nity3, and others, which emphasize tightly coupled\\nretrieval and reasoning (Zhang et al., 2025f). These\\nsystems employ agentic capabilities to orchestrate\\nmulti-step web search and leverage reasoning to\\ncomprehensively interpret retrieved content, solv-\\ning problems demanding in-depth investigation.\\nThis survey charts the shift from isolated en-\\nhancements to cutting-edge synergized frameworks\\nwhere retrieval and reasoning are deeply interwo-\\nven and co-evolve. While surveys on RAG (Fan\\net al., 2024a; Gao et al., 2023b) and LLM Reason-\\ning (Chen et al., 2025c; Li et al., 2025e) exist, a\\ndedicated synthesis focusing on their integration',\n",
       " 'ven and co-evolve. While surveys on RAG (Fan\\net al., 2024a; Gao et al., 2023b) and LLM Reason-\\ning (Chen et al., 2025c; Li et al., 2025e) exist, a\\ndedicated synthesis focusing on their integration\\nremains lacking. Our goal is to provide a compre-\\nhensive overview of how the symbiosis between\\nretrieval and reasoning is advancing LLM capabili-\\nties, with particular emphasis on the move towards\\na synergized RAG and Reasoning framework.\\nThe survey is structured as follows: Section 2\\nintroduces the background; Section 3 and 4 review\\ntwo one-way enhancements, respectively. Section 5\\n1https://openai.com/index/\\nintroducing-deep-research/\\n2https://gemini.google/overview/\\ndeep-research/\\n3https://www.perplexity.ai/hub/blog/\\nintroducing-perplexity-deep-research\\nunifies both lines into synergized RAG–Reasoning\\nframeworks. Section 6 lists benchmarks, and Sec-\\ntion 7 outlines open challenges.\\n2\\nBackground and Preliminary\\nRAG mitigates knowledge cut-off of LLMs through',\n",
       " 'frameworks. Section 6 lists benchmarks, and Sec-\\ntion 7 outlines open challenges.\\n2\\nBackground and Preliminary\\nRAG mitigates knowledge cut-off of LLMs through\\nthree sequential stages: (i) Retrieval, fetching task-\\nrelevant content from external knowledge stores;\\n(ii) Integration, deduplicating, resolving conflicts,\\nand re-ranking the retrieved content; and (iii) Gen-\\neration, reasoning over the curated context to pro-\\nduce the final answer.\\nConcurrently, Chain-of-\\nThought technique has significantly enhanced the\\nreasoning capabilities of modern LLMs by encour-\\naging them to “think step by step” before answering.\\nThe synergy between the structured RAG pipeline\\nand these multi-step reasoning capacities grounds\\nthe emerging RAG-Reasoning paradigm explored\\nin this survey.\\n3\\nReasoning-Enhanced RAG\\nTraditional RAG methods first retrieve relevant doc-\\numents, then concatenate the retrieved knowledge\\nwith the original query to generate the final answer.',\n",
       " 'in this survey.\\n3\\nReasoning-Enhanced RAG\\nTraditional RAG methods first retrieve relevant doc-\\numents, then concatenate the retrieved knowledge\\nwith the original query to generate the final answer.\\nThese methods often fail to capture the deeper con-\\ntext or intricate relationships necessary for complex\\nreasoning tasks. By integrating reasoning capabili-\\nties across Retrieval, Integration, and Generation\\nstages of the RAG pipeline, the system can identify\\nand fetch the most relevant information, reducing\\nhallucinations and improving response accuracy.4\\n4If reasoning only serves to better leverage fixed retrieved\\nknowledge in a unidirectional manner, it is considered within\\n2',\n",
       " 'A survey of RAG and Reasoning\\nReasoning-enhanced\\nRAG (§3)\\nRetrieval\\nOptimization (§3.1)\\nReasoning-Aware Query Reformulation (§3.1.1)\\ne.g. Collab-RAG (Xu et al., 2025b), DynQR (Anonymous, 2025), DeepRetrieval (Jiang et al., 2025)\\nRetrieval Strategy and Planning (§3.1.2)\\ne.g. PAR-RAG (Zhang et al., 2025d), LPKG (Wang et al., 2024b), FIND (Jia et al., 2025)\\nRetrieval Model Enhancement (§3.1.3)\\ne.g. GNN-RAG (Mavromatis and Karypis, 2024), RuleRAG (Chen et al., 2024c),\\nIntegration\\nEnhancement (§3.2)\\nRelevance Assessment & Filtering (§3.2.1)\\ne.g. SEER (Zhao et al., 2024c), M-RAG-R (Yoran et al., 2024)\\nInformation Synthesis & Fusion (§3.2.2)\\ne.g. BeamAggR (Chu et al., 2024), DualRAG (Cheng et al., 2025) , CRP-RAG (Xu et al., 2024)\\nGeneration\\nEnhancement (§3.3)\\nContext-Aware Generation (§3.3.1)\\ne.g. Open-RAG (Islam et al., 2024), RARE (Wang et al., 2025d), Self-Reasoning (Xia et al., 2025b)\\nGrounded Generation Control (§3.3.2)',\n",
       " 'Generation\\nEnhancement (§3.3)\\nContext-Aware Generation (§3.3.1)\\ne.g. Open-RAG (Islam et al., 2024), RARE (Wang et al., 2025d), Self-Reasoning (Xia et al., 2025b)\\nGrounded Generation Control (§3.3.2)\\ne.g. RARR (Gao et al., 2023a), TRACE (Fang et al., 2024), AlignRAG (Wei et al., 2025b)\\nRAG-enhanced\\nReasoning (§4)\\nExternal Knowledge\\nRetrieval (§4.1)\\nKnowledge Base (§4.1.1)\\ne.g. Premise-Retrieval (Tao et al., 2025), ReaRAG (Lee et al., 2025), CBR-RAG (Wiratunga et al., 2024)\\nWeb Retrieval (§4.1.2)\\ne.g. ALR2 (Li et al., 2024d) , RARE (Tran et al., 2024), Open-RAG (Islam et al., 2024)\\nTool Using (§4.1.3)\\ne.g. TATU (Li et al., 2024g), TRICE(Qiao et al., 2024), Re-Invoke (Chen et al., 2024a)\\nIn-Context\\nRetrieval (§4.2)\\nPrior Experience (§4.2.1)\\ne.g. RAP (Kagaya et al., 2024), JARVIS-1 (Wang et al., 2024f), EM-LLM (Fountas et al., 2024)\\nExample or Training Data (§4.2.2)\\ne.g. MoD (Wang et al., 2024c), RE4 (Li et al., 2024c), UPRISE (Cheng et al., 2023)\\nSynergized RAG-\\nReasoning (§5)',\n",
       " 'Example or Training Data (§4.2.2)\\ne.g. MoD (Wang et al., 2024c), RE4 (Li et al., 2024c), UPRISE (Cheng et al., 2023)\\nSynergized RAG-\\nReasoning (§5)\\nReasoning Workflow\\n(§5.1)\\nChain-based (§5.1.1)\\ne.g. IRCoT (Trivedi et al., 2023), Rat (Wang et al., 2024g), CoV-RAG (He et al., 2024a), RAFT (Zhang et al., 2024a)\\nTree-based\\n(§5.1.2)\\nToT e.g. RATT (Zhang et al., 2025a), Tree of Clarifications (Kim et al., 2023), GROVE (Wen et al., 2023),\\nMCTS e.g. AirRAG (Feng et al., 2025), MCTS-RAG (Hu et al., 2025), SeRTS (Hu et al., 2024)\\nGraph-based\\n(§5.1.3)\\nWalk-on-Graph: e.g. QA-GNN (Yasunaga et al., 2021), LightRAG (Guo et al., 2024), StructRAG (Li et al., 2024h)\\nThink-on-Graph: e.g. ToG (Sun et al., 2024b), ToG-2.0 (Ma et al., 2024a), Graph-CoT (Jin et al., 2024),\\nAgent Orchestration\\n(§5.2)\\nSignle-Agent\\n(§ 5.2.1)\\nPrompting: e.g. ReAct (Yao et al., 2023b), Search-O1 (Li et al., 2025b); SFT: e.g. Toolformer (Schick et al., 2023),',\n",
       " 'Agent Orchestration\\n(§5.2)\\nSignle-Agent\\n(§ 5.2.1)\\nPrompting: e.g. ReAct (Yao et al., 2023b), Search-O1 (Li et al., 2025b); SFT: e.g. Toolformer (Schick et al., 2023),\\nINTERS (Zhu et al., 2024); RL: e.g. Search-R1 (Jin et al., 2025) R1-Searcher (Song et al., 2025)\\nMulti-Agent\\n(§ 5.2.2)\\nDecentralized: e.g. M-RAG (Wang et al., 2024e), MDocAgent (Han et al., 2025), Agentic reasoning (Wu et al., 2025c)\\nCentralized: e.g. HM-RAG (Liu et al., 2025), SurgRAW (Low et al., 2025), Chain of Agents (Zhang et al., 2024c)\\nFigure 2: Taxonomy of Recent Advances in RAG-Reasoning System.\\n3.1\\nRetrieval Optimization\\nRetrieval optimization leverages reasoning to im-\\nprove result relevance and quality. Existing meth-\\nods are broadly categorized (1) Reasoning-Aware\\nQuery Reformulation, (2) Retrieval Strategy and\\nPlanning, and (3) Retrieval Model Enhancement.\\n3.1.1\\nReasoning-Aware Query Reformulation\\nIt reformulates the original query to better retrieve\\nreasoning-relevant context. First, query decompo-',\n",
       " 'Planning, and (3) Retrieval Model Enhancement.\\n3.1.1\\nReasoning-Aware Query Reformulation\\nIt reformulates the original query to better retrieve\\nreasoning-relevant context. First, query decompo-\\nsition breaks down complex queries into simpler\\nsub-queries (Xu et al., 2025b). Second, query re-\\nformulation recasts ambiguous queries into more\\nclear ones.\\nTo align with reasoning needs of\\ngenerator, certain works train rewrites with RL\\nsignals (Anonymous, 2025; Wang et al., 2025c).\\nThird, query expansion enrich the semantic rich-\\nness of the query via CoT reasoning (Dhuliawala\\net al., 2024; Li et al., 2024e; Lee et al., 2024).\\n3.1.2\\nRetrieval Strategy and Planning\\nThis section covers global retrieval guidance. Ad-\\nvance planning uses a reasoning model to gener-\\nate a complete retrieval blueprint prior to execu-\\ntion. PAR-RAG (Zhang et al., 2025d) applies CoT\\nfor multi-step planning, mitigating local optima.\\nLPKG (Wang et al., 2024b) fine-tunes LLMs on',\n",
       " 'ate a complete retrieval blueprint prior to execu-\\ntion. PAR-RAG (Zhang et al., 2025d) applies CoT\\nfor multi-step planning, mitigating local optima.\\nLPKG (Wang et al., 2024b) fine-tunes LLMs on\\nknowledge graphs to encode relational structure. In\\ncontrast, adaptive retrieval decision methods make\\na one-step prediction on whether and how to re-\\ntrieve. FIND (Jia et al., 2025) and adaptive RAG\\n§3.3. In contrast, if reasoning dynamically triggers new\\nretrieval, it is discussed in §5.\\n(Jeong et al., 2024) use classifiers to assess query\\ncomplexity and select retrieval strategies, reducing\\nunnecessary calls. Marina et al. (2025) further adds\\nfeatures like entity popularity and question type.\\n3.1.3\\nRetrieval Model Enhancement\\nA line of work enhances retrievers with reason-\\ning via two strategies.\\nThe first one leverages\\nstructured knowledge: GNN-RAG (Mavromatis\\nand Karypis, 2024) encodes knowledge graphs\\nwith GNNs for implicit multi-hop reasoning, while',\n",
       " 'ing via two strategies.\\nThe first one leverages\\nstructured knowledge: GNN-RAG (Mavromatis\\nand Karypis, 2024) encodes knowledge graphs\\nwith GNNs for implicit multi-hop reasoning, while\\nRuleRAG (Chen et al., 2024c) appends symbolic\\nrules to guide retrieval toward logical consistency.\\nAnother strategy integrates explicit reasoning: Ji\\net al. (2024) combines CoT with the query to im-\\nprove intermediate knowledge recall in multi-hop\\nQA.\\n3.2\\nIntegration Enhancement\\nIntegration enhancement uses reasoning to assess\\nrelevance and merge heterogeneous evidence, pre-\\nventing irrelevant content from disrupting gener-\\nation. Methods fall into two categories: (1) rele-\\nvance assessment and (2) information synthesis.\\n3.2.1\\nRelevance Assessment & Filtering\\nThese methods assess the relevance of each re-\\ntrieved fragment to the user query through deeper\\nreasoning. SEER (Zhao et al., 2024c) employs\\nassessor experts to select faithful, helpful, and con-\\ncise evidence while discarding irrelevant content.',\n",
       " 'reasoning. SEER (Zhao et al., 2024c) employs\\nassessor experts to select faithful, helpful, and con-\\ncise evidence while discarding irrelevant content.\\nYoran et al. (2024) improves robustness by filtering\\nnon-entailing passages using an NLI model, then\\n3',\n",
       " 'fine-tuning the LLM on mixed relevant/irrelevant\\ncontexts to help it ignore residual noise.\\n3.2.2\\nInformation Synthesis & Fusion\\nOnce relevant snippets are identified, the challenge\\nis to fuse them into a coherent evidence set. Beam-\\nAggR (Chu et al., 2024) enumerates sub-question\\nanswer combinations and aggregates them via prob-\\nabilistic reasoning. DualRAG (Cheng et al., 2025)\\ncombines reasoning-augmented querying with pro-\\ngressive knowledge aggregation to filter and or-\\nganize retrieved information into an evolving out-\\nline. CRP-RAG (Xu et al., 2024) builds a rea-\\nsoning graph to retrieve, evaluate, and aggregate\\nknowledge at each node, dynamically selecting\\nknowledge-sufficiency paths before generation.\\n3.3\\nGeneration Enhancement\\nEven with retrieved context, traditional RAG may\\nstill generate unfaithful content without reasoning.\\nReasoning during generation addresses this issue\\nthrough two main approaches: (1) context-aware\\nsynthesis and (2) grounded generation control.\\n3.3.1',\n",
       " 'Reasoning during generation addresses this issue\\nthrough two main approaches: (1) context-aware\\nsynthesis and (2) grounded generation control.\\n3.3.1\\nContext-Aware Synthesis Strategies\\nContext-aware generation ensures outputs remain\\nrelevance while reducing noise. Selective–context\\nutilization prunes or re-weights content based on\\ntask relevance. Open-RAG (Islam et al., 2024)\\nuses a sparse expert mixture to dynamically select\\nknowledge modules, while RARE (Wang et al.,\\n2025d) adds domain knowledge to prompts to pro-\\nmote reliance on external context over memoriza-\\ntion. Reasoning path generation builds explicit log-\\nical chains to enhance transparency, e.g., Ranaldi\\net al. (2024) generate contrasting explanations by\\ncomparing paragraph relevance step-by-step, guid-\\ning the model toward accurate conclusions. Self-\\nReasoning (Xia et al., 2025b) constructs structured\\nreasoning chains through sequential evidence se-\\nlection and verification.\\n3.3.2\\nGrounded Generation Control',\n",
       " 'Reasoning (Xia et al., 2025b) constructs structured\\nreasoning chains through sequential evidence se-\\nlection and verification.\\n3.3.2\\nGrounded Generation Control\\nGrounded generation control introduces verifica-\\ntion mechanisms to ensure outputs remain an-\\nchored to retrieved evidence through reasoning.\\nFact verification methods use reasoning to assess\\nfactual consistency between generated content and\\nretrieved evidence, e.g., Self-RAG (Asai et al.,\\n2023) introduces reflection markers during decod-\\ning to trigger critical review and correction. Cita-\\ntion generation links generated content to source\\nmaterials to enhance traceability and credibility, as\\nin RARR (Gao et al., 2023a), which inserts cita-\\ntions while preserving stylistic coherence. Faith-\\nful reasoning ensures that each reasoning step ad-\\nheres to retrieved evidence without introducing\\nunverified content. TRACE (Fang et al., 2024)\\nbuilds knowledge graphs to form coherent evidence',\n",
       " 'ful reasoning ensures that each reasoning step ad-\\nheres to retrieved evidence without introducing\\nunverified content. TRACE (Fang et al., 2024)\\nbuilds knowledge graphs to form coherent evidence\\nchains, while AlignRAG (Wei et al., 2025b) applies\\ncriticism alignment to refine reasoning paths.\\n4\\nRAG-Enhanced Reasoning\\nIntegrating external knowledge or in-context\\nknowledge during reasoning can help LLMs reduce\\nhallucinations and bridge logical gaps. External re-\\ntrieval leverages structured sources like databases\\nor web content, providing factual grounding, like\\nIAG (Zhang et al., 2023). In-context retrieval uti-\\nlizes internal contexts like prior interactions or\\ntraining examples, enhancing contextual coherence,\\nlike RA-DT (Schmied et al., 2024). Both strategies\\ncollectively improve factual accuracy, interpretabil-\\nity, and logical consistency of reasoning processes.\\n4.1\\nExternal Knowledge Retrieval\\nExternal knowledge retrieval incorporates web',\n",
       " 'collectively improve factual accuracy, interpretabil-\\nity, and logical consistency of reasoning processes.\\n4.1\\nExternal Knowledge Retrieval\\nExternal knowledge retrieval incorporates web\\ncontent, database information, or external tools\\ninto reasoning, effectively filling knowledge gaps.\\nTargeted retrieval improves factual accuracy, en-\\nabling language models to reliably address complex\\nqueries by grounding reasoning steps in verified\\nexternal evidence.\\n4.1.1\\nKnowledge Base\\nKnowledge base (KB) typically stores arithmetic,\\ncommonsense, or logical knowledge in databases,\\nbooks, or documents, with retrieval approaches\\nvarying by task. For question answering (QA) rea-\\nsoning, AlignRAG (Wei et al., 2025b), MultiHop-\\nRAG (Tang and Yang, 2024), and CRP-RAG (Xu\\net al., 2025a) retrieve interconnected factual entries\\nfrom general KBs to enhance sequential reasoning.\\nIn specialized reasoning tasks, mathematical ap-\\nproaches like Premise-Retrieval (Tao et al., 2025)',\n",
       " 'from general KBs to enhance sequential reasoning.\\nIn specialized reasoning tasks, mathematical ap-\\nproaches like Premise-Retrieval (Tao et al., 2025)\\nand ReaRAG (Lee et al., 2025) utilize formal lem-\\nmas from theorem libraries for structured deduc-\\ntion; legal approaches like CASEGPT (Yang, 2024)\\nand CBR-RAG (Wiratunga et al., 2024) extract\\njudicial precedents for analogical reasoning. For\\ncode generation tasks, CodeRAG (Li et al., 2025a)\\nand Koziolek et al. (2024) access code snippets\\nfrom repositories, ensuring syntactic correctness.\\n4',\n",
       " '4.1.2\\nWeb Retrieval\\nWeb retrieval accesses dynamic online content like\\nweb pages, news or social media. Specifically,\\nin fact-checking tasks, approaches such as Ver-\\naCT Scan (Niu et al., 2024), Ragar (Khaliq et al.,\\n2024), PACAR (Zhao et al., 2024b), and STEEL\\n(Li et al., 2024b) verify claims step-by-step using\\nevidence from news or social media, enhancing log-\\nical reasoning. Meanwhile, QA-based reasoning\\nlike RARE (Tran et al., 2024), RAG-Star (Jiang\\net al., 2024), MindSearch (Chen et al., 2024b),\\nand OPEN-RAG (Islam et al., 2024) iteratively\\nrefine reasoning with broad web content, align-\\ning with current trends in agentic search, which\\ninvolve synthesizing complex online materials to\\nenhance context-aware and robust reasoning. Con-\\nversely, in specialized areas like medical reasoning,\\napproaches such as FRVA (Fan et al., 2024b) and\\nALR2 (Li et al., 2024d), retrieve literature for accu-\\nrate diagnostics.\\n4.1.3\\nTool Using\\nTool-using approaches leverage external resources',\n",
       " 'approaches such as FRVA (Fan et al., 2024b) and\\nALR2 (Li et al., 2024d), retrieve literature for accu-\\nrate diagnostics.\\n4.1.3\\nTool Using\\nTool-using approaches leverage external resources\\nlike calculators, libraries, or APIs to enhance rea-\\nsoning interactively. In QA-based reasoning, Re-\\nInvoke (Chen et al., 2024a), AVATAR (Wu et al.,\\n2024), ToolkenGPT (Hao et al., 2023), and Tool-\\nLLM (Qin et al., 2023) invoke calculators or APIs\\n(e.g., Yahoo Finance, Wikidata), improving numer-\\nical accuracy and factual precision. Within the con-\\ntext of scientific modeling, SCIAGENT (Ma et al.,\\n2024b) and TRICE (Qiao et al., 2024) integrate\\nsymbolic computation tools (e.g., WolframAlpha),\\nstrengthening computational robustness. Similarly,\\nin mathematical computation, llm-tool-use (Luo\\net al., 2025b) autonomously employs calculators\\nfor accurate numerical reasoning. Distinctively in\\ncode generation tasks, RAR (Dutta et al., 2024)\\nretrieves code documentation via OSCAT libraries,',\n",
       " 'et al., 2025b) autonomously employs calculators\\nfor accurate numerical reasoning. Distinctively in\\ncode generation tasks, RAR (Dutta et al., 2024)\\nretrieves code documentation via OSCAT libraries,\\nensuring syntactic accuracy and executable logic.\\n4.2\\nIn-context Retrieval\\nIn-context retrieval leverages a model’s internal\\nexperiences or retrieved examples from demonstra-\\ntions and training data to guide reasoning. This\\nretrieval provides relevant exemplars, guiding mod-\\nels to emulate reasoning patterns and enhancing\\naccuracy and logical coherence in novel questions.\\n4.2.1\\nPrior Experience\\nPrior experience refers to past interactions or suc-\\ncessful strategies stored in a model’s internal mem-\\nory, with retrieval varying by task. In tasks in-\\nvolving planning and decision-making tasks such\\nas robot path finding, RAHL (Sun et al., 2024a)\\nand RA-DT (Schmied et al., 2024) leverage past\\ndecisions and reinforcement signals for sequential\\nreasoning. For interactive reasoning tasks, JARVIS-',\n",
       " 'as robot path finding, RAHL (Sun et al., 2024a)\\nand RA-DT (Schmied et al., 2024) leverage past\\ndecisions and reinforcement signals for sequential\\nreasoning. For interactive reasoning tasks, JARVIS-\\n1 (Wang et al., 2024f), RAP (Kagaya et al., 2024),\\nand EM-LLM (Fountas et al., 2024) dynamically\\nrecall multimodal interactions and conversational\\nhistories, facilitating adaptive reasoning for person-\\nalized interactions. In the domain for logical rea-\\nsoning, CoPS (Yang et al., 2024a) retrieves struc-\\ntured prior cases like medical and legal cases for\\nrobust logical reasoning in medical and legal sce-\\nnarios.\\n4.2.2\\nExample or Training Data\\nUnlike approaches relying on prior experiences,\\nexample-based reasoning retrieves external exam-\\nples from demonstrations or training data. For ex-\\nample, In complex text-understanding, RE4 (Li\\net al., 2024c) and Fei et al. (2024) utilize anno-\\ntated sentence pairs to enhance relation recogni-\\ntion. Addressing QA-based reasoning, OpenRAG',\n",
       " 'ample, In complex text-understanding, RE4 (Li\\net al., 2024c) and Fei et al. (2024) utilize anno-\\ntated sentence pairs to enhance relation recogni-\\ntion. Addressing QA-based reasoning, OpenRAG\\n(Zhou and Chen, 2025), UPRISE (Cheng et al.,\\n2023), MoD (Wang et al., 2024c), and Dr.ICL (Luo\\net al., 2023) select demonstrations closely match-\\ning queries, improving generalization. Addition-\\nally, in code generation tasks, PERC (Yoo et al.,\\n2025) retrieves pseudocode by semantic or struc-\\ntural similarity from datasets like HumanEval, en-\\nsuring alignment with target code.\\n5\\nSynergized RAG-Reasoning\\nMany real-world problems, such as open-domain\\nquestion answering (Yang et al., 2015; Chen and\\nYih, 2020) and scientific discovery (Lu et al., 2024;\\nWang et al., 2023; Baek et al., 2024; Schmidgall\\net al., 2025), require an iterative approach where\\nnew evidence continuously informs better reason-\\ning and vice versa. A single retrieval step may not\\nprovide sufficient information, and a single round',\n",
       " 'new evidence continuously informs better reason-\\ning and vice versa. A single retrieval step may not\\nprovide sufficient information, and a single round\\nof reasoning may overlook key insights (Trivedi\\net al., 2023). By tightly integrating retrieval and\\nreasoning in a multi-step, interactive manner, these\\nsystems can progressively refine both the search rel-\\nevance of retrieved information and the reasoning-\\nbased understanding of the original query. We\\nfocus on two complementary perspectives within\\nexisting approaches: reasoning workflows, which\\nemphasize structured, often pre-defined inference\\nformats for multi-step reasoning; and agent orches-\\n5',\n",
       " 'tration, which focus on how agents interact with\\nenvironment and coordinate with each others.\\n5.1\\nReasoning Workflow\\nBroadly, the reasoning workflows can be catego-\\nrized as chain-based, tree-based, or graph-based,\\nreflecting an evolution from linear reasoning chains\\nto branching and expressive reasoning structures.\\n5.1.1\\nChain-based\\nChain-of-Thought (CoT) (Wei et al., 2022) struc-\\ntures the reasoning process as a linear sequence\\nof intermediate steps. However, relying solely on\\nthe parametric knowledge of LLMs can lead to\\nerror propagation. To solve this, IRCoT (Trivedi\\net al., 2023) and Rat (Wang et al., 2024g) interleave\\nretrieval operations between reasoning steps. Sev-\\neral recent methods further improve the robustness\\nand rigor of this chain-based paradigm via verifi-\\ncation and filtering. CoV-RAG (He et al., 2024a)\\nintroduces a chain-of-verification that checks and\\ncorrects each reasoning step against retrieved ref-\\nerences. To combat noisy or irrelevant context, ap-',\n",
       " 'introduces a chain-of-verification that checks and\\ncorrects each reasoning step against retrieved ref-\\nerences. To combat noisy or irrelevant context, ap-\\nproaches like RAFT (Zhang et al., 2024a) fine-tune\\nLLMs to ignore distractor documents, while Chain-\\nof-Note (Yu et al., 2024) prompts the model to take\\nsequential “reading notes” on retrieved documents\\nto filter out unhelpful information.\\n5.1.2\\nTree-based\\nTree-based reasoning methods typically adopt ei-\\nther Tree-of-Thought (ToT) (Yao et al., 2023a)\\nor Monte Carlo Tree Search (MCTS) (Browne\\net al., 2012) approaches. ToT extends the CoT\\nto explicitly construct a deterministic reasoning\\ntree and branch multiple logical pathways. Exam-\\nples include RATT (Zhang et al., 2025a), which\\nconstruct retrieval-augmented thought trees to si-\\nmultaneously evaluate multiple reasoning trajec-\\ntories.\\nSuch ToT principles avoid LLM being\\ntrapped by an early mistaken assumption and have\\nbeen applied to address ambiguous questions (Kim',\n",
       " 'multaneously evaluate multiple reasoning trajec-\\ntories.\\nSuch ToT principles avoid LLM being\\ntrapped by an early mistaken assumption and have\\nbeen applied to address ambiguous questions (Kim\\net al., 2023), to cover different diagnostic possibili-\\nties (Yang and Huang, 2025), and to create complex\\nstories (Wen et al., 2023). Conversely, MCTS-\\nbased approaches like AirRAG (Feng et al., 2025),\\nMCTS-RAG (Hu et al., 2025), and SeRTS (Hu\\net al., 2024) employ probabilistic tree search, dy-\\nnamically prioritizing exploration based on heuris-\\ntic probabilities. To ensure retrieval and reason-\\ning quality, AirRAG (Feng et al., 2025) incorpo-\\nrates self-consistency checks, and MCTS-RAG (Hu\\net al., 2025) integrates adaptive MCTS retrieval to\\nrefine evidence and reduce hallucinations.\\n5.1.3\\nGraph-based\\nWalk-on-Graph methods mainly rely on graph\\nlearning techniques for the retrieval and rea-\\nsoning.\\nFor example, PullNet (Sun et al.,\\n2019), QA-GNN (Yasunaga et al., 2021), and',\n",
       " '5.1.3\\nGraph-based\\nWalk-on-Graph methods mainly rely on graph\\nlearning techniques for the retrieval and rea-\\nsoning.\\nFor example, PullNet (Sun et al.,\\n2019), QA-GNN (Yasunaga et al., 2021), and\\nGreaseLM (Zhang et al., 2022b) directly integrate\\ngraph neural networks (GNNs) to iteratively aggre-\\ngate information from neighbor nodes, excelling\\nat modeling the intricate relationships inherent in\\ngraph-structured data. Methods such as SR (Zhang\\net al., 2022a), LightRAG (Guo et al., 2024), and\\nStructRAG (Li et al., 2024h) employ lightweight\\ngraph techniques such as vector indexing and\\nPageRank to efficiently retrieve and reason in multi-\\nhop context, providing the LLM with high-quality,\\nstructured content tailored for the queries. In con-\\ntrast, Think-on-Graph methods integrate graph\\nstructures directly into the LLM reasoning loop,\\nenabling dynamic and iterative retrieval and reason-\\ning processes guided by the LLMs themselves. In\\nthe Think-on-Graph (ToG) framework (Sun et al.,',\n",
       " 'structures directly into the LLM reasoning loop,\\nenabling dynamic and iterative retrieval and reason-\\ning processes guided by the LLMs themselves. In\\nthe Think-on-Graph (ToG) framework (Sun et al.,\\n2024b; Ma et al., 2024a), the LLM uses the KG as\\na “reasoning playground”: at each step, it decides\\nwhich connected entity or relation to explore next,\\ngradually building a path that leads to the answer.\\nWhile Graph-CoT (Jin et al., 2024) introduces a\\nthree-stage iterative loop (reasoning, graph inter-\\naction, and execution), KGP (Wang et al., 2024d)\\nprioritize first constructing a document-level KG,\\nboth enabling LLM-driven graph traversal agent to\\nnavigate passages in each step with globally coher-\\nent context. GraphReader (Li et al., 2024f) further\\nrefines this paradigm by coupling LLM reasoning\\nwith explicit subgraph retrieval and evidence an-\\nchoring at each step\\n5.2\\nAgent Orchestration\\nAccording to agent architectures (Luo et al., 2025a),',\n",
       " 'refines this paradigm by coupling LLM reasoning\\nwith explicit subgraph retrieval and evidence an-\\nchoring at each step\\n5.2\\nAgent Orchestration\\nAccording to agent architectures (Luo et al., 2025a),\\nwe organize existing work into single-agent and\\nmulti-agent. Particularly, we have attached recent\\nadvances in agentic deep research and implementa-\\ntions in Appendix B.\\n5.2.1\\nSingle-Agent\\nSingle agentic system interweaves knowledge re-\\ntrieval (search) into an LLM’s reasoning loop, en-\\nabling dynamic information lookup at each step\\nof problem solving and incentivizing it to actively\\nseek out relevant evidence when needed.\\n6',\n",
       " 'The ReAct (Yao et al., 2023b) paradigm and its\\nderivatives (Li et al., 2025b; Alzubi et al., 2025)\\nhave pioneered this prompting strategy by guid-\\ning LLMs to explicitly alternate between reason-\\ning steps and external tool interactions, such as\\ndatabase searches. Different from ReAct that sepa-\\nrates reasoning and action, with explicit commands\\nlike “search” triggering external retrieval, meth-\\nods such as Self-Ask (Press et al., 2023) and IR-\\nCoT (Trivedi et al., 2023) prompt the model to\\nrecursively formulate and answer sub-questions,\\nenabling interleaved retrieval within the Chain-of-\\nThought (step-by-step retrieval and reasoning). In-\\nvolving self-reflection strategies, DeepRAG (Guan\\net al., 2025) and Self-RAG (Asai et al., 2024) em-\\npower LLMs to introspectively assess their knowl-\\nedge limitations and retrieve only when necessary.\\nRather than relying solely on prompting or static\\nretrievers, Toolformer (Schick et al., 2023) and IN-',\n",
       " 'edge limitations and retrieve only when necessary.\\nRather than relying solely on prompting or static\\nretrievers, Toolformer (Schick et al., 2023) and IN-\\nTERS (Zhu et al., 2024) represent a complementary\\napproach via supervised fine-tuning (SFT) LLMs\\non instruction-based or synthetic datasets that inter-\\nleave search and reasoning. Synthetic data genera-\\ntion (Schick et al., 2023; Mao et al., 2024; Zhang\\net al., 2024a) aims to create large-scale, diverse,\\nand task-specific datasets for search without the\\nneed for extensive human annotation. In contrast,\\ninstruction-based data reformulation (Zhu et al.,\\n2024; Wang et al., 2024a; Lin et al., 2023; Nguyen\\net al., 2024) repurposes existing datasets into in-\\nstructional formats to fine-tune models for im-\\nproved generalization and alignment with human-\\nlike reasoning. INTERS (Zhu et al., 2024) exem-\\nplifies this approach by introducing a SFT dataset\\nencompassing 20 tasks, derived from 43 distinct\\ndatasets with manually written templates.',\n",
       " 'like reasoning. INTERS (Zhu et al., 2024) exem-\\nplifies this approach by introducing a SFT dataset\\nencompassing 20 tasks, derived from 43 distinct\\ndatasets with manually written templates.\\nReinforcement learning (RL)-incentivized ap-\\nproaches provides a mechanism to optimize answer\\nquality via reward signals on incentivizing agents’\\nbehaviors – what to search, how to integrate re-\\ntrieved evidence, and when to stop, aiming at com-\\nplex knowledge-intensive tasks (or “deep research”\\nquestions). Notable efforts like WebGPT (Nakano\\net al., 2021) and RAG-RL (Huang et al., 2025a)\\nfocus on improving reasoning fidelity by rewarding\\noutputs based on factual correctness or human pref-\\nerence. More recent contributions operate directly\\nin dynamic environments (e.g., live web search, lo-\\ncal search tools), training agents to explore, reflect,\\nand self-correct in noisy real-world conditions. For\\nexample, Search-R1 (Jin et al., 2025) learns to gen-\\nerate <search> token during reasoning and con-',\n",
       " 'and self-correct in noisy real-world conditions. For\\nexample, Search-R1 (Jin et al., 2025) learns to gen-\\nerate <search> token during reasoning and con-\\ncurrently R1-Searcher (Song et al., 2025) builds\\non RL-driven search demonstrating strong gener-\\nalization across domains. Deep-Researche (Zheng\\net al., 2025) make step further by introducing the\\nfirst end-to-end RL-trained research agent that in-\\nteracts with the open web. These settings showcase\\nemergent capabilities, like decomposition, itera-\\ntive verification, and retrieval planning, that su-\\npervised methods often hard to instill. Moreover,\\nReSearch (Chen et al., 2025b) and ReARTeR (Sun\\net al., 2025c) tackle a deeper challenge: not just\\nproducing correct answers, but aligning reasoning\\nsteps with both factuality and interpretability.\\n5.2.2\\nMulti-Agent\\nThe exploration of multi-agent collaboration within\\nRAG and reasoning has led to diverse orchestra-\\ntions: centralized architectures (harness collective',\n",
       " '5.2.2\\nMulti-Agent\\nThe exploration of multi-agent collaboration within\\nRAG and reasoning has led to diverse orchestra-\\ntions: centralized architectures (harness collective\\nintelligence from workers-manager paradigm) and\\ndecentralized architectures (leverage complemen-\\ntary capabilities from role-specialized agents).\\nDecentralized architectures deploy multiple\\nagents to collaboratively perform retrieval, reason-\\ning, and knowledge integration, aiming to broaden\\ncoverage of relevant information and fully exploit\\nthe heterogeneous strengths of specialized agents.\\nWang et al. (2024e) and Salve et al. (2024) in-\\ntroduce multi-agent systems where each agent re-\\ntrieves from a partitioned database or a specific data\\nsource (relational databases, NoSQL document\\nstores, etc.). Beyond retrieval, Collab-RAG (Xu\\net al., 2025b) and RAG-KG-IL (Yu and McQuade,\\n2025) integrate different model capacities and as-\\nsign them different roles in reasoning and knowl-',\n",
       " 'stores, etc.). Beyond retrieval, Collab-RAG (Xu\\net al., 2025b) and RAG-KG-IL (Yu and McQuade,\\n2025) integrate different model capacities and as-\\nsign them different roles in reasoning and knowl-\\nedge integration. This philosophy extends to multi-\\nmodal settings as in MDocAgent (Han et al., 2025),\\nwhich employs a team of text and image agents to\\nprocess and reason the document-based QA. A gen-\\neral formulation is seen in Agentic reasoning (Wu\\net al., 2025c), which unites tool-using agents for\\nsearch, computation, and structured reasoning, or-\\nchestrated to solve complex analytical tasks.\\nCentralized architectures structure agents in hi-\\nerarchical centralized patterns, supporting efficient\\ntask decomposition and progressive refinement.\\nHM-RAG (Liu et al., 2025) and SurgRAW (Low\\net al., 2025) both employ decomposer-retriever-\\ndecider architectures, where different agent roles\\nisolate subproblems such as multimodal processing\\nor surgical decision-making. Wu et al. (2025a) and',\n",
       " 'et al., 2025) both employ decomposer-retriever-\\ndecider architectures, where different agent roles\\nisolate subproblems such as multimodal processing\\nor surgical decision-making. Wu et al. (2025a) and\\nIannelli et al. (2024) emphasize dynamic routing\\nand system reconfiguration, respectively—enabling\\n7',\n",
       " 'intelligent agent selection based on task relevance\\nor resource constraints. Chain of Agents (Zhang\\net al., 2024c) and the cooperative multi-agent con-\\ntrol framework for on-ramp merging (Zhang et al.,\\n2025c) illustrate hierarchical agent designs where\\nlayered processing enables long-context summa-\\nrization or policy refinement. Collectively, these\\nworks demonstrate how centralized control and hi-\\nerarchical pipelining foster efficiency and adapt-\\nability in multi-agent RAG-reasoning systems.\\n6\\nBenchmarks and Datasets\\nBenchmarks and datasets for simultaneously evalu-\\nating knowledge (RAG) and reasoning capability\\ncover a wide range of complexities, from basic\\nfact retrieval to intricate multi-step reasoning in\\ngeneral or specific domains. We categorize no-\\ntable benchmarks in several tasks and list them in\\nTable 1 and highlight their details and properties.\\nThese representative tasks include Web browsing,\\nsuch as BrowseComp (Wei et al., 2025a), single-',\n",
       " 'table benchmarks in several tasks and list them in\\nTable 1 and highlight their details and properties.\\nThese representative tasks include Web browsing,\\nsuch as BrowseComp (Wei et al., 2025a), single-\\nhop QA, such as TriviaQA (Joshi et al., 2017),\\nmulti-hop QA, such as HotpotQA (Yang et al.,\\n2018), multiple-choice QA, such as MMLU-Pro\\n(Wang et al., 2025b), mathematics, such as MATH\\n(Hendrycks et al., 2021), and code-centric eval-\\nuations from LiveCodeBench (Jain et al., 2024).\\nMore tasks can refer to Appendix A and Table 2.\\n7\\nFuture Work\\nFuture research directions for Synergized RAG-\\nReasoning systems center around enhancing both\\nreasoning and retrieval capabilities to meet real-\\nworld demands for accuracy, efficiency, trust, and\\nuser alignment. We outline several key challenges\\nand opportunities below.\\n• Reasoning Efficiency. Despite their advantages\\nin complex reasoning, Synergized RAG-Reasoning\\nsystems can suffer significant latency due to itera-',\n",
       " 'and opportunities below.\\n• Reasoning Efficiency. Despite their advantages\\nin complex reasoning, Synergized RAG-Reasoning\\nsystems can suffer significant latency due to itera-\\ntive retrieval and multi-step reasoning loops (Sui\\net al., 2025). For instance, executing a single deep\\nresearch query can take over 10 minutes in prac-\\ntical settings. This issue is especially pronounced\\nin chain-based workflows discussed in Section 5.\\nFuture research should explore reasoning efficiency\\nthrough latent reasoning approaches and strategic\\ncontrol over reasoning depth via thought distilla-\\ntion and length-penalty (Xia et al., 2025a; Zhang\\net al., 2025b). Beyond reasoning itself, emerging\\ndirections in models compression like quantization,\\npruning, and knowledge distillation is worth to ex-\\nplore for efficient small RAG-reasoning systems.\\n• Retrieval Efficiency. On the retrieval side, effi-\\nciency demands budget-aware query planning and\\nmemory-aware mechanisms that cache prior evi-',\n",
       " 'plore for efficient small RAG-reasoning systems.\\n• Retrieval Efficiency. On the retrieval side, effi-\\nciency demands budget-aware query planning and\\nmemory-aware mechanisms that cache prior evi-\\ndence or belief states to reduce redundant access\\n(Zhao et al., 2024a). Additionally, adaptive re-\\ntrieval control, learning when and how much to\\nretrieve based on uncertainty signals can reduce\\nwasteful operations. These technical paths push\\nthe system beyond static RAG, toward dynamic\\nself-regulation of efficient retreival behaviors un-\\nder real-world constraints.\\n• Human-Agent Collaboration. Many applica-\\ntions of RAG-Reasoning, such as literature reviews\\nor interactive programming, are inherently person-\\nalized and cannot assume users know precisely\\nwhat to ask or how to process retrieved results (Sun\\net al., 2025b). Corresponding to Section 5.2, hu-\\nmans can act as advanced agents, providing nu-\\nanced feedback to steer reasoning processes. Fu-',\n",
       " 'what to ask or how to process retrieved results (Sun\\net al., 2025b). Corresponding to Section 5.2, hu-\\nmans can act as advanced agents, providing nu-\\nanced feedback to steer reasoning processes. Fu-\\nture systems should develop methods for modeling\\nuser intent under uncertainty (Zhang et al., 2025e;\\nYang et al., 2025), building interactive interfaces\\nfor iterative clarification, and designing agents that\\nadapt reasoning strategies based on user exper-\\ntise and preferences (Zhang et al., 2025g). This\\nhuman-in-the-loop approach (Zou et al., 2025) is\\nessential for creating robust and user-aligned RAG-\\nReasoning systems in open-ended domains.\\n• Agentic Structures and Capabilities. A key fea-\\nture of Synergized RAG-Reasoning is its agentic ar-\\nchitecture, where the system autonomously decides\\nthe roles of different agents and which tools or re-\\ntrieval strategies to invoke during inference stages\\n(Luo et al., 2025a; Bei et al., 2025). To fully ex-',\n",
       " 'the roles of different agents and which tools or re-\\ntrieval strategies to invoke during inference stages\\n(Luo et al., 2025a; Bei et al., 2025). To fully ex-\\nploit this potential, future research should focus on\\ndeveloping agent frameworks capable of dynamic\\ntool selection, retrieval planning, and adaptive or-\\nchestration across reasoning workflows. Such ca-\\npabilities enable flexible, context-aware problem\\nsolving and are critical for handling diverse, com-\\nplex tasks (Schneider, 2025).\\n• Multimodal Retrieval. As also shown in our\\nbenchmark analysis, most existing Synergized\\nRAG-Reasoning systems remain confined to text-\\nonly tasks. However, real-world applications in-\\ncreasingly require the ability to retrieve and in-\\ntegrate multimodal content (Liang et al., 2024).\\n8',\n",
       " 'Task\\nDataset\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning\\nSize\\nInput\\nOutput\\nWeb Browsing\\nBrowseComp (Wei et al., 2025a)\\nGeneral\\nHuman, Internet\\nCommonsense, Logical\\nDeductive\\n1,266\\nQuestion/Text\\nNatural Language\\nGAIA (Mialon et al., 2023)\\nGeneral\\nInternet, TooL\\nCommonsense, Logical\\nDeductive\\n466\\nQuestion/Text,\\nImage/File/Code\\nNatural Language\\nWebWalkerQA (Wu et al., 2025b)\\nGeneral\\nHuman, LLM\\nCommonsense, Logical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nSingle-hop QA\\nTriviaQA (Joshi et al., 2017)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n650,000+\\nQuestion/Text\\nNatural Language\\nNQ (Kwiatkowski et al., 2019)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n307,373\\nQuestion/Text\\nNatural Language\\nMulti-hop QA\\n2WikiMultiHopQA (Ho et al., 2020)\\nGeneral\\nInternet\\nCommonsense, Logical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al., 2018)\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al., 2022)\\nGeneral',\n",
       " 'Deductive\\n192,606\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al., 2018)\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al., 2022)\\nGeneral\\nPrevious Resource,\\nInternet\\nCommonsense, Logical\\nDeductive\\n25,000\\nQuestion/Text\\nNatural Language\\nMulti-choice QA QuALITY (Pang et al., 2022)\\nNarrative\\nBooks\\nCommonsense, Logical\\nDeductive,\\nAbductive\\n6,737\\nQuestion/Text,\\nOptions\\nOptions\\nMMLU-Pro (Wang et al., 2025b)\\nScience\\nPrevious Resource,\\nInternet\\nArithmetic, Commonsense,\\nLogical\\nDeductive,\\nInductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Langue,\\nNumber, Options\\nMath\\nMATH (Hendrycks et al., 2021)\\nMath\\nExam\\nArithmetic, Logic\\nDeductive\\n12,500\\nQuestion/Text,\\nFigure, Equation\\nNatural Langue,\\nNumber\\nAQuA (Ling et al., 2017)\\nMath\\nExam, Internet,\\nPrevious Resource\\nArithmetic, Logic\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equation\\nNatural Langue,\\nOptions\\nCode\\nRefactoring Oracle (Tsantalis et al.,\\n2020)\\nSoftware\\nInternet, Human\\nLogical\\nDeductive',\n",
       " 'Arithmetic, Logic\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equation\\nNatural Langue,\\nOptions\\nCode\\nRefactoring Oracle (Tsantalis et al.,\\n2020)\\nSoftware\\nInternet, Human\\nLogical\\nDeductive\\n7,226\\nCode, Instruction\\nCode\\nLiveCodeBench (Jain et al., 2024)\\nContest\\nInternet\\nLogical\\nDeductive,\\nAbductive\\n500+\\nQuestion/Text,\\nCode, Instruction\\nCode, Test Output\\nTable 1: Overview of representative knowledge and reasoning intensive benchmarks by task category.\\nFuture research should move beyond the tradi-\\ntional vision-text paradigm to achieve genuine mul-\\ntimodality. This advancement necessitates strength-\\nening foundational abilities of MLLMs, including\\ngrounding and cross-modal reasoning (Liang et al.,\\n2024). Additionally, enhancing the agentic capabil-\\nities of these models through hybrid-modal chain-\\nof-thought reasoning is crucial, enabling interac-\\ntion with the real world via multimodal search tools\\n(Wang et al., 2025a). Concurrently, developing uni-',\n",
       " 'of-thought reasoning is crucial, enabling interac-\\ntion with the real world via multimodal search tools\\n(Wang et al., 2025a). Concurrently, developing uni-\\nfied multimodal retrievers that can jointly embed\\nimages, tables, text, and heterogeneous documents\\nis essential.\\n• Retrieval Trustworthiness. Synergized RAG-\\nReasoning systems remain vulnerable to adversar-\\nial attacks through poisoned or misleading external\\nknowledge sources. Ensuring the trustworthiness\\nof retrieved content is therefore crucial for main-\\ntaining fully reliable downstream reasoning (Huang\\net al., 2024). Techniques like watermarking and\\ndigital fingerprinting have been employed to en-\\nhance system traceability. However, there’s a press-\\ning need to develop more dynamic and adaptive\\nmethods that can keep pace with the evolving land-\\nscape of LLMs, emerging attack techniques, and\\nshifting model contexts (Liu et al., 2024). Existing\\nstudies have also individually explored uncertainty',\n",
       " 'scape of LLMs, emerging attack techniques, and\\nshifting model contexts (Liu et al., 2024). Existing\\nstudies have also individually explored uncertainty\\nquantification and robust generation to bolster sys-\\ntem reliability (Shorinwa et al., 2025). Future re-\\nsearch should aim to integrate these approaches,\\nas their combination can mutually reinforce sys-\\ntem robustness and trustworthiness. Moreover, fu-\\nture efforts should also focus on extending current\\nbenchmarks to encompass multi-dimensional trust\\nmetrics beyond mere accuracy.\\n8\\nConclusion\\nThis survey charts the rapid convergence of re-\\ntrieval and reasoning in LLMs.\\nWe reviewed\\nthree evolutionary stages: (1) Reasoning-Enhanced\\nRAG, which uses multi-step reasoning to refine\\neach stage of RAG; (2) RAG-Enhanced Reason-\\ning, which leverages retrieved knowledge to bridge\\nfactual gaps during long CoT; and (3) Synergized\\nRAG-Reasoning systems, where single- or multi-\\nagents iteratively refine both search and reason-',\n",
       " 'ing, which leverages retrieved knowledge to bridge\\nfactual gaps during long CoT; and (3) Synergized\\nRAG-Reasoning systems, where single- or multi-\\nagents iteratively refine both search and reason-\\ning, exemplified by recent “Deep Research” plat-\\nforms. Collectively, these lines demonstrate that\\ntight retrieval–reasoning coupling improves fac-\\ntual grounding, logical coherence, and adaptability\\nbeyond one-way enhancement. Looking forward,\\nwe identify research avenues toward synergized\\nRAG-Reasoning systems that are more effective,\\nmultimodally-adaptive, trustworthy, and human-\\ncentric.\\nLimitations\\nWhile this survey synthesizes over 200 research\\npapers across RAG and reasoning with large lan-\\nguage models, its scope favors breadth over depth.\\nIn striving to provide a unified and comprehen-\\nsive taxonomy, we may not delve deeply into the\\ntechnical nuances or implementation details of indi-\\nvidual methods-especially within specialized sub-\\nfields of either RAG (e.g., sparse vs. dense re-',\n",
       " 'sive taxonomy, we may not delve deeply into the\\ntechnical nuances or implementation details of indi-\\nvidual methods-especially within specialized sub-\\nfields of either RAG (e.g., sparse vs. dense re-\\ntrieval, memory-augmented retrievers) or reason-\\ning (e.g., formal logic solvers, symbolic methods,\\nor long-context reasoning). Moreover, our cate-\\n9',\n",
       " 'gorization framework (reasoning-enhanced RAG,\\nRAG-enhanced reasoning, and synergized RAG\\nand reasoning) abstracts across diverse methodolo-\\ngies. While this facilitates a high-level understand-\\ning of design patterns, it may obscure the finer-\\ngrained trade-offs, assumptions, and limitations\\nunique to each class of approach.\\nReferences\\nVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-\\nman, Harm de Vries, and Siva Reddy. 2022. Topi-\\nocqa: Open-domain conversational question answer-\\ning with topic switching. Transactions of the Associ-\\nation for Computational Linguistics, 10:468–483.\\nFiroj Alam, Ferda Ofli, and Muhammad Imran. 2018.\\nCrisismmd: Multimodal twitter datasets from natural\\ndisasters. In Proceedings of the international AAAI\\nconference on web and social media, volume 12.\\nSalaheddin Alzubi, Creston Brooks, Purva Chiniya,\\nEdoardo Contente, Chiara von Gerlach, Lucas Irwin,\\nYihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong\\nOh, et al. 2025. Open deep search: Democratizing',\n",
       " 'Salaheddin Alzubi, Creston Brooks, Purva Chiniya,\\nEdoardo Contente, Chiara von Gerlach, Lucas Irwin,\\nYihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong\\nOh, et al. 2025. Open deep search: Democratizing\\nsearch with open-source reasoning agents. arXiv\\npreprint arXiv:2503.20201.\\nAnonymous. 2025.\\nDynQR: Dynamic uncertainty-\\nguided query rewriting for effective retrieval-\\naugmented generation. In Submitted to ACL Rolling\\nReview - December 2024. Under review.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil,\\nand Hannaneh Hajishirzi. 2023.\\nSelf-RAG: Self-\\nreflective retrieval augmented generation.\\nIn\\nNeurIPS 2023 Workshop on Instruction Tuning and\\nInstruction Following.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\\nHannaneh Hajishirzi. 2024. Self-RAG: Learning to\\nretrieve, generate, and critique through self-reflection.\\nIn The Twelfth International Conference on Learning\\nRepresentations.\\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,\\nand Sung Ju Hwang. 2024.\\nResearchagent: Iter-',\n",
       " 'In The Twelfth International Conference on Learning\\nRepresentations.\\nJinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,\\nand Sung Ju Hwang. 2024.\\nResearchagent: Iter-\\native research idea generation over scientific liter-\\nature with large language models. arXiv preprint\\narXiv:2404.07738.\\nYuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi\\nChen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu,\\nShirui Pan, Yizhou Yu, et al. 2025. Graphs meet ai\\nagents: Taxonomy, progress, and future opportunities.\\narXiv preprint arXiv:2506.18019.\\nCameron B Browne, Edward Powley, Daniel White-\\nhouse, Simon M Lucas, Peter I Cowling, Philipp\\nRohlfshagen, Stephen Tavener, Diego Perez, Spyri-\\ndon Samothrakis, and Simon Colton. 2012. A survey\\nof monte carlo tree search methods. IEEE Transac-\\ntions on Computational Intelligence and AI in games,\\n4(1):1–43.\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-',\n",
       " '4(1):1–43.\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-\\nvey on evaluation of large language models. ACM\\ntransactions on intelligent systems and technology,\\n15(3):1–45.\\nDanqi Chen and Wen-tau Yih. 2020. Open-domain\\nquestion answering. In Proceedings of the 58th an-\\nnual meeting of the association for computational\\nlinguistics: tutorial abstracts, pages 34–37.\\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou,\\nChenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen\\nZhang, Huajun Chen, Fan Yang, et al. 2025a.\\nResearch:\\nLearning to reason with search for\\nllms via reinforcement learning.\\narXiv preprint\\narXiv:2503.19470.\\nMingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou,\\nChenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng\\nChen, Haofen Wang, Jeff Z Pan, et al. 2025b. Learn-\\ning to reason with search for llms via reinforcement\\nlearning. arXiv preprint arXiv:2503.19470.',\n",
       " 'Chenzheng Zhu, Fan Yang, Zenan Zhou, Weipeng\\nChen, Haofen Wang, Jeff Z Pan, et al. 2025b. Learn-\\ning to reason with search for llms via reinforcement\\nlearning. arXiv preprint arXiv:2503.19470.\\nQiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng,\\nJiannan Guan, Peng Wang, Mengkang Hu, Yuhang\\nZhou, Te Gao, and Wanxiang Che. 2025c. Towards\\nreasoning era: A survey of long chain-of-thought\\nfor reasoning large language models. arXiv preprint\\narXiv:2503.09567.\\nYanfei Chen, Jinsung Yoon, Devendra Sachan, Qingze\\nWang, Vincent Cohen-Addad, Mohammadhossein\\nBateni, Chen-Yu Lee, and Tomas Pfister. 2024a. Re-\\ninvoke: Tool invocation rewriting for zero-shot tool\\nretrieval. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 4705–4726.\\nZehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu,\\nWenwei Zhang, Kai Chen, and Feng Zhao. 2024b.\\nMindsearch: Mimicking human minds elicits deep ai\\nsearcher. arXiv preprint arXiv:2407.20183.\\nZhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen',\n",
       " 'Wenwei Zhang, Kai Chen, and Feng Zhao. 2024b.\\nMindsearch: Mimicking human minds elicits deep ai\\nsearcher. arXiv preprint arXiv:2407.20183.\\nZhongwu Chen, Chengjin Xu, Dingmin Wang, Zhen\\nHuang, Yong Dou, Xuhui Jiang, and Jian Guo. 2024c.\\nRulerag: Rule-guided retrieval-augmented genera-\\ntion with language models for question answering.\\narXiv preprint arXiv:2410.22353.\\nDaixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng\\nZhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu\\nWei, Weiwei Deng, and Qi Zhang. 2023. Uprise:\\nUniversal prompt retrieval for improving zero-shot\\nevaluation. In Proceedings of the 2023 Conference\\non Empirical Methods in Natural Language Process-\\ning, pages 12318–12337.\\nRong Cheng, Jinyi Liu, Yan Zheng, Fei Ni, Jiazhen Du,\\nHangyu Mao, Fuzheng Zhang, Bo Wang, and Jianye\\nHao. 2025. Dualrag: A dual-process approach to in-\\ntegrate reasoning and retrieval for multi-hop question\\nanswering. arXiv preprint arXiv:2504.18243.\\nZheng Chu, Jingchang Chen, Qianglong Chen, Haotian',\n",
       " 'tegrate reasoning and retrieval for multi-hop question\\nanswering. arXiv preprint arXiv:2504.18243.\\nZheng Chu, Jingchang Chen, Qianglong Chen, Haotian\\nWang, Kun Zhu, Xiyuan Du, Weijiang Yu, Ming Liu,\\n10',\n",
       " 'and Bing Qin. 2024. Beamaggr: Beam aggregation\\nreasoning over multi-source knowledge for multi-\\nhop question answering. In Proceedings of the 62nd\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 1229–\\n1248.\\nDebrup Das, Debopriyo Banerjee, Somak Aditya,\\nand Ashish Kulkarni. 2024. Mathsensei: A tool-\\naugmented large language model for mathematical\\nreasoning. In Proceedings of the 2024 Conference of\\nthe North American Chapter of the Association for\\nComputational Linguistics: Human Language Tech-\\nnologies (Volume 1: Long Papers), pages 942–966.\\nChao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-\\nZhi Li, Jian Xu, Xiao-Hui Li, Yuan Gao, Jun Song,\\nBo Zheng, et al. 2024. Longdocurl: a comprehensive\\nmultimodal long document benchmark integrating un-\\nderstanding, reasoning, and locating. arXiv preprint\\narXiv:2412.18424.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and',\n",
       " 'derstanding, reasoning, and locating. arXiv preprint\\narXiv:2412.18424.\\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\\nJason Weston. 2024. Chain-of-verification reduces\\nhallucination in large language models. In Findings\\nof the Association for Computational Linguistics ACL\\n2024, pages 3563–3578.\\nAvik Dutta, Mukul Singh, Gust Verbruggen, Sumit Gul-\\nwani, and Vu Le. 2024. Rar: Retrieval-augmented re-\\ntrieval for code generation in low resource languages.\\nIn Proceedings of the 2024 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n21506–21515.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang,\\nHengyun Li, Dawei Yin, Tat-Seng Chua, and Qing\\nLi. 2024a. A survey on rag meeting llms: Towards\\nretrieval-augmented large language models. In Pro-\\nceedings of the 30th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, pages 6491–\\n6501.\\nYue Fan, Hu Zhang, Ru Li, Yujie Wang, Hongye Tan,',\n",
       " 'ceedings of the 30th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, pages 6491–\\n6501.\\nYue Fan, Hu Zhang, Ru Li, Yujie Wang, Hongye Tan,\\nand Jiye Liang. 2024b. Frva: Fact-retrieval and ver-\\nification augmented entailment tree generation for\\nexplainable question answering. In Findings of the\\nAssociation for Computational Linguistics ACL 2024,\\npages 9111–9128.\\nJinyuan Fang, Zaiqiao Meng, and Craig Macdonald.\\n2024. Trace the evidence: Constructing knowledge-\\ngrounded reasoning chains for retrieval-augmented\\ngeneration. In Findings of the Association for Com-\\nputational Linguistics: EMNLP 2024, pages 8472–\\n8494.\\nWeizhi Fei, Xueyan Niu, Guoqing Xie, Yanhua Zhang,\\nBo Bai, Lei Deng, and Wei Han. 2024.\\nRe-\\ntrieval meets reasoning: Dynamic in-context edit-\\ning for long-text understanding.\\narXiv preprint\\narXiv:2406.12331.\\nWenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi\\nSong, and Hao Wang. 2025.\\nAirrag:\\nActivat-\\ning intrinsic reasoning for retrieval augmented gen-',\n",
       " 'arXiv preprint\\narXiv:2406.12331.\\nWenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi\\nSong, and Hao Wang. 2025.\\nAirrag:\\nActivat-\\ning intrinsic reasoning for retrieval augmented gen-\\neration via tree-based search.\\narXiv preprint\\narXiv:2501.10053.\\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi,\\nTushar Khot, and Pradeep Dasigi. 2020.\\nIirc: A\\ndataset of incomplete information reading compre-\\nhension questions. In Proceedings of the 2020 Con-\\nference on Empirical Methods in Natural Language\\nProcessing (EMNLP), pages 1137–1147.\\nZafeirios Fountas, Martin A Benfeghoul, Adnan Oomer-\\njee, Fenia Christopoulou, Gerasimos Lampouras,\\nHaitham Bou-Ammar, and Jun Wang. 2024. Human-\\nlike episodic memory for infinite context llms. arXiv\\npreprint arXiv:2407.09450.\\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\\nKelvin Guu. 2023a. RARR: Researching and revis-\\ning what language models say, using language mod-',\n",
       " 'Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\\nKelvin Guu. 2023a. RARR: Researching and revis-\\ning what language models say, using language mod-\\nels. In Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1:\\nLong Papers), pages 16477–16508, Toronto, Canada.\\nAssociation for Computational Linguistics.\\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang\\nJia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun,\\nHaofen Wang, and Haofen Wang. 2023b. Retrieval-\\naugmented generation for large language models: A\\nsurvey. arXiv preprint arXiv:2312.10997, 2:1.\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\\nDan Roth, and Jonathan Berant. 2021. Did aristotle\\nuse a laptop? a question answering benchmark with\\nimplicit reasoning strategies. Transactions of the\\nAssociation for Computational Linguistics, 9:346–\\n361.\\nXinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin,\\nYaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and',\n",
       " 'Association for Computational Linguistics, 9:346–\\n361.\\nXinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin,\\nYaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and\\nJie Zhou. 2025. Deeprag: Thinking to retrieval step\\nby step for large language models. arXiv preprint\\narXiv:2502.01142.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and\\nChao Huang. 2024.\\nLightrag: Simple and fast\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2410.05779.\\nSiwei Han, Peng Xia, Ruiyi Zhang, Tong Sun, Yun Li,\\nHongtu Zhu, and Huaxiu Yao. 2025. Mdocagent: A\\nmulti-modal multi-agent framework for document\\nunderstanding. arXiv preprint arXiv:2503.13964.\\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu.\\n2023.\\nToolkengpt: Augmenting frozen language\\nmodels with massive tools via tool embeddings. In\\nAdvances in Neural Information Processing Systems,\\nvolume 36, pages 45870–45894.\\nBolei He, Nuo Chen, Xinran He, Lingyong Yan,\\nZhenkai Wei, Jinchang Luo, and Zhen-Hua Ling.\\n11',\n",
       " '2024a.\\nRetrieving, rethinking and revising: The\\nchain-of-verification can improve retrieval aug-\\nmented generation. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2024, pages\\n10371–10393.\\nJie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Z\\nPan. 2024b. Mintqa: A multi-hop question answer-\\ning benchmark for evaluating llms on new and tail\\nknowledge. arXiv preprint arXiv:2412.17032.\\nXiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla,\\nThomas Laurent, Yann LeCun, Xavier Bresson,\\nand Bryan Hooi. 2024c.\\nG-retriever: Retrieval-\\naugmented generation for textual graph understand-\\ning and question answering. Advances in Neural\\nInformation Processing Systems, 37:132876–132907.\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\nArora, Steven Basart, Eric Tang, Dawn Song, and\\nJacob Steinhardt. 2021. Measuring mathematical\\nproblem solving with the MATH dataset. In Thirty-\\nfifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.',\n",
       " 'Jacob Steinhardt. 2021. Measuring mathematical\\nproblem solving with the MATH dataset. In Thirty-\\nfifth Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.\\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\\nand Akiko Aizawa. 2020. Constructing a multi-hop\\nqa dataset for comprehensive evaluation of reasoning\\nsteps. In Proceedings of the 28th International Con-\\nference on Computational Linguistics, pages 6609–\\n6625.\\nMinda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou,\\nJingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li,\\nand Irwin King. 2024. Serts: Self-rewarding tree\\nsearch for biomedical retrieval-augmented genera-\\ntion. arXiv preprint arXiv:2406.11258.\\nYunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan.\\n2025. Mcts-rag: Enhancing retrieval-augmented gen-\\neration with monte carlo tree search. arXiv preprint\\narXiv:2503.20757.\\nJerry Huang, Siddarth Madala, Risham Sidhu, Cheng\\nNiu, Julia Hockenmaier, and Tong Zhang. 2025a.\\nRag-rl: Advancing retrieval-augmented generation',\n",
       " 'arXiv:2503.20757.\\nJerry Huang, Siddarth Madala, Risham Sidhu, Cheng\\nNiu, Julia Hockenmaier, and Tong Zhang. 2025a.\\nRag-rl: Advancing retrieval-augmented generation\\nvia rl and curriculum learning.\\narXiv preprint\\narXiv:2503.12759.\\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\\nZhangyin Feng, Haotian Wang, Qianglong Chen,\\nWeihua Peng, Xiaocheng Feng, Bing Qin, et al.\\n2025b. A survey on hallucination in large language\\nmodels: Principles, taxonomy, challenges, and open\\nquestions. ACM Transactions on Information Sys-\\ntems, 43(2):1–55.\\nXiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie\\nJin, Yi Dong, Changshun Wu, Saddek Bensalem,\\nRonghui Mu, Yi Qi, Xingyu Zhao, et al. 2024. A sur-\\nvey of safety and trustworthiness of large language\\nmodels through the lens of verification and validation.\\nArtificial Intelligence Review, 57(7):175.\\nYulong Hui, Yao Lu, and Huanchen Zhang. 2024. Uda:\\nA benchmark suite for retrieval augmented genera-\\ntion in real-world document analysis. In The Thirty-',\n",
       " 'Artificial Intelligence Review, 57(7):175.\\nYulong Hui, Yao Lu, and Huanchen Zhang. 2024. Uda:\\nA benchmark suite for retrieval augmented genera-\\ntion in real-world document analysis. In The Thirty-\\neight Conference on Neural Information Processing\\nSystems Datasets and Benchmarks Track.\\nMichael Iannelli, Sneha Kuchipudi, and Vera Dvorak.\\n2024. Sla management in reconfigurable multi-agent\\nrag: A systems approach to question answering.\\narXiv preprint arXiv:2412.06832.\\nShayekh Islam, Md Asib Rahman, KSM Tozammel Hos-\\nsain, Enamul Hoque, Shafiq Joty, and Md Rizwan\\nParvez. 2024. Open-rag: Enhanced retrieval aug-\\nmented reasoning with open-source large language\\nmodels. In Findings of the Association for Compu-\\ntational Linguistics: EMNLP 2024, pages 14231–\\n14244.\\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia\\nYan, Tianjun Zhang, Sida Wang, Armando Solar-\\nLezama, Koushik Sen, and Ion Stoica. 2024. Live-\\ncodebench: Holistic and contamination free eval-',\n",
       " 'Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia\\nYan, Tianjun Zhang, Sida Wang, Armando Solar-\\nLezama, Koushik Sen, and Ion Stoica. 2024. Live-\\ncodebench: Holistic and contamination free eval-\\nuation of large language models for code. arXiv\\npreprint arXiv:2403.07974.\\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\\ning to adapt retrieval-augmented large language mod-\\nels through question complexity. In Proceedings of\\nthe 2024 Conference of the North American Chap-\\nter of the Association for Computational Linguistics:\\nHuman Language Technologies (Volume 1: Long Pa-\\npers), pages 7029–7043.\\nYixin Ji, Kaixin Wu, Juntao Li, Wei Chen, Mingjie\\nZhong, Xu Jia, and Min Zhang. 2024. Retrieval and\\nreasoning on kgs: Integrate knowledge graphs into\\nlarge language models for complex question answer-\\ning. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 7598–7610.\\nMingyi Jia, Junwen Duan, Yan Song, and Jianxin Wang.',\n",
       " 'ing. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2024, pages 7598–7610.\\nMingyi Jia, Junwen Duan, Yan Song, and Jianxin Wang.\\n2025. Find: Fine-grained information density guided\\nadaptive retrieval-augmented generation for disease\\ndiagnosis. arXiv preprint arXiv:2502.14614.\\nJinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie\\nWang, Wayne Xin Zhao, Yang Song, and Tao Zhang.\\n2024. Rag-star: Enhancing deliberative reasoning\\nwith retrieval augmented verification and refinement.\\narXiv preprint arXiv:2412.12881.\\nPengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu\\nTian, SeongKu Kang, Zifeng Wang, Jimeng Sun,\\nand Jiawei Han. 2025. Deepretrieval: Hacking real\\nsearch engines and retrievers with large language\\nmodels via reinforcement learning. arXiv preprint\\narXiv:2503.00223.\\nBowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar\\nRoy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang,\\nSuhang Wang, Yu Meng, et al. 2024. Graph chain-\\nof-thought: Augmenting large language models by',\n",
       " 'Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar\\nRoy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang,\\nSuhang Wang, Yu Meng, et al. 2024. Graph chain-\\nof-thought: Augmenting large language models by\\nreasoning on graphs. In Findings of the Association\\nfor Computational Linguistics ACL 2024, pages 163–\\n184.\\n12',\n",
       " 'Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang,\\nHamed Zamani, and Jiawei Han. 2025.\\nSearch-\\nr1: Training llms to reason and leverage search en-\\ngines with reinforcement learning. arXiv preprint\\narXiv:2503.09516.\\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\\nZettlemoyer. 2017. Triviaqa: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 1601–1611.\\nTomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou,\\nJayashree Karlekar, Sugiri Pranata, Akira Kinose,\\nKoki Oguri, Felix Wick, and Yang You. 2024. Rap:\\nRetrieval-augmented planning with contextual mem-\\nory for multimodal llm agents.\\narXiv preprint\\narXiv:2402.03610.\\nMohammed Khaliq, Paul Chang, Mingyang Ma, Bern-\\nhard Pflugfelder, and Filip Mileti´c. 2024.\\nRagar,\\nyour falsehood radar: Rag-augmented reasoning for\\npolitical fact-checking using multimodal large lan-',\n",
       " 'hard Pflugfelder, and Filip Mileti´c. 2024.\\nRagar,\\nyour falsehood radar: Rag-augmented reasoning for\\npolitical fact-checking using multimodal large lan-\\nguage models. In Proceedings of the Seventh Fact Ex-\\ntraction and VERification Workshop (FEVER), pages\\n280–296.\\nGangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joon-\\nsuk Park, and Jaewoo Kang. 2023. Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-\\naugmented large language models. In Proceedings\\nof the 2023 Conference on Empirical Methods in\\nNatural Language Processing, pages 996–1009.\\nNeema Kotonya and Francesca Toni. 2020. Explainable\\nautomated fact-checking for public health claims. In\\nProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP),\\npages 7740–7754.\\nHeiko Koziolek, Sten Grüner, Rhaban Hark, Viren-\\ndra Ashiwal, Sofia Linsbauer, and Nafise Eskandani.\\n2024. Llm-based and retrieval-augmented control\\ncode generation. In Proceedings of the 1st Inter-',\n",
       " 'Heiko Koziolek, Sten Grüner, Rhaban Hark, Viren-\\ndra Ashiwal, Sofia Linsbauer, and Nafise Eskandani.\\n2024. Llm-based and retrieval-augmented control\\ncode generation. In Proceedings of the 1st Inter-\\nnational Workshop on Large Language Models for\\nCode, pages 22–29.\\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mo-\\nhananey, Steven Schwarcz, Adam Stambler, Shyam\\nUpadhyay, and Manaal Faruqui. 2024.\\nFact,\\nfetch,\\nand reason:\\nA unified evaluation of\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2409.12941.\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, et al. 2019. Natural questions: a benchmark\\nfor question answering research. Transactions of the\\nAssociation for Computational Linguistics, 7:453–\\n466.\\nSung-Min Lee, Eunhwan Park, Donghyeon Jeon, Inho\\nKang, and Seung-Hoon Na. 2024. Radcot: Retrieval-\\naugmented distillation to specialization models for',\n",
       " '466.\\nSung-Min Lee, Eunhwan Park, Donghyeon Jeon, Inho\\nKang, and Seung-Hoon Na. 2024. Radcot: Retrieval-\\naugmented distillation to specialization models for\\ngenerating chain-of-thoughts in query expansion. In\\nProceedings of the 2024 Joint International Con-\\nference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING 2024),\\npages 13514–13523.\\nZhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang,\\nWeichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi\\nLi. 2025. Rearag: Knowledge-guided reasoning en-\\nhances factuality of large reasoning models with iter-\\native retrieval augmented generation. arXiv preprint\\narXiv:2503.21729.\\nDawei Li, Shu Yang, Zhen Tan, Jae Baik, Sukwon Yun,\\nJoseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-\\nTran, Ying Ding, et al. 2024a. Dalk: Dynamic co-\\naugmentation of llms and kg to answer alzheimer’s\\ndisease questions with scientific literature. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2024, pages 2187–2205.',\n",
       " 'augmentation of llms and kg to answer alzheimer’s\\ndisease questions with scientific literature. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2024, pages 2187–2205.\\nGuanghua Li, Wensheng Lu, Wei Zhang, Defu Lian,\\nKezhong Lu, Rui Mao, Kai Shu, and Hao Liao.\\n2024b. Re-search for the truth: Multi-round retrieval-\\naugmented large language models are strong fake\\nnews detectors. arXiv preprint arXiv:2403.09747.\\nGuozheng Li, Peng Wang, Wenjun Ke, Yikai Guo, Ke Ji,\\nZiyu Shang, Jiajun Liu, and Zijie Xu. 2024c. Recall,\\nretrieve and reason: towards better in-context relation\\nextraction. In Proceedings of the Thirty-Third Inter-\\nnational Joint Conference on Artificial Intelligence,\\npages 6368–6376.\\nHuayang Li, Pat Verga, Priyanka Sen, Bowen Yang,\\nVijay Viswanathan, Patrick Lewis, Taro Watanabe,\\nand Yixuan Su. 2024d.\\nAlr2:\\nA retrieve-then-\\nreason framework for long-context question answer-\\ning. arXiv preprint arXiv:2410.03227.',\n",
       " 'Vijay Viswanathan, Patrick Lewis, Taro Watanabe,\\nand Yixuan Su. 2024d.\\nAlr2:\\nA retrieve-then-\\nreason framework for long-context question answer-\\ning. arXiv preprint arXiv:2410.03227.\\nJia Li, Xianjie Shi, Kechi Zhang, Lei Li, Ge Li, Zheng-\\nwei Tao, Fang Liu, Chongyang Tao, and Zhi Jin.\\n2025a. Coderag: Supportive code retrieval on bi-\\ngraph for real-world code generation. arXiv preprint\\narXiv:2504.10046.\\nMinghan Li, Honglei Zhuang, Kai Hui, Zhen Qin,\\nJimmy Lin, Rolf Jagerman, Xuanhui Wang, and\\nMichael Bendersky. 2024e. Can query expansion im-\\nprove generalization of strong cross-encoder rankers?\\nIn Proceedings of the 47th International ACM SIGIR\\nConference on Research and Development in Infor-\\nmation Retrieval, pages 2321–2326.\\nShilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu,\\nGe Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yang-\\nguang Li, Wanli Ouyang, et al. 2024f. Graphreader:\\nBuilding graph-based agent to enhance long-context\\nabilities of large language models. In Findings of the',\n",
       " 'guang Li, Wanli Ouyang, et al. 2024f. Graphreader:\\nBuilding graph-based agent to enhance long-context\\nabilities of large language models. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2024, pages 12758–12786.\\nXiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang,\\nYujia Zhou,\\nYutao Zhu,\\nPeitian Zhang,\\nand\\nZhicheng Dou. 2025b. Search-o1: Agentic search-\\nenhanced large reasoning models. arXiv preprint\\narXiv:2501.05366.\\n13',\n",
       " 'Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yu-\\ntao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng\\nDou. 2025c. Webthinker: Empowering large rea-\\nsoning models with deep research capability. arXiv\\npreprint arXiv:2504.21776.\\nYangning Li, Yinghui Li, Xinyu Wang, Yong Jiang,\\nZhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao\\nZheng, Fei Huang, Jingren Zhou, and Philip S. Yu.\\n2025d.\\nBenchmarking multimodal retrieval aug-\\nmented generation with dynamic VQA dataset and\\nself-adaptive planning agent. In The Thirteenth Inter-\\nnational Conference on Learning Representations.\\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\\nCao, and Shuzi Niu. 2017. Dailydialog: A manually\\nlabelled multi-turn dialogue dataset. arXiv preprint\\narXiv:1710.03957.\\nZhi Li, Yicheng Li, Hequan Ye, and Yin Zhang. 2024g.\\nTowards autonomous tool utilization in language\\nmodels: A unified, efficient and scalable frame-\\nwork. In Proceedings of the 2024 Joint International\\nConference on Computational Linguistics, Language',\n",
       " 'Towards autonomous tool utilization in language\\nmodels: A unified, efficient and scalable frame-\\nwork. In Proceedings of the 2024 Joint International\\nConference on Computational Linguistics, Language\\nResources and Evaluation (LREC-COLING 2024),\\npages 16422–16432.\\nZhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Ji-\\naxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian\\nXu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al.\\n2025e. From system 1 to system 2: A survey of\\nreasoning large language models.\\narXiv preprint\\narXiv:2502.17419.\\nZhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu\\nLin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xian-\\npei Han, Le Sun, and Yongbin Li. 2024h. Struc-\\ntrag: Boosting knowledge intensive reasoning of llms\\nvia inference-time hybrid information structurization.\\narXiv preprint arXiv:2410.08815.\\nZijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang,\\nQi Wang, Qiang Fu, and Ke Liu. 2024. A survey of\\nmultimodel large language models. In Proceedings\\nof the 3rd International Conference on Computer,',\n",
       " 'Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang,\\nQi Wang, Qiang Fu, and Ke Liu. 2024. A survey of\\nmultimodel large language models. In Proceedings\\nof the 3rd International Conference on Computer,\\nArtificial Intelligence and Control Engineering, pages\\n405–409.\\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\\nMaria Lomeli, Richard James, Pedro Rodriguez, Ja-\\ncob Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023.\\nRa-dit: Retrieval-augmented dual instruction tuning.\\nIn The Twelfth International Conference on Learning\\nRepresentations.\\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\\nsom. 2017. Program induction by rationale genera-\\ntion: Learning to solve and explain algebraic word\\nproblems. arXiv preprint arXiv:1705.04146.\\nAiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming\\nHu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong,\\nand Philip Yu. 2024. A survey of text watermarking\\nin the era of large language models. ACM Computing\\nSurveys, 57(2):1–36.',\n",
       " 'Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong,\\nand Philip Yu. 2024. A survey of text watermarking\\nin the era of large language models. ACM Computing\\nSurveys, 57(2):1–36.\\nPei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan\\nMeng, Ding Wang, and Jun Ma. 2025. Hm-rag: Hier-\\narchical multi-agent multimodal retrieval augmented\\ngeneration. arXiv preprint arXiv:2504.12330.\\nChang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao\\nZeng, Zhu Zhuo, Evangelos B Mazomenos, and\\nYueming Jin. 2025. Surgraw: Multi-agent workflow\\nwith chain-of-thought reasoning for surgical intelli-\\ngence. arXiv preprint arXiv:2503.10265.\\nChris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foer-\\nster, Jeff Clune, and David Ha. 2024. The ai scientist:\\nTowards fully automated open-ended scientific dis-\\ncovery. arXiv preprint arXiv:2408.06292.\\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:',\n",
       " 'covery. arXiv preprint arXiv:2408.06292.\\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\nClark, and Ashwin Kalyan. 2022. Learn to explain:\\nMultimodal reasoning via thought chains for science\\nquestion answering. Advances in Neural Information\\nProcessing Systems, 35:2507–2521.\\nJunyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Jun-\\nwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue\\nQiao, Qingqing Long, et al. 2025a. Large language\\nmodel agent: A survey on methodology, applications\\nand challenges. arXiv preprint arXiv:2503.21460.\\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasu-\\npat, Mehran Kazemi, Chitta Baral, Vaiva Im-\\nbrasaite, and Vincent Y Zhao. 2023.\\nDr. icl:\\nDemonstration-retrieved in-context learning. arXiv\\npreprint arXiv:2305.14128.\\nNe Luo, Aryo Pradipta Gema, Xuanli He, Emile\\nvan Krieken, Pietro Lesci, and Pasquale Minervini.\\n2025b.\\nSelf-training large language models for\\ntool-use without demonstrations.\\narXiv preprint',\n",
       " 'Ne Luo, Aryo Pradipta Gema, Xuanli He, Emile\\nvan Krieken, Pietro Lesci, and Pasquale Minervini.\\n2025b.\\nSelf-training large language models for\\ntool-use without demonstrations.\\narXiv preprint\\narXiv:2502.05867.\\nShengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li,\\nHuaren Qu, Cehao Yang, Jiaxin Mao, and Jian Guo.\\n2024a. Think-on-graph 2.0: Deep and faithful large\\nlanguage model reasoning with knowledge-guided\\nretrieval augmented generation.\\narXiv preprint\\narXiv:2407.10805.\\nYubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu,\\nShuohang Wang, Liangming Pan, Yujiu Yang, Yixin\\nCao, and Aixin Sun. 2024b.\\nSciagent:\\nTool-\\naugmented language models for scientific reasoning.\\nIn Proceedings of the 2024 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n15701–15736.\\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen,\\nYizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. 2025. Mmlongbench-doc:\\nBenchmarking long-context document understanding',\n",
       " 'Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen,\\nYizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. 2025. Mmlongbench-doc:\\nBenchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information\\nProcessing Systems, 37:95963–96010.\\nKelong Mao, Zheng Liu, Hongjin Qian, Fengran Mo,\\nChenlong Deng, and Zhicheng Dou. 2024.\\nRag-\\nstudio: Towards in-domain adaptation of retrieval\\n14',\n",
       " 'augmented generation through self-alignment. In\\nFindings of the Association for Computational Lin-\\nguistics: EMNLP 2024, pages 725–735.\\nMaria Marina, Nikolay Ivanov, Sergey Pletenev,\\nMikhail Salnikov,\\nDaria Galimzianova,\\nNikita\\nKrayko, Vasily Konovalov, Alexander Panchenko,\\nand Viktor Moskvoretskii. 2025. Llm-independent\\nadaptive rag: Let the question speak for itself. arXiv\\npreprint arXiv:2505.04253.\\nCostas Mavromatis and George Karypis. 2024. Gnn-\\nrag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139.\\nGrégoire Mialon, Clémentine Fourrier, Thomas Wolf,\\nYann LeCun, and Thomas Scialom. 2023. Gaia: a\\nbenchmark for general ai assistants. In The Twelfth\\nInternational Conference on Learning Representa-\\ntions.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021.\\nWebgpt: Browser-assisted question-\\nanswering with human feedback.',\n",
       " 'Long Ouyang, Christina Kim, Christopher Hesse,\\nShantanu Jain, Vineet Kosaraju, William Saunders,\\net al. 2021.\\nWebgpt: Browser-assisted question-\\nanswering with human feedback.\\narXiv preprint\\narXiv:2112.09332.\\nShashi Narayan, Shay B Cohen, and Mirella Lapata.\\n2018. Don’t give me the details, just the summary!\\ntopic-aware convolutional neural networks for ex-\\ntreme summarization. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 1797–1807.\\nXuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam,\\nAustin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Sil-\\nvio Savarese, Caiming Xong, and Shafiq Joty. 2024.\\nSfr-rag: Towards contextually faithful llms. arXiv\\npreprint arXiv:2409.09916.\\nCheng Niu, Yang Guan, Yuanhao Wu, Juno Zhu, Jun-\\ntong Song, Randy Zhong, Kaihua Zhu, Siliang Xu,\\nShizhe Diao, and Tong Zhang. 2024. Veract scan:\\nRetrieval-augmented fake news detection with justifi-\\nable reasoning. In Proceedings of the 62nd Annual',\n",
       " 'tong Song, Randy Zhong, Kaihua Zhu, Siliang Xu,\\nShizhe Diao, and Tong Zhang. 2024. Veract scan:\\nRetrieval-augmented fake news detection with justifi-\\nable reasoning. In Proceedings of the 62nd Annual\\nMeeting of the Association for Computational Lin-\\nguistics (Volume 3: System Demonstrations), pages\\n266–277.\\nYasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and\\nGreg Durrett. 2021. Creak: A dataset for common-\\nsense reasoning over entity knowledge. OpenReview.\\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\\nPadmakumar, Johnny Ma, Jana Thompson, He He,\\net al. 2022. Quality: Question answering with long\\ninput texts, yes! In Proceedings of the 2022 Con-\\nference of the North American Chapter of the Asso-\\nciation for Computational Linguistics: Human Lan-\\nguage Technologies, pages 5336–5358.\\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li,\\nJosephina Hu, Hugh Zhang, Chen Bo Calvin Zhang,\\nMohamed Shaaban, John Ling, Sean Shi, et al.\\n2025.',\n",
       " 'guage Technologies, pages 5336–5358.\\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li,\\nJosephina Hu, Hugh Zhang, Chen Bo Calvin Zhang,\\nMohamed Shaaban, John Ling, Sean Shi, et al.\\n2025.\\nHumanity’s last exam.\\narXiv preprint\\narXiv:2501.14249.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\\nNoah A Smith, and Mike Lewis. 2023. Measuring\\nand narrowing the compositionality gap in language\\nmodels. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2023, pages 5687–5711.\\nShuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai\\nJia, Huajun Chen, and Ningyu Zhang. 2024. Making\\nlanguage models better tool learners with execution\\nfeedback. In Proceedings of the 2024 Conference\\nof the North American Chapter of the Association\\nfor Computational Linguistics: Human Language\\nTechnologies (Volume 1: Long Papers), pages 3550–\\n3568.\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\\nBill Qian, et al. 2023. Toolllm: Facilitating large',\n",
       " '3568.\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\\nBill Qian, et al. 2023. Toolllm: Facilitating large\\nlanguage models to master 16000+ real-world apis.\\narXiv preprint arXiv:2307.16789.\\nLeonardo Ranaldi, Marco Valentino, and Andrè Fre-\\nitas. 2024. Eliciting critical reasoning in retrieval-\\naugmented language models via contrastive explana-\\ntions. arXiv preprint arXiv:2410.22874.\\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jack-\\nson Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-\\nlian Michael, and Samuel R Bowman. 2024. Gpqa:\\nA graduate-level google-proof q&a benchmark. In\\nFirst Conference on Language Modeling.\\nAniruddha Salve, Saba Attar, Mahesh Deshmukh, Say-\\nali Shivpuje, and Arnab Mitra Utsab. 2024. A collab-\\norative multi-agent approach to retrieval-augmented\\ngeneration across diverse data.\\narXiv preprint\\narXiv:2412.05838.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta',\n",
       " 'orative multi-agent approach to retrieval-augmented\\ngeneration across diverse data.\\narXiv preprint\\narXiv:2412.05838.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\\nmoyer, Nicola Cancedda, and Thomas Scialom. 2023.\\nToolformer: Language models can teach themselves\\nto use tools. Advances in Neural Information Pro-\\ncessing Systems, 36:68539–68551.\\nSamuel Schmidgall, Yusheng Su, Ze Wang, Ximeng\\nSun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng\\nLiu, and Emad Barsoum. 2025.\\nAgent labora-\\ntory: Using llm agents as research assistants. arXiv\\npreprint arXiv:2501.04227.\\nThomas Schmied, Fabian Paischer, Vihang Patil,\\nMarkus Hofmarcher, Razvan Pascanu, and Sepp\\nHochreiter. 2024.\\nRetrieval-augmented decision\\ntransformer:\\nExternal memory for in-context rl.\\narXiv preprint arXiv:2410.07071.\\nJohannes Schneider. 2025. Generative to agentic ai:\\nSurvey, conceptualization, and challenges. arXiv\\npreprint arXiv:2504.18875.\\n15',\n",
       " 'Eva Sharma, Chen Li, and Lu Wang. 2019. Bigpatent:\\nA large-scale dataset for abstractive and coherent\\nsummarization. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Lin-\\nguistics, pages 2204–2213.\\nOla Shorinwa, Zhiting Mei, Justin Lidard, Allen Z Ren,\\nand Anirudha Majumdar. 2025. A survey on un-\\ncertainty quantification of large language models:\\nTaxonomy, open research challenges, and future di-\\nrections. ACM Computing Surveys.\\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote,\\nYonatan Bisk,\\nAdam Trischler,\\nand Matthew\\nHausknecht. Alfworld: Aligning text and embod-\\nied environments for interactive learning. In Interna-\\ntional Conference on Learning Representations.\\nHuatong Song, Jinhao Jiang, Yingqian Min, Jie Chen,\\nZhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-\\nRong Wen. 2025. R1-searcher: Incentivizing the\\nsearch capability in llms via reinforcement learning.\\narXiv preprint arXiv:2503.05592.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,',\n",
       " 'Rong Wen. 2025. R1-searcher: Incentivizing the\\nsearch capability in llms via reinforcement learning.\\narXiv preprint arXiv:2503.05592.\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\\nAdam R Brown, Adam Santoro, Aditya Gupta,\\nAdrià Garriga-Alonso, et al. 2022.\\nBeyond the\\nimitation game: Quantifying and extrapolating the\\ncapabilities of language models.\\narXiv preprint\\narXiv:2206.04615.\\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu\\nZhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, An-\\ndrew Wen, Shaochen Zhong, Hanjie Chen, et al.\\n2025. Stop overthinking: A survey on efficient rea-\\nsoning for large language models. arXiv preprint\\narXiv:2503.16419.\\nChuanneng Sun, Songjun Huang, and Dario Pompili.\\n2024a. Retrieval-augmented hierarchical in-context\\nreinforcement learning and hindsight modular reflec-\\ntions for task planning with llms. arXiv preprint\\narXiv:2408.06520.\\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.',\n",
       " 'reinforcement learning and hindsight modular reflec-\\ntions for task planning with llms. arXiv preprint\\narXiv:2408.06520.\\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.\\n2019. Pullnet: Open domain question answering\\nwith iterative retrieval on knowledge bases and text.\\nIn Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th\\nInternational Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP), pages 2380–2390.\\nHao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan\\nHou, Yong Jiang, Pengjun Xie, Fei Huang, and Yan\\nZhang. 2025a. Zerosearch: Incentivize the search\\ncapability of llms without searching. arXiv preprint\\narXiv:2505.04588.\\nJiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo\\nWang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-\\nYeung Shum, and Jian Guo. 2024b. Think-on-graph:\\nDeep and responsible reasoning of large language\\nmodel on knowledge graph. In The Twelfth Interna-\\ntional Conference on Learning Representations.',\n",
       " 'Yeung Shum, and Jian Guo. 2024b. Think-on-graph:\\nDeep and responsible reasoning of large language\\nmodel on knowledge graph. In The Twelfth Interna-\\ntional Conference on Learning Representations.\\nQiang Sun, Tingting Bi, Sirui Li, Eun-Jung Holden,\\nPaul Duuring, Kai Niu, and Wei Liu. 2025b. Sym-\\nbioticrag: Enhancing document intelligence through\\nhuman-llm symbiotic collaboration. arXiv preprint\\narXiv:2505.02418.\\nZhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue\\nZang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang,\\nand Han Li. 2025c. Rearter: Retrieval-augmented\\nreasoning with trustworthy process rewarding. arXiv\\npreprint arXiv:2501.07861.\\nAlon Talmor and Jonathan Berant. 2018. The web as\\na knowledge-base for answering complex questions.\\nIn Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 641–651.\\nYixuan Tang and Yi Yang. 2024. Multihop-rag: Bench-',\n",
       " 'American Chapter of the Association for Computa-\\ntional Linguistics: Human Language Technologies,\\nVolume 1 (Long Papers), pages 641–651.\\nYixuan Tang and Yi Yang. 2024. Multihop-rag: Bench-\\nmarking retrieval-augmented generation for multi-\\nhop queries. arXiv preprint arXiv:2401.15391.\\nYicheng Tao, Haotian Liu, Shanwen Wang, and\\nHongteng Xu. 2025. Assisting mathematical for-\\nmalization with a learning-based premise retriever.\\narXiv preprint arXiv:2501.13959.\\nJames\\nThorne,\\nAndreas\\nVlachos,\\nChristos\\nChristodoulopoulos,\\nand\\nArpit\\nMittal.\\n2018.\\nFever: a large-scale dataset for fact extraction and\\nverification. arXiv preprint arXiv:1803.05355.\\nSM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vip-\\nula Rawte, Aman Chadha, and Amitava Das. 2024.\\nA comprehensive survey of hallucination mitigation\\ntechniques in large language models. arXiv preprint\\narXiv:2401.01313.\\nHieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang,\\nZhichao Yang, and Hong Yu. 2024. Rare: Retrieval-',\n",
       " 'techniques in large language models. arXiv preprint\\narXiv:2401.01313.\\nHieu Tran, Zonghai Yao, Junda Wang, Yifan Zhang,\\nZhichao Yang, and Hong Yu. 2024. Rare: Retrieval-\\naugmented reasoning enhancement for large lan-\\nguage models. arXiv preprint arXiv:2412.02830.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2022.\\nmusique: Multi-\\nhop questions via single-hop question composition.\\nTransactions of the Association for Computational\\nLinguistics, 10:539–554.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\\nand Ashish Sabharwal. 2023. Interleaving retrieval\\nwith chain-of-thought reasoning for knowledge-\\nintensive multi-step questions. In Proceedings of the\\n61st Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n10014–10037.\\nNikolaos Tsantalis, Ameya Ketkar, and Danny Dig.\\n2020. Refactoringminer 2.0. IEEE Transactions\\non Software Engineering, 48(3):930–950.\\nBoxin Wang, Wei Ping, Lawrence Mcafee, Peng Xu,',\n",
       " '10014–10037.\\nNikolaos Tsantalis, Ameya Ketkar, and Danny Dig.\\n2020. Refactoringminer 2.0. IEEE Transactions\\non Software Engineering, 48(3):930–950.\\nBoxin Wang, Wei Ping, Lawrence Mcafee, Peng Xu,\\nBo Li, Mohammad Shoeybi, and Bryan Catanzaro.\\n2024a. Instructretro: Instruction tuning post retrieval-\\naugmented pretraining. In International Conference\\non Machine Learning, pages 51255–51272. PMLR.\\n16',\n",
       " 'Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao\\nGao, Kexin Huang, Ziming Liu, Payal Chandak,\\nShengchao Liu, Peter Van Katwyk, Andreea Deac,\\net al. 2023. Scientific discovery in the age of artificial\\nintelligence. Nature, 620(7972):47–60.\\nJunjie Wang, Mingyang Chen, Binbin Hu, Dan Yang,\\nZiqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jin-\\njie Gu, Jun Zhou, et al. 2024b. Learning to plan\\nfor retrieval-augmented large language models from\\nknowledge graphs. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2024, pages\\n7813–7835.\\nSong Wang, Zihan Chen, Chengshuai Shi, Cong Shen,\\nand Jundong Li. 2024c. Mixture of demonstrations\\nfor in-context learning. Advances in Neural Informa-\\ntion Processing Systems, 37:88091–88116.\\nYaoting Wang, Shengqiong Wu, Yuecheng Zhang,\\nShuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao\\nFei. 2025a.\\nMultimodal chain-of-thought reason-\\ning:\\nA comprehensive survey.\\narXiv preprint\\narXiv:2503.12605.\\nYu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi',\n",
       " 'Fei. 2025a.\\nMultimodal chain-of-thought reason-\\ning:\\nA comprehensive survey.\\narXiv preprint\\narXiv:2503.12605.\\nYu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi\\nZhang, and Tyler Derr. 2024d. Knowledge graph\\nprompting for multi-document question answering.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 38, pages 19206–19214.\\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,\\nAbhranil Chandra, Shiguang Guo, Weiming Ren,\\nAaran Arulraj, Xuan He, Ziyan Jiang, et al. 2025b.\\nMmlu-pro: A more robust and challenging multi-task\\nlanguage understanding benchmark. Advances in\\nNeural Information Processing Systems, 37:95266–\\n95290.\\nYujing Wang, Hainan Zhang, Liang Pang, Binghui\\nGuo, Hongwei Zheng, and Zhiming Zheng. 2025c.\\nMaferw: Query rewriting with multi-aspect feed-\\nbacks for retrieval-augmented large language models.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 39, pages 25434–25442.\\nZheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and',\n",
       " 'backs for retrieval-augmented large language models.\\nIn Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 39, pages 25434–25442.\\nZheng Wang, Shu Teo, Jieer Ouyang, Yongjun Xu, and\\nWei Shi. 2024e. M-rag: Reinforcing large language\\nmodel performance through retrieval-augmented gen-\\neration with multiple partitions.\\nIn Proceedings\\nof the 62nd Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers),\\npages 1966–1978.\\nZhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen,\\nYu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang,\\nLinpeng Tang, Wentao Zhang, et al. 2025d. Rare:\\nRetrieval-augmented reasoning modeling.\\narXiv\\npreprint arXiv:2503.23513.\\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jin-\\nbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\\nHe, Zilong Zheng, Yaodong Yang, et al. 2024f.\\nJarvis-1: Open-world multi-task agents with memory-\\naugmented multimodal language models.\\nIEEE\\nTransactions on Pattern Analysis and Machine In-\\ntelligence.',\n",
       " 'Jarvis-1: Open-world multi-task agents with memory-\\naugmented multimodal language models.\\nIEEE\\nTransactions on Pattern Analysis and Machine In-\\ntelligence.\\nZihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xi-\\naojian Ma, and Yitao Liang. 2024g.\\nRat:\\nRe-\\ntrieval augmented thoughts elicit context-aware rea-\\nsoning in long-horizon generation. arXiv preprint\\narXiv:2403.05313.\\nJason Wei, Nguyen Karina, Hyung Won Chung,\\nYunxin Joy Jiao, Spencer Papay, Amelia Glaese, John\\nSchulman, and William Fedus. 2024. Measuring\\nshort-form factuality in large language models. arXiv\\npreprint arXiv:2411.04368.\\nJason Wei, Zhiqing Sun, Spencer Papay, Scott McK-\\ninney, Jeffrey Han, Isa Fulford, Hyung Won Chung,\\nAlex Tachard Passos, William Fedus, and Amelia\\nGlaese. 2025a. Browsecomp: A simple yet challeng-\\ning benchmark for browsing agents. arXiv preprint\\narXiv:2504.12516.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,',\n",
       " 'ing benchmark for browsing agents. arXiv preprint\\narXiv:2504.12516.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\\net al. 2022. Chain-of-thought prompting elicits rea-\\nsoning in large language models. Advances in neural\\ninformation processing systems, 35:24824–24837.\\nJiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie\\nQiu, Wei Wei, Jinzhe Li, Wanli Ouyang, and Siqi\\nSun. 2025b. Alignrag: An adaptable framework for\\nresolving misalignments in retrieval-aware reasoning\\nof rag. arXiv preprint arXiv:2504.14858.\\nZhihua Wen, Zhiliang Tian, Wei Wu, Yuxin Yang, Yanqi\\nShi, Zhen Huang, and Dongsheng Li. 2023. Grove: A\\nretrieval-augmented complex story generation frame-\\nwork with a forest of evidence. In Findings of the\\nAssociation for Computational Linguistics: EMNLP\\n2023, pages 3980–3998.\\nNirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawar-\\ndena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-\\nOrji, Ruvan Weerasinghe, Anne Liret, and Bruno',\n",
       " '2023, pages 3980–3998.\\nNirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawar-\\ndena, Kyle Martin, Stewart Massie, Ikechukwu Nkisi-\\nOrji, Ruvan Weerasinghe, Anne Liret, and Bruno\\nFleisch. 2024. Cbr-rag: case-based reasoning for\\nretrieval augmented generation in llms for legal ques-\\ntion answering. In International Conference on Case-\\nBased Reasoning, pages 445–460. Springer.\\nFeijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding,\\nand Jing Gao. 2025a. Talk to right specialists: Rout-\\ning and planning in multi-agent system for question\\nanswering. arXiv preprint arXiv:2501.07813.\\nJialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang,\\nZekun Xi, Runnan Fang, Linhai Zhang, Yulan He,\\nDeyu Zhou, Pengjun Xie, et al. 2025b. Webwalker:\\nBenchmarking llms in web traversal. arXiv preprint\\narXiv:2501.07572.\\nJunde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025c. Agen-\\ntic reasoning: Reasoning llms with tools for the deep\\nresearch. arXiv preprint arXiv:2502.04644.\\nShirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang,',\n",
       " 'Junde Wu, Jiayuan Zhu, and Yuyuan Liu. 2025c. Agen-\\ntic reasoning: Reasoning llms with tools for the deep\\nresearch. arXiv preprint arXiv:2502.04644.\\nShirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang,\\nMichihiro Yasunaga, Kaidi Cao, Vassilis Ioannidis,\\nKarthik Subbian, Jure Leskovec, and James Y Zou.\\n2024. Avatar: Optimizing llm agents for tool us-\\nage via contrastive reasoning. Advances in Neural\\nInformation Processing Systems, 37:25981–26010.\\n17',\n",
       " 'Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang,\\nand Wenjie Li. 2025a.\\nTokenskip: Controllable\\nchain-of-thought compression in llms. arXiv preprint\\narXiv:2502.12067.\\nYuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, and\\nHaifeng Huang. 2025b.\\nImproving retrieval aug-\\nmented language model with self-reasoning. In Pro-\\nceedings of the AAAI conference on artificial intelli-\\ngence, volume 39, pages 25534–25542.\\nGuangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang,\\nHaolin Liu, Yifan Yang, Fangyuan Chen, Zhix-\\ning Song, Dengyu Wang, Minjia Zhang, et al.\\n2025. Rag-gym: Optimizing reasoning and search\\nagents with process supervision.\\narXiv preprint\\narXiv:2502.13957.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2024. Crp-rag: A retrieval-\\naugmented generation framework for supporting\\ncomplex logical reasoning and knowledge planning.\\nElectronics, 14(1):47.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2025a. Crp-rag: A retrieval-',\n",
       " 'complex logical reasoning and knowledge planning.\\nElectronics, 14(1):47.\\nKehan Xu, Kun Zhang, Jingyuan Li, Wei Huang,\\nand Yuanzhuo Wang. 2025a. Crp-rag: A retrieval-\\naugmented generation framework for supporting\\ncomplex logical reasoning and knowledge planning.\\nElectronics (2079-9292), 14(1).\\nRan Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C\\nHo, Haoyu Wang, and Carl Yang. 2025b. Collab-rag:\\nBoosting retrieval-augmented generation for complex\\nquestion answering via white-box and black-box llm\\ncollaboration. arXiv preprint arXiv:2504.04915.\\nChen Yang, Chenyang Zhao, Quanquan Gu, and Don-\\ngruo Zhou. 2024a. Cops: Empowering llm agents\\nwith provable cross-task experience sharing. arXiv\\npreprint arXiv:2410.16670.\\nRui Yang. 2024. Casegpt: a case reasoning framework\\nbased on language models and retrieval-augmented\\ngeneration. arXiv preprint arXiv:2407.07913.\\nWooseong Yang, Weizhi Zhang, Yuqing Liu, Yuwei\\nHan, Yu Wang, Junhyun Lee, and Philip S Yu. 2025.',\n",
       " 'based on language models and retrieval-augmented\\ngeneration. arXiv preprint arXiv:2407.07913.\\nWooseong Yang, Weizhi Zhang, Yuqing Liu, Yuwei\\nHan, Yu Wang, Junhyun Lee, and Philip S Yu. 2025.\\nCold-start recommendation with knowledge-guided\\nretrieval-augmented generation.\\narXiv preprint\\narXiv:2505.20773.\\nXiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla,\\nXiangsen Chen, Sajal Choudhary, Rongze Gui, Ziran\\nJiang, Ziyu Jiang, et al. 2024b. Crag-comprehensive\\nrag benchmark. Advances in Neural Information\\nProcessing Systems, 37:10470–10490.\\nYahe Yang and Chengyue Huang. 2025. Tree-based\\nrag-agent recommendation system: A case study in\\nmedical test data. arXiv preprint arXiv:2501.02727.\\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\\nWikiqa: A challenge dataset for open-domain ques-\\ntion answering.\\nIn Proceedings of the 2015 con-\\nference on empirical methods in natural language\\nprocessing, pages 2013–2018.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,',\n",
       " 'tion answering.\\nIn Proceedings of the 2015 con-\\nference on empirical methods in natural language\\nprocessing, pages 2013–2018.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\\npher D Manning. 2018. Hotpotqa: A dataset for\\ndiverse, explainable multi-hop question answering.\\nIn Proceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing, pages\\n2369–2380.\\nShunyu Yao, Howard Chen, John Yang, and Karthik\\nNarasimhan. 2022. Webshop: Towards scalable real-\\nworld web interaction with grounded language agents.\\nAdvances in Neural Information Processing Systems,\\n35:20744–20757.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\\nTom Griffiths, Yuan Cao, and Karthik Narasimhan.\\n2023a. Tree of thoughts: Deliberate problem solving\\nwith large language models.\\nAdvances in neural\\ninformation processing systems, 36:11809–11822.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.',\n",
       " 'with large language models.\\nAdvances in neural\\ninformation processing systems, 36:11809–11822.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\\nShafran, Karthik Narasimhan, and Yuan Cao. 2023b.\\nReact: Synergizing reasoning and acting in language\\nmodels. In International Conference on Learning\\nRepresentations (ICLR).\\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\\nPercy Liang, and Jure Leskovec. 2021. Qa-gnn: Rea-\\nsoning with language models and knowledge graphs\\nfor question answering. In North American Chap-\\nter of the Association for Computational Linguistics\\n(NAACL).\\nJaeseok Yoo, Hojae Han, Youngwon Lee, Jaejin Kim,\\nand Seung-won Hwang. 2025. Perc: Plan-as-query\\nexample retrieval for underrepresented code genera-\\ntion. In Proceedings of the 31st International Con-\\nference on Computational Linguistics, pages 7982–\\n7997.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\\nBerant. 2024. Making retrieval-augmented language\\nmodels robust to irrelevant context. In ICLR 2024',\n",
       " '7997.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\\nBerant. 2024. Making retrieval-augmented language\\nmodels robust to irrelevant context. In ICLR 2024\\nWorkshop on Large Language Model (LLM) Agents.\\nHong Qing Yu and Frank McQuade. 2025. Rag-kg-il:\\nA multi-agent hybrid framework for reducing halluci-\\nnations and enhancing llm reasoning through rag and\\nincremental knowledge graph learning integration.\\narXiv preprint arXiv:2503.13514.\\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Peixin\\nCao, Kaixin Ma, Jian Li, Hongwei Wang, and Dong\\nYu. 2024. Chain-of-note: Enhancing robustness in\\nretrieval-augmented language models. In Proceed-\\nings of the 2024 Conference on Empirical Methods in\\nNatural Language Processing, pages 14672–14685.\\nJing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie\\nTang, Cuiping Li, and Hong Chen. 2022a. Subgraph\\nretrieval enhanced model for multi-hop knowledge\\nbase question answering. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational',\n",
       " 'retrieval enhanced model for multi-hop knowledge\\nbase question answering. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers), pages 5773–\\n5784.\\n18',\n",
       " 'Jinghan Zhang, Xiting Wang, Weijieying Ren, Lu Jiang,\\nDongjie Wang, and Kunpeng Liu. 2025a. Ratt: A\\nthought structure for coherent and correct llm reason-\\ning. In Proceedings of the AAAI Conference on Arti-\\nficial Intelligence, volume 39, pages 26733–26741.\\nJintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo,\\nShuofei Qiao, Lun Du, Da Zheng, Huajun Chen,\\nand Ningyu Zhang. 2025b.\\nLightthinker: Think-\\ning step-by-step compression.\\narXiv preprint\\narXiv:2502.15589.\\nMiao Zhang, Zhenlong Fang, Tianyi Wang, Qian Zhang,\\nShuai Lu, Junfeng Jiao, and Tianyu Shi. 2025c. A\\ncascading cooperative multi-agent framework for on-\\nramp merging control integrating large language mod-\\nels. arXiv preprint arXiv:2503.08199.\\nNingning Zhang, Chi Zhang, Zhizhong Tan, Xingxing\\nYang, Weiping Deng, and Wenyong Wang. 2025d.\\nCredible plan-driven rag method for multi-hop ques-\\ntion answering. arXiv preprint arXiv:2504.16787.\\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng',\n",
       " 'Yang, Weiping Deng, and Wenyong Wang. 2025d.\\nCredible plan-driven rag method for multi-hop ques-\\ntion answering. arXiv preprint arXiv:2504.16787.\\nTianjun Zhang, Shishir G Patil, Naman Jain, Sheng\\nShen, Matei Zaharia, Ion Stoica, and Joseph E Gon-\\nzalez. 2024a. Raft: Adapting language model to do-\\nmain specific rag. In First Conference on Language\\nModeling.\\nWeizhi\\nZhang,\\nYuanchen\\nBei,\\nLiangwei\\nYang,\\nHenry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui\\nLi, Hao Chen, Jianling Wang, Yu Wang, et al. 2025e.\\nCold-start recommendation towards the era of large\\nlanguage models (llms): A comprehensive survey\\nand roadmap. arXiv preprint arXiv:2501.01945.\\nWeizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo,\\nGuancheng Wan, Liangwei Yang, Chenxuan Xie,\\nYuyao Yang, Wei-Chieh Huang, Chunyu Miao, et al.\\n2025f. From web search towards agentic deep re-\\nsearch: Incentivizing search with reasoning agents.\\narXiv preprint arXiv:2506.18959.\\nWeizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liang-',\n",
       " '2025f. From web search towards agentic deep re-\\nsearch: Incentivizing search with reasoning agents.\\narXiv preprint arXiv:2506.18959.\\nWeizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liang-\\nwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng\\nZou, Zijie Huang, Zhengyang Wang, Yifan Gao,\\net al. 2025g. Personaagent: When large language\\nmodel agents meet personalization at test time. arXiv\\npreprint arXiv:2506.06254.\\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\\nHongyu Ren, Percy Liang, Christopher D Manning,\\nand Jure Leskovec. 2022b. Greaselm: Graph rea-\\nsoning enhanced language models. In International\\nConference on Learning Representations.\\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang\\nXu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai,\\nShuo Wang, Zhiyuan Liu, et al. 2024b. ∞bench:\\nExtending long context evaluation beyond 100k to-\\nkens. In Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 15262–15277.',\n",
       " 'Extending long context evaluation beyond 100k to-\\nkens. In Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pages 15262–15277.\\nYusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister,\\nRui Zhang, and Sercan Arik. 2024c. Chain of agents:\\nLarge language models collaborating on long-context\\ntasks. Advances in Neural Information Processing\\nSystems, 37:132208–132237.\\nZhebin Zhang, Xinyu Zhang, Yuanhang Ren, Saijiang\\nShi, Meng Han, Yongkang Wu, Ruofei Lai, and Zhao\\nCao. 2023. Iag: Induction-augmented generation\\nframework for answering reasoning questions. arXiv\\npreprint arXiv:2311.18397.\\nSiyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He,\\nLuna K Qiu, and Lili Qiu. 2024a. Retrieval aug-\\nmented generation (rag) and beyond: A comprehen-\\nsive survey on how to make your llms use external\\ndata more wisely. arXiv preprint arXiv:2409.14924.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen',\n",
       " 'sive survey on how to make your llms use external\\ndata more wisely. arXiv preprint arXiv:2409.14924.\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\\nsurvey of large language models.\\narXiv preprint\\narXiv:2303.18223, 1(2).\\nXiaoyan Zhao, Lingzhi Wang, Zhanghao Wang, Hong\\nCheng, Rui Zhang, and Kam-Fai Wong. 2024b.\\nPacar: Automated fact-checking with planning and\\ncustomized action reasoning using large language\\nmodels.\\nIn Proceedings of the 2024 Joint In-\\nternational Conference on Computational Linguis-\\ntics, Language Resources and Evaluation (LREC-\\nCOLING 2024), pages 12564–12573.\\nXinping Zhao, Dongfang Li, Yan Zhong, Boren Hu,\\nYibin Chen, Baotian Hu, and Min Zhang. 2024c.\\nSeer: Self-aligned evidence extraction for retrieval-\\naugmented generation. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 3027–3041.',\n",
       " 'Seer: Self-aligned evidence extraction for retrieval-\\naugmented generation. In Proceedings of the 2024\\nConference on Empirical Methods in Natural Lan-\\nguage Processing, pages 3027–3041.\\nKunhao Zheng, Jesse Michael Han, and Stanislas Polu.\\n2021. Minif2f: a cross-system benchmark for for-\\nmal olympiad-level mathematics.\\narXiv preprint\\narXiv:2109.00110.\\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai,\\nLyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025.\\nDeepresearcher: Scaling deep research via reinforce-\\nment learning in real-world environments. arXiv\\npreprint arXiv:2504.03160.\\nJiawei Zhou and Lei Chen. 2025. Openrag: Optimiz-\\ning rag end-to-end via in-context retrieval learning.\\narXiv preprint arXiv:2503.08398.\\nPeilin Zhou, Bruce Leon, Xiang Ying, Can Zhang,\\nYifan Shao, Qichen Ye, Dading Chong, Zhiling\\nJin, Chenxuan Xie, Meng Cao, et al. 2025a.\\nBrowsecomp-zh: Benchmarking web browsing abil-\\nity of large language models in chinese.\\narXiv\\npreprint arXiv:2504.19314.',\n",
       " 'Jin, Chenxuan Xie, Meng Cao, et al. 2025a.\\nBrowsecomp-zh: Benchmarking web browsing abil-\\nity of large language models in chinese.\\narXiv\\npreprint arXiv:2504.19314.\\nYifei Zhou, Song Jiang, Yuandong Tian, Jason Weston,\\nSergey Levine, Sainbayar Sukhbaatar, and Xian Li.\\n2025b.\\nSweet-rl: Training multi-turn llm agents\\non collaborative reasoning tasks.\\narXiv preprint\\narXiv:2503.15478.\\n19',\n",
       " 'Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen,\\nBinyu Xie, Zheng Liu, Ji-Rong Wen, and Zhicheng\\nDou. 2024. Inters: Unlocking the power of large\\nlanguage models in search with instruction tuning.\\nIn Proceedings of the 62nd Annual Meeting of the\\nAssociation for Computational Linguistics (Volume\\n1: Long Papers), pages 2782–2809.\\nHenry Peng Zou, Wei-Chieh Huang, Yaozu Wu,\\nYankai Chen, Chunyu Miao, Hoang Nguyen, Yue\\nZhou, Weizhi Zhang, Liancheng Fang, Langzhou\\nHe, et al. 2025.\\nA survey on large language\\nmodel based human-agent systems. arXiv preprint\\narXiv:2505.00753.\\n20',\n",
       " 'A\\nFull Benchmark\\nSection 6 introduces representative benchmarks for\\ndifferent RAG-reasoning tasks. This appendix com-\\nplements that discussion with a comprehensive list\\nof benchmarks organized by task and domain. Ta-\\nble 2 details each benchmark’s attributes, including\\nthe publication venue, code repository, task cate-\\ngory, domain, primary knowledge sources, knowl-\\nedge type, and reasoning capabilities. By consoli-\\ndating these attributes into a single table, we facili-\\ntate the selection and comparison of benchmarks,\\nenabling researchers to identify the most suitable\\ndatasets for future studies on RAG-enhanced rea-\\nsoning.\\nOur benchmark compilation is primarily derived\\nfrom the methods surveyed in Sections 3 to 5 of\\nthis paper, with a particular focus on synergized\\napproaches discussed in Section 5. We deliber-\\nately targeted benchmarks that require both exter-\\nnal knowledge retrieval and internal deep reason-\\ning, as this dual requirement reflects real-world',\n",
       " 'approaches discussed in Section 5. We deliber-\\nately targeted benchmarks that require both exter-\\nnal knowledge retrieval and internal deep reason-\\ning, as this dual requirement reflects real-world\\nscenarios where models must not only access rel-\\nevant information but also integrate and reason\\nover it effectively. For example, in the QA do-\\nmain, we include datasets that necessitate synthe-\\nsizing evidence across multiple documents to an-\\nswer questions that cannot be resolved through\\nsingle-sentence retrieval. HotpotQA (Yang et al.,\\n2018) exemplifies this challenge, requiring reason-\\ning across different Wikipedia articles. In coding\\ntasks, benchmarks such as LiveCodeBench (Jain\\net al., 2024) and Refactoring Oracle (Tsantalis et al.,\\n2020) extend beyond pure algorithmic problem-\\nsolving by demanding retrieval of external code\\nsnippets and documentation. Similarly, in mathe-\\nmatics, benchmarks like MATH (Hendrycks et al.,\\n2021) and AQUA-RAT (Das et al., 2024) assess',\n",
       " 'solving by demanding retrieval of external code\\nsnippets and documentation. Similarly, in mathe-\\nmatics, benchmarks like MATH (Hendrycks et al.,\\n2021) and AQUA-RAT (Das et al., 2024) assess\\nnot only computational proficiency but also the re-\\ntrieval of relevant theorems and formulas, testing\\nthe model’s ability to integrate external mathemati-\\ncal knowledge with internal reasoning processes.\\nIn addition to established benchmarks, we have\\nincorporated newer and more challenging datasets\\nthat better mirror real-world applications. These\\ndatasets often demand extensive retrieval processes\\ncombined with expert-level or domain-specific\\nreasoning, as seen in Humanity’s Last Exam\\n(HLE) (Phan et al., 2025) and web search evalua-\\ntion tasks like BrowseComp (Wei et al., 2025a).\\nOverall, our collection encompasses 46 bench-\\nmarks covering 13 distinct tasks across 12 domains,\\neach explicitly annotated with features such as\\nknowledge source, knowledge type, and reasoning',\n",
       " 'Overall, our collection encompasses 46 bench-\\nmarks covering 13 distinct tasks across 12 domains,\\neach explicitly annotated with features such as\\nknowledge source, knowledge type, and reasoning\\ncapacity. This breadth ensures coverage of diverse\\ndomains and task types, forming a solid foundation\\nfor evaluating the interplay between retrieval and\\nreasoning in RAG systems.\\nWithin this benchmark set, single-hop QA\\ndatasets like TriviaQA (Joshi et al., 2017) focus on\\nprecise retrieval and fact recall, requiring models\\nto locate and synthesize a single piece of evidence.\\nIn contrast, multi-hop QA benchmarks such as Hot-\\npotQA (Yang et al., 2018) and MuSiQue (Trivedi\\net al., 2022) challenge models to chain information\\nfrom multiple documents and employ deductive\\nreasoning to bridge disparate facts into coherent\\nanswers. Structured knowledge benchmarks, such\\nas GraphQA (He et al., 2024c), require reasoning\\nover relational graph representations, integrating',\n",
       " 'reasoning to bridge disparate facts into coherent\\nanswers. Structured knowledge benchmarks, such\\nas GraphQA (He et al., 2024c), require reasoning\\nover relational graph representations, integrating\\nnodes and edges to resolve complex queries be-\\nyond plain text retrieval. Complementing these\\nopen-ended tasks, multiple-choice evaluations like\\nMMLU-Pro (Wang et al., 2025b) test domain-\\nspecific knowledge in areas such as science, history,\\nor law, assessing the model’s ability to perform\\nvarious reasoning styles, including inductive and\\nabductive inference. Multimodal QA benchmarks,\\nlike WebShop (Yao et al., 2022), test a model’s\\ncapacity to align textual and visual information\\nto determine the correct answer. Long-form QA\\ndatasets such as ∞BENCH (Zhang et al., 2024b)\\nevaluate models’ ability to maintain logical consis-\\ntency and perform inductive reasoning over lengthy\\ncontexts. Collectively, these benchmarks establish\\na comprehensive evaluation chain for systemati-',\n",
       " 'tency and perform inductive reasoning over lengthy\\ncontexts. Collectively, these benchmarks establish\\na comprehensive evaluation chain for systemati-\\ncally assessing RAG-reasoning capabilities.\\nBeyond text-based QA, RAG-augmented bench-\\nmarks span diverse tasks involving long-form\\ngeneration, interactive reasoning, and domain-\\nspecific challenges in mathematics and pro-\\ngramming.\\nMathematics benchmarks such as\\nMATH (Hendrycks et al., 2021) draw from\\ncompetition-level problems to assess arithmetic\\nand symbolic reasoning.\\nSummarization tasks\\nlike XSum (Narayan et al., 2018) evaluate a\\nmodel’s ability to condense entire news articles\\ninto concise summaries while preserving factual\\ncorrectness.\\nFact-checking benchmarks, such\\nas FEVER (Thorne et al., 2018), test the ca-\\npacity for evidence retrieval and claim verifica-\\ntion. Code-focused evaluations, including Live-\\nCodeBench (Jain et al., 2024), examine deductive\\nand abductive reasoning in the context of algo-\\n21',\n",
       " 'Dataset\\nVenue\\nResource\\nTask\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning Capability\\nSize\\nInput\\nOutput\\nCode\\nLiveCodeBench (Jain\\net al., 2024)\\nArxiv’24\\nLink\\nCode\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n1,055\\nQuestion/Text, Code,\\nInstruction\\nCode Instance, Test\\nOutput\\nRefactoring Oracle\\n(Tsantalis et al., 2020)\\nIEEE’22\\nLink\\nCode\\nSoftware\\nInternet, Human\\nLogical\\nDeductive\\n7,226\\nCode, Instruction\\nCode Instance\\nColBench (Zhou et al.,\\n2025b)\\nArxiv’25\\nLink\\nCode\\nSoftware\\nLLM, Human\\nLogical\\nAbductive, Inductive\\n10,000+\\nQuestion/Text,\\nLinks/Sources, Code\\nCode Instance\\nMath\\nMATH (Hendrycks\\net al., 2021)\\nNeurIPS’21\\nLink\\nDomain-specific\\nQA\\nMath\\nExam/Competition\\nLogical, Arithmetic\\nDeductive\\n12,500\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nMiniF2F (Zheng et al.,\\n2021)\\nICLR’22\\nLink\\nDomain-specific\\nQA\\nMath\\nExam/Competition,\\nBooks\\nLogical, Arithmetic\\nDeductive\\n488\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nAQuA (Ling et al.,\\n2017)\\nArxiv’17\\nLink\\nDomain-specific\\nQA\\nMath',\n",
       " 'Link\\nDomain-specific\\nQA\\nMath\\nExam/Competition,\\nBooks\\nLogical, Arithmetic\\nDeductive\\n488\\nQuestion/Text,\\nEquations\\nNumber, Natural\\nLanguage\\nAQuA (Ling et al.,\\n2017)\\nArxiv’17\\nLink\\nDomain-specific\\nQA\\nMath\\nPrevious Source,\\nExam/Competition,\\nInternet\\nArithmetic, Logical\\nDeductive\\n100,000\\nQuestion/Text,\\nOptions, Equations\\nNatural Language,\\nOptions/Labels\\nFact Checking\\nCRAG (Yang et al.,\\n2024b)\\nNeurIPS’24\\nLink\\nFact Checking\\nGeneral\\nInternet\\nCommonsense\\nDeductive, Abductive\\n4,409\\nQuestion/Text\\nNatural Language\\nCREAK (Onoe et al.,\\n2021)\\nNeurIPS’21\\nLink\\nFact Checking\\nGeneral\\nHuman\\nCommonsense\\nDeductive, Abductive,\\nAnalogical\\n13,000\\nQuestion/Text\\nOptions/Labels,\\nNatural Language\\nFever (Thorne et al.,\\n2018)\\nACL’18\\nLink\\nFact Checking\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n185,445\\nQuestion/Text,\\nLinks/Sources\\nNatural Language,\\nOptions/Labels\\nPubHealth (Kotonya\\nand Toni, 2020)\\nEMNLP’20\\nLink\\nFact Checking\\nHealth\\nInternet\\nCommonsense,\\nLogical\\nAbductive, Deductive\\n11,800\\nQuestion/Text',\n",
       " 'Links/Sources\\nNatural Language,\\nOptions/Labels\\nPubHealth (Kotonya\\nand Toni, 2020)\\nEMNLP’20\\nLink\\nFact Checking\\nHealth\\nInternet\\nCommonsense,\\nLogical\\nAbductive, Deductive\\n11,800\\nQuestion/Text\\nNatural Language,\\nOptions\\nGraph QA\\nGraphQA (He et al.,\\n2024c)\\nNeurIPS’24\\nLink\\nGraph QA\\nGeneral\\nPrevious Source\\nCommonsense,\\nMultimodal\\nDeductive, Abductive\\n107,503\\nQuestion/Text\\nNatural Language\\nGRBENCH (Jin et al.,\\n2024)\\nACL’24\\nLink\\nGraph QA\\nGeneral\\nLLM, Human\\nLogical\\nDeductive, Inductive\\n1,740\\nQuestion/Text\\nNatural Language\\nLong-form QA\\n∞BENCH (Zhang\\net al., 2024b)\\nArxiv’24\\nLink\\nLong-form QA\\nGeneral\\nInternet, Human\\nMultimodal, Logical Inductive, Abductive\\n3,946\\nQuestion/Text, Code,\\nEquations\\nNatural Language,\\nNumber, Code\\nInstance\\nMultimodal QA\\nCrisisMMD (Alam\\net al., 2018)\\nArxiv’18\\nLink\\nMultimodal QA Crisis Response\\nMedia, Internet\\nCommonsense,\\nMultimodal\\nAbductive\\n16,097\\nQuestion/Text,\\nFigure/Image\\nOptions, Natural\\nLanguage\\nALFWORLD (Shridhar\\net al.)\\nICLR’21\\nLink\\nMultimodal QA\\nGame',\n",
       " 'Multimodal QA Crisis Response\\nMedia, Internet\\nCommonsense,\\nMultimodal\\nAbductive\\n16,097\\nQuestion/Text,\\nFigure/Image\\nOptions, Natural\\nLanguage\\nALFWORLD (Shridhar\\net al.)\\nICLR’21\\nLink\\nMultimodal QA\\nGame\\nPrevious Source\\nMultimodal\\nDeductive, Abductive\\n3,827\\nQuestion/Text,\\nFigure/Image\\nNatural Language\\nMMLongBench-DOC\\n(Ma et al., 2025)\\nNeurIPS’24\\nLink\\nMultimodal QA\\nNarrative\\nPrevious Source,\\nInternet\\nMultimodal\\nDeductive, Abductive\\n1,082\\nFigure/Image,\\nQuestion/Text,\\nDocuments\\nNatural Language,\\nNumber\\nLongDocURL (Deng\\net al., 2024)\\nArxiv’24\\nLink\\nMultimodal QA\\nNarrative\\nInternet, Previous\\nSource, LLM\\nMultimodal\\nDeductive, Abductive\\n2,325\\nFigure/Image,\\nQuestion/Text,\\nDocuments\\nNatural Language,\\nNumber\\nUDA (Hui et al., 2024)\\nNIPS’24\\nLink\\nMultimodal QA\\nNarrative\\nInternet,\\nPaper/Report\\nMultimodal\\nDeductive\\n29,590\\nDocuments,\\nQuestion/Text\\nNatural Language,\\nNumber\\nSCIENCEQA (Lu et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nScience\\nHuman\\nLogical, Multimodal\\nDeductive\\n21,000\\nQuestion/Text,\\nOptions,',\n",
       " 'Deductive\\n29,590\\nDocuments,\\nQuestion/Text\\nNatural Language,\\nNumber\\nSCIENCEQA (Lu et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nScience\\nHuman\\nLogical, Multimodal\\nDeductive\\n21,000\\nQuestion/Text,\\nOptions,\\nFigure/Image\\nOptions, Natural\\nLanguage, Number\\nWebShop (Yao et al.,\\n2022)\\nNeurIPS’22\\nLink\\nMultimodal QA\\nE-commerce\\nInternet\\nMultimodal\\nInductive, Abductive\\n12,087\\nInstruction,\\nQuestion/Text\\nNatural Language,\\nImage/Figure\\nSurgCoTBench (Low\\net al., 2025)\\nArxiv’25\\n—\\nMultimodal QA\\nHealth\\nHuman\\nMultimodal, Logical Abductive, Deductive\\n14,176\\nQuestion/Text,\\nFigure/Image,\\nOptions\\nOptions, Natural\\nLanguage, Number\\nTable 2: Full representative knowledge and reasoning intensive benchmarks across diverse task categories (Part 1).\\n22',\n",
       " 'Dataset\\nVenue\\nResource\\nTask\\nDomain\\nKnowledge Source\\nKnowledge Type\\nReasoning Capability\\nSize\\nInput\\nOutput\\nMulti-choice QA\\nBamboogle (Press et al.,\\n2023)\\nEMNLP’23\\nLink\\nMulti-choice QA\\nGeneral\\nInternet\\nLogical\\nDeductive, Abductive\\n125\\nQuestion/Text\\nNatural Language\\nBIG-Bench (Srivastava\\net al., 2022)\\nArxiv’22\\nLink\\nMulti-choice QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive, Abductive,\\nInductive, Analogical\\n204\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber,\\nOptions/Labels\\nADQA (Li et al.,\\n2024a)\\nEMNLP’24\\nLink\\nMulti-choice QA\\nHealth\\nPrevious Source\\nCommonsense,\\nLogical\\nDeductive, Abductive\\n446\\nQuestion/Text,\\nOptions\\nOptions\\nQuALITY (Pang et al.,\\n2022)\\nNAACL’22\\nLink\\nMulti-choice QA\\nNarrative\\nBooks\\nCommonsense,\\nLogical\\nDeductive, Abductive\\n6,737\\nQuestion/Text,\\nOptions\\nOptions\\nMMLU-Pro (Wang\\net al., 2025b)\\nNeurIPS’24\\nLink\\nMulti-choice QA\\nScience\\nPrevious Source,\\nInternet\\nArithmetic,\\nCommonsense,\\nLogical\\nDeductive, Inductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options',\n",
       " 'et al., 2025b)\\nNeurIPS’24\\nLink\\nMulti-choice QA\\nScience\\nPrevious Source,\\nInternet\\nArithmetic,\\nCommonsense,\\nLogical\\nDeductive, Inductive\\n12,032\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options\\nMulti-hop QA\\nFRAMES (Krishna\\net al., 2024)\\nArxiv’24\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical, Arithmetic\\nDeductive\\n824\\nQuestion/Text\\nNatural Language\\nHotpotQA (Yang et al.,\\n2018)\\nEMNLP’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n113,000\\nQuestion/Text\\nNatural Language\\nGPQA (Rein et al.,\\n2024)\\nArxiv’24\\nLink\\nMulti-hop QA\\nScience\\nHuman\\nLogical\\nDeductive, Abductive\\n448\\nQuestion/Text,\\nOptions\\nNatural Language,\\nNumber, Options\\nHLE (Phan et al., 2025)\\nArxiv’25\\nLink\\nMulti-hop QA\\nScience\\nHuman\\nLogical, Arithmetic,\\nMultimodal\\nDeductive, Abductive\\n2,500\\nQuestion/Text,\\nOptions,\\nFigure/Image\\nNatural Language,\\nNumber, Options\\nCWQ (Talmor and\\nBerant, 2018)\\nNAACL’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n34,689\\nQuestion/Text\\nNatural Language',\n",
       " 'Options,\\nFigure/Image\\nNatural Language,\\nNumber, Options\\nCWQ (Talmor and\\nBerant, 2018)\\nNAACL’18\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense\\nDeductive\\n34,689\\nQuestion/Text\\nNatural Language\\nIIRC (Ferguson et al.,\\n2020)\\nEMNLP’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n13,000+\\nQuestion/Text,\\nLinks/Sources\\nNumber, Natural\\nLanguage\\nMINTQA (He et al.,\\n2024b)\\nArxiv’24\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n10,479\\nQuestion/Text\\nNatural Language\\nMuSiQue (Trivedi et al.,\\n2022)\\nACL’22\\nLink\\nMulti-hop QA\\nGeneral\\nPrevious Source,\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n25,000\\nQuestion/Text\\nNatural Language\\nTopiOCQA (Adlakha\\net al., 2022)\\nTACL’22\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n54,494\\nQuestion/Text\\nNatural Language\\n2WikiMultiHopQA (Ho\\net al., 2020)\\nCOLING’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nMulti-step QA\\nStrategyQA (Geva\\net al., 2021)\\nTACL’21',\n",
       " 'et al., 2020)\\nCOLING’20\\nLink\\nMulti-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n192,606\\nQuestion/Text\\nNatural Language\\nMulti-step QA\\nStrategyQA (Geva\\net al., 2021)\\nTACL’21\\nLink\\nMulti-step QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n2,780\\nQuestion/Text\\nNatural Language\\nSingle-hop QA\\nSimpleQA (Wei et al.,\\n2024)\\nArxiv’24\\nLink\\nSingle-hop QA\\nGeneral\\nLLM, Human\\nCommonsense\\nDeductive\\n4,326\\nQuestion/Text\\nNatural Language\\nTriviaQA (Joshi et al.,\\n2017)\\nACL’17\\nLink\\nSingle-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n650,000+\\nQuestion/Text\\nNatural Language\\nNQ (Kwiatkowski et al.,\\n2019)\\nACL’19\\nLink\\nSingle-hop QA\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\nDeductive\\n307,373\\nQuestion/Text\\nNatural Language\\nText Summarization\\nXSum (Narayan et al.,\\n2018)\\nEMNLP’18\\nLink\\nText\\nSummarization\\nNarrative\\nInternet, Media\\nLogical,\\nCommonsense\\nAbductive\\n226,711\\nQuestion/Text\\nNatural Language\\nBIGPATENT (Sharma\\net al., 2019)\\nACL’19\\nLink\\nText\\nSummarization\\nPatent\\nInternet\\nCommonsense,\\nLogical',\n",
       " 'Narrative\\nInternet, Media\\nLogical,\\nCommonsense\\nAbductive\\n226,711\\nQuestion/Text\\nNatural Language\\nBIGPATENT (Sharma\\net al., 2019)\\nACL’19\\nLink\\nText\\nSummarization\\nPatent\\nInternet\\nCommonsense,\\nLogical\\nAbductive\\n1.3 M\\nQuestion/Text\\nNatural Language\\nWeb Browsing\\nBrowseComp (Wei\\net al., 2025a)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, Internet\\nCommonsense,\\nLogical\\nDeductive\\n1,266\\nQuestion/Text\\nNatural Language\\nBrowseComp-ZH\\n(Zhou et al., 2025a)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, Internet\\nCommonsense,\\nLogical\\nDeductive\\n289\\nQuestion/Text\\nNatural Language\\nGAIA (Mialon et al.,\\n2023)\\nICLR’23\\nLink\\nWeb Browsing\\nGeneral\\nInternet, TooL\\nCommonsense,\\nLogical\\nDeductive\\n466\\nQuestion/Text,\\nImage/File/Code\\nNatural Language\\nWebWalkerQA (Wu\\net al., 2025b)\\nArxiv’25\\nLink\\nWeb Browsing\\nGeneral\\nHuman, LLM\\nCommonsense,\\nLogical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nDialog\\nDailyDialog (Li et al.,\\n2017)\\nArxiv’17\\nLink\\nDialog\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\n–\\n13,118\\nQuestion/Text\\nNatural Language',\n",
       " 'Logical\\nDeductive\\n680\\nQuestion/Text\\nNatural Language\\nDialog\\nDailyDialog (Li et al.,\\n2017)\\nArxiv’17\\nLink\\nDialog\\nGeneral\\nInternet\\nCommonsense,\\nLogical\\n–\\n13,118\\nQuestion/Text\\nNatural Language\\nTable 3: Full epresentative knowledge and reasoning intensive benchmarks across diverse task categories (Part 2,\\ncontinued).\\n23',\n",
       " 'Benchmark\\nDomain\\nPrimary Retrieval Challenge\\nPrimary Reasoning Challenge\\nTriviaQA, NQ\\nGeneral\\nScale & Noise: Retrieval from massive, noisy cor-\\npora.\\nAmbiguity: Handling real-world queries that are of-\\nten underspecified or ambiguous.\\nHotpotQA,\\n2WikiMultiHopQA,\\nMuSiQue, HLE\\nGeneral\\nMulti-document / High-dependency Synthesis: Re-\\nquires finding and connecting evidence scattered\\nacross multiple Wikipedia articles.\\nMulti-hop Deduction: Explicitly designed to test\\nthe ability to link two or more discrete facts into a\\ncoherent reasoning path.\\nMMLU-Pro, QUALITY\\nScience, Narrative\\nExpert-level Retrieval: Requires accessing deep spe-\\ncialized knowledge from academic or densely written\\nnarrative sources.\\nComplex & Long-form Reasoning: MMLU-Pro de-\\nmands expert-level problem-solving over rote memo-\\nrization. QUALITY uniquely requires comprehension\\nof very long texts (often >5,000 tokens).\\nMATH, AQUA-RAT\\nMath\\nFormal Knowledge Retrieval: Locating precise',\n",
       " 'rization. QUALITY uniquely requires comprehension\\nof very long texts (often >5,000 tokens).\\nMATH, AQUA-RAT\\nMath\\nFormal Knowledge Retrieval: Locating precise\\nmathematical theorems, lemmas, or formulas in for-\\nmal corpora.\\nSymbolic & Deductive Reasoning: Involves per-\\nforming precise, multi-step logical and algebraic oper-\\nations where each step must be correct. AQUA-RAT\\nis unique in providing natural language rationales,\\nthus testing the model’s ability to explain its formal\\nreasoning.\\nLiveCodeBench\\nCode\\nStructural & Modal Heterogeneity: Must retrieve\\nfrom diverse, heterogeneous sources such as code\\nrepositories, documentation, and community forums\\nlike Stack Overflow.\\nTool Use & Self-correction Reasoning: Requires ap-\\nplying retrieved code snippets/APIs, executing code,\\nand reasoning based on test outputs to debug and iter-\\natively improve solutions.\\nBrowseComp,\\nWebWalkerQA\\nGeneral (Web)\\nDynamism, Interactivity, and Long-tail Retrieval:',\n",
       " 'and reasoning based on test outputs to debug and iter-\\natively improve solutions.\\nBrowseComp,\\nWebWalkerQA\\nGeneral (Web)\\nDynamism, Interactivity, and Long-tail Retrieval:\\nTests agentic planning and tool use in live, unstruc-\\ntured web environments. BrowseComp requires cre-\\native, persistent navigation to locate hard-to-find, in-\\ntertwined information, while WebWalkerQA focuses\\non systematic traversal of a website’s subpages.\\nAgentic & Strategic Reasoning: Requires planning\\nand executing multi-step strategies (e.g., searching,\\nclicking, extracting) in dynamic and unpredictable\\ncontexts to achieve a defined goal.\\nTable 4: The primary retrieval and reasoning challenges for different RAG-Reasoning benchmarks.\\nrithmic problem-solving. Web-based tasks, exem-\\nplified by BrowseComp (Wei et al., 2025a), emu-\\nlate real-world search behavior, requiring iterative\\nquery formulation and navigation across multiple\\nwebpages.\\nIn addition to cataloging datasets, Table 4 pro-',\n",
       " 'late real-world search behavior, requiring iterative\\nquery formulation and navigation across multiple\\nwebpages.\\nIn addition to cataloging datasets, Table 4 pro-\\nvides a synthesized overview of the primary re-\\ntrieval and reasoning challenges associated with\\neach benchmark discussed in this survey. This\\ncomparative analysis reveals critical gaps in cur-\\nrent benchmark coverage that future research must\\naddress. From a domain perspective, most bench-\\nmarks still focus on a limited set of general or\\nacademic scenarios, with few tackling real-world,\\nrealistic industrial or vertical-domain tasks where\\nretrieval sources might be personalized, proprietary\\nor highly specialized. Regarding retrieval capa-\\nbilities, existing benchmarks rarely test systems’\\nability to handle heterogeneous or multimodal con-\\ntent, nor do they systematically evaluate robust-\\nness against noisy, evolving, or conflicting infor-\\nmation within a unified framework for trustworthi-',\n",
       " 'tent, nor do they systematically evaluate robust-\\nness against noisy, evolving, or conflicting infor-\\nmation within a unified framework for trustworthi-\\nness. In terms of reasoning capabilities, current\\nbenchmarks primarily assess deductive reasoning,\\nleaving underexplored more complex forms such\\nas deep causal reasoning, counterfactual thinking,\\ndecision-oriented reasoning, or analogical reason-\\ning in specialized domains. Moreover, there is a\\nlack of standardized benchmarks and metrics for\\nevaluating the entire reasoning-retrieval trajectory,\\nincluding the efficiency of retrieval steps, the qual-\\nity of intermediate queries, and the logical consis-\\ntency of multi-step reasoning chains.\\nB\\nDeep Research Implementations\\nIn this section, we extend the discussion of the\\nagentic paradigm introduced in Section 5.2, in\\nwhich RAG systems adopt the role of active re-\\nsearchers who plan multistep queries, interleave\\nretrieval with reasoning, and coordinate specialized',\n",
       " 'agentic paradigm introduced in Section 5.2, in\\nwhich RAG systems adopt the role of active re-\\nsearchers who plan multistep queries, interleave\\nretrieval with reasoning, and coordinate specialized\\ntools or agents. These characteristics collectively\\ndefine what we refer to as deep research, represent-\\ning the ability of a system to autonomously break\\ndown complex questions, iteratively gather diverse\\nevidence, and synthesize information through mul-\\ntiple reasoning steps. This paradigm seeks to en-\\nhance autonomy, reduce hallucinations, and im-\\nprove factual accuracy in open-domain tasks.\\nSuch deep research systems can be realized\\nthrough either single-agent or multi-agent archi-\\ntectures. Single-agent systems rely on a single\\nmodel to manage the entire process of question\\ndecomposition, retrieval, and synthesis, offering\\nsimplicity and shared context but facing limita-\\ntions in handling highly specialized or multi-modal\\ntasks. In contrast, multi-agent systems distribute',\n",
       " 'simplicity and shared context but facing limita-\\ntions in handling highly specialized or multi-modal\\ntasks. In contrast, multi-agent systems distribute\\nthese responsibilities among specialized agents, en-\\nabling modularity and potentially greater robust-\\nness. However, this collaborative design introduces\\n24',\n",
       " 'Name\\nBase Model\\nOptimizationReward\\nRetriever\\nAgent\\nArchitecture\\nTrain Data\\nEvaluation Data\\nLink\\nAgentic Reasoning\\n(Wu et al., 2025c)\\nN/A\\nPrompting\\nN/A\\nWeb Search\\nCentralized\\nN/A\\nGPQA\\nLink\\ngpt-researcher\\nPrompting\\nN/A\\nWeb Search,\\nLocal Retrieval\\nCentralized\\nN/A\\nN/A\\nLink\\ndeep-searcher\\nDeepseek, , Claude,\\nGemini, Qwen\\nPrompting\\nN/A\\nWeb Search\\nHierarchical\\nN/A\\nN/A\\nLink\\nSearch-R1 (Jin\\net al., 2025)\\nQwen2.5-7B-Instruct,\\nQwen2.5-7B-Base,\\nQwen-2.5-3B-Instruct,\\nQwen-2.5-3B-Base\\nGRPO,\\nPPO\\nExact\\nMatch\\nWeb Search\\nSingle\\nNQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nZeroSearch (Sun\\net al., 2025a)\\nQwen2.5-3B-Base,\\nQwen2.5-7B-Base,\\nQwen2.5-7B-Instruct,\\nQwen2.5-3B-Instruct,\\nLLaMA3.2-3B-Instruct,\\nLLaMA3.2-3B-Base\\nGRPO,\\nPPO,\\nReinforce\\nExact\\nMatch\\nWeb Search\\nSingle\\nNQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nWebthinker (Li\\net al., 2025c)\\nGPT-o1, GPT-o3,\\nDeepseek-R1, QwQ-32B,\\nQwen2.5-32B-Instruct\\nDPO\\nPreference\\nPairs',\n",
       " 'NQ, HotpotQA\\nNQ, TriviaQA, PopQA, HotpotQA,\\n2WikiMultiHopQA, MuSiQue,\\nBamboogle\\nLink\\nWebthinker (Li\\net al., 2025c)\\nGPT-o1, GPT-o3,\\nDeepseek-R1, QwQ-32B,\\nQwen2.5-32B-Instruct\\nDPO\\nPreference\\nPairs\\nWeb Search\\nSingle\\nSuperGPQA,\\nWebWalkerQA,\\nOpenThoughts,\\nNaturalReasoning,\\nNuminaMath\\nGPQA, GAIA, WebWalkerQA,\\nHumanity’s Last Exam\\nLink\\nnanoDeepResearch\\nOpenAI series, Claude\\nPrompting\\nN/A\\nWeb Search\\nCentralized\\nN/A\\nN/A\\nLink\\nDeerFlow\\nQwen,\\nPrompting\\nN/A\\nWeb Search\\nDecentralized\\nN/A\\nN/A\\nLink\\ndeep-research\\nDeepseek,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nopen-deep-research\\nOpenAI series, Deepseek,\\nClaude, Gemini\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nDeepResearcher\\n(Zheng et al., 2025)\\nQwen2.5-7B-Instruct\\nGRPO\\nFormat\\nWeb Search\\nDecentralized\\nNQ, TQ, HotpotQA,\\n2WikiMultiHopQA\\nMuSiQue, Bamboogle, PopQA, NQ,\\nTQ, HotpotQA, 2WikiMultiHopQA\\nLink\\nR1-Searcher (Song\\net al., 2025)\\nQwen2.5-7B-Base,\\nLlama3.1-8B-Instruct\\nGRPO, Re-\\ninforce++,\\nSFT\\nRetrieval,\\nFormat\\nWeb Search,\\nLocal Retrieval',\n",
       " 'TQ, HotpotQA, 2WikiMultiHopQA\\nLink\\nR1-Searcher (Song\\net al., 2025)\\nQwen2.5-7B-Base,\\nLlama3.1-8B-Instruct\\nGRPO, Re-\\ninforce++,\\nSFT\\nRetrieval,\\nFormat\\nWeb Search,\\nLocal Retrieval\\nSingle\\nHotpotQA,\\n2WikiMultiHopQA\\nHotpotQA, 2WikiMultiHopQA,\\nMuSiQue, Bamboogle\\nLink\\nReSearch (Chen\\net al., 2025a)\\nQwen2.5-7B-Instruct,\\nQwen2.5-32B-Instruct\\nGRPO\\nFormat,\\nAnswer\\nWeb Search\\nSingle\\nMuSiQue\\nHotpotQA, 2WikiMultiHopQA,\\nMuSiQue, Bamboogle\\nLink\\nSearch-o1 (Li et al.,\\n2025b)\\nQwQ-32B-Preview\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nGPQA, MATH500, AMC2023,\\nAIME2024, LiveCodeBench, Natural\\nQuestions, TriviaQA, HotpotQA,\\n2Wiki, MuSiQue, Bamboogle\\nLink\\nr1-reasoning-rag\\nDeepseek\\nPrompting\\nN/A\\nLocal Retrieval,\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nOpen Deep Search\\n(Alzubi et al., 2025)\\nLlama3.1-70B,\\nDeepseek-R1\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nSimpleQA, FRAME\\nLink\\nnode-DeepResearch\\nGemini,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\ndeep-research\\nGemini, OpenAI series,\\nDeepseek, Claude, Grok\\nPrompt\\nN/A\\nLocal Retrieval,',\n",
       " 'Single\\nN/A\\nSimpleQA, FRAME\\nLink\\nnode-DeepResearch\\nGemini,\\nPrompting\\nN/A\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\ndeep-research\\nGemini, OpenAI series,\\nDeepseek, Claude, Grok\\nPrompt\\nN/A\\nLocal Retrieval,\\nWeb Search\\nSingle\\nN/A\\nN/A\\nLink\\nTable 5: Overview of deep research implementations.\\nadditional complexity in coordination and commu-\\nnication, as well as higher computational costs.\\nAlongside these developments in agent orches-\\ntration, the nature of retrievers used in deep re-\\nsearch has also evolved significantly. Early RAG\\nsystems relied on sparse keyword-based retrieval,\\nlater surpassed by dense retrievers employing bi-\\nencoder architectures for semantic matching. More\\nrecent deep research systems increasingly integrate\\nweb search-based retrievers, allowing real-time ac-\\ncess to open-domain information. Some retrievers\\nhave also been transformed into LLM-callable tools\\nfor flexible invocation. This evolution of retrievers\\nhas played a crucial role in enabling the sophisti-',\n",
       " 'have also been transformed into LLM-callable tools\\nfor flexible invocation. This evolution of retrievers\\nhas played a crucial role in enabling the sophisti-\\ncated information-gathering processes required for\\ndeep research.\\nC\\nComparison of Reasoning Workflows\\nand Agent Orchestration Strategies\\nTable 6 summarizes the diverse reasoning work-\\nflows and agent orchestration strategies employed\\nin Synergized RAG-Reasoning systems, highlight-\\ning their respective strengths, limitations, and suit-\\nable application scenarios. Reasoning workflows\\nvary from linear chain-based approaches, which\\nare efficient but vulnerable to error propagation, to\\nmore complex tree-based and graph-based methods\\nthat offer higher recall and transparency at the cost\\nof increased computational overhead. Similarly,\\nagent orchestration strategies range from single-\\nagent setups to multi-agent systems that distribute\\nspecialized roles among agents, enhancing robust-\\nness and scalability. However, these advanced de-',\n",
       " 'agent setups to multi-agent systems that distribute\\nspecialized roles among agents, enhancing robust-\\nness and scalability. However, these advanced de-\\nsigns often introduce additional communication\\noverhead and complexity in conflict resolution.\\nThis comparison illustrates the trade-offs inherent\\nin choosing particular workflows or orchestration\\narchitectures and underscores the need for adaptive\\nsystems that can dynamically balance efficiency,\\naccuracy, and resource constraints in real-world\\napplications.\\n25',\n",
       " 'Category\\nSub-category\\nStrengths\\nLimitations\\nSuitable Scenarios\\nReasoning\\nWorkflow\\nChain-based\\nOne retrieval per reasoning step; low\\nlatency and token cost. Easy to cache\\nand monitor.\\nAn early wrong sub-query propagates;\\ncontext grows fast on long chains.\\nSingle-hop or short multi-hop QA\\nwhere each intermediate fact is easy to\\naccess.\\nTree-based (ToT)\\nHigh recall: explores multiple\\nbranches in parallel, hedges against\\nearly errors. Transparent what-if traces.\\nQuadratic cost; tree branches require\\nmany retrieval calls.\\nAmbiguous or “multiple plausible\\npaths” tasks (e.g., HotpotQA, legal\\nreasoning) where missing one clue kills\\naccuracy.\\nTree-based\\n(MCTS)\\nBudget-aware exploration: focuses\\ncalls on promising branches; graceful\\nanytime stopping.\\nTuning-heavy and may converge to a\\nsuboptimal subtree.\\nDeep-search problems under tight\\nAPI-call or token budgets (e.g.,\\nbiomedical QA).\\nGraph-based\\n(Walk-on-Graph)\\nEfficient in explicit KG/document\\ngraphs; short reasoning paths on KGs.',\n",
       " 'Deep-search problems under tight\\nAPI-call or token budgets (e.g.,\\nbiomedical QA).\\nGraph-based\\n(Walk-on-Graph)\\nEfficient in explicit KG/document\\ngraphs; short reasoning paths on KGs.\\nRequires high-quality KGs; fails if\\ngraphs lack explicit edges; less flexible\\nfor open-web contexts.\\nEnterprise or domain-specific QA\\nwhere a curated KG exists (e.g.,\\nproduct catalogs).\\nGraph-based\\n(Think-on-Graph)\\nAdaptive and verifiable; LLM updates\\na live evidence graph, allowing\\nnode-level citation checks and high\\nfactual accuracy.\\nHigher latency; many micro-tool calls;\\nsearch space can explode without\\npruning.\\nOpen-domain “deep research” or\\nfact-dense synthesis tasks (e.g.,\\nBrowseComp, systematic reviews).\\nAgent\\nOrchestration\\nSingle-agent\\n(Prompt-only)\\nSimple implementation via a ReAct\\nloop; low resource overhead.\\nConstrained by prompt engineering\\nand system design flexibility.\\nPrototyping demos and small-scale\\napplications where simplicity\\noutweighs performance.\\nSingle-agent\\n(SFT)',\n",
       " 'loop; low resource overhead.\\nConstrained by prompt engineering\\nand system design flexibility.\\nPrototyping demos and small-scale\\napplications where simplicity\\noutweighs performance.\\nSingle-agent\\n(SFT)\\nClear, well-defined RAG and\\nreasoning patterns; higher precision\\nthan prompt-only approaches.\\nRequires large synthetic data; may\\noverfit tool schemas, reducing\\nout-of-domain generalization.\\nProduction chatbots with stable APIs\\nand predictable query formats (e.g.,\\ninternal customer support).\\nSingle-agent (RL)\\nAdaptive RAG and reasoning yields\\nhigh recall and accuracy; learns when\\nto retrieve and reason.\\nChallenging to define suitable reward\\nsignals; computationally expensive to\\ntrain.\\nOpen-domain research or long-form\\nQA where call costs are high and\\noptimal stop conditions matter.\\nMulti-agent\\n(Decentralized)\\nHigh recall via parallel domain\\nexperts; robustness to noisy or diverse\\ncorpora.\\nHigh communication and consensus\\noverhead; conflicting answers require\\nresolution.',\n",
       " 'Multi-agent\\n(Decentralized)\\nHigh recall via parallel domain\\nexperts; robustness to noisy or diverse\\ncorpora.\\nHigh communication and consensus\\noverhead; conflicting answers require\\nresolution.\\nLarge-scale evidence aggregation\\nacross heterogeneous sources (e.g.,\\nmeta-analysis, news tracking).\\nMulti-agent\\n(Central-\\nized/Hierarchical)\\nBudget-efficient: manager avoids\\nduplicate searches and ensures a clear\\nprovenance chain. Scales horizontally\\nwithout exponential cost growth.\\nManager prompts or policies can\\nbecome a single-point bottleneck,\\nlimiting performance.\\nComplex tasks requiring coordinated\\nsubtasks under strict API-call budgets.\\nTable 6: Comparison of reasoning workflows and agent orchestration in Synergized RAG-Reasoning systems.\\n26',\n",
       " '1 \\nSample PDF \\n \\nCreated for testing PDFObject \\n \\nThis PDF is three pages long. Three long pages. Or three short pages if \\nyou’re optimistic. Is it the same as saying “three long minutes”, knowing \\nthat all minutes are the same duration, and one cannot possibly be longer \\nthan the other? If these pages are all the same size, can one possibly be \\nlonger than the other? \\n \\nI digress. Here’s some Latin. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec \\nodio. Praesent libero. Sed cursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum \\nimperdiet. Duis sagittis ipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris \\nmassa. Vestibulum lacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per \\nconubia nostra, per inceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem',\n",
       " 'conubia nostra, per inceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem \\nat dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. \\nMorbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia \\naliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh.  \\n \\nQuisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia \\nnostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque \\nadipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna.',\n",
       " 'Nunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet. Donec lacus nunc, viverra nec, blandit \\nvel, egestas et, augue. Vestibulum tincidunt malesuada tellus. Ut ultrices ultrices enim. Curabitur \\nsit amet mauris.  \\n \\nMorbi in dui quis est pulvinar ullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras \\nmetus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique,',\n",
       " 'metus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique, \\ndignissim in, ultrices sit amet, augue. Proin sodales libero eget ante. Nulla quam. Aenean laoreet. \\nVestibulum nisi lectus, commodo ac, facilisis ac, ultricies eu, pede. Ut orci risus, accumsan \\nporttitor, cursus quis, aliquet eget, justo. Sed pretium blandit orci.  \\n \\nUt eu diam at pede suscipit sodales. Aenean lectus elit, fermentum non, convallis id, sagittis at, \\nneque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem',\n",
       " '2 \\nipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante \\ndapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. \\nPraesent mauris.  \\n \\nFusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class \\naptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur \\nsodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean \\nquam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem.  \\n \\nProin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, \\nmassa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper \\nvel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia,',\n",
       " 'vel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, \\nurna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla \\nfacilisi. Ut fringilla. Suspendisse potenti.  \\n \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet.  \\n \\nDonec lacus nunc, viverra nec, blandit vel, egestas et, augue. Vestibulum tincidunt malesuada',\n",
       " 'elementum. Morbi in ipsum sit amet pede facilisis laoreet.  \\n \\nDonec lacus nunc, viverra nec, blandit vel, egestas et, augue. Vestibulum tincidunt malesuada \\ntellus. Ut ultrices ultrices enim. Curabitur sit amet mauris. Morbi in dui quis est pulvinar \\nullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras metus. Sed aliquet risus a \\ntortor. Integer id quam. Morbi mi.  \\n \\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed \\ncursus ante dapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis \\nipsum. Praesent mauris. Fusce nec tellus sed augue semper porta. Mauris massa. Vestibulum \\nlacinia arcu eget nulla. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per \\ninceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem',\n",
       " 'inceptos himenaeos. Curabitur sodales ligula in libero.  \\n \\nSed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean quam. In scelerisque sem \\nat dolor. Maecenas mattis. Sed convallis tristique sem. Proin ut ligula vel nunc egestas porttitor. \\nMorbi lectus risus, iaculis vel, suscipit quis, luctus non, massa. Fusce ac turpis quis ligula lacinia \\naliquet. Mauris ipsum. Nulla metus metus, ullamcorper vel, tincidunt sed, euismod in, nibh.  \\n \\nQuisque volutpat condimentum velit. Class aptent taciti sociosqu ad litora torquent per conubia \\nnostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, urna non tincidunt mattis, tortor neque \\nadipiscing diam, a cursus ipsum ante quis turpis. Nulla facilisi. Ut fringilla. Suspendisse potenti. \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna.',\n",
       " 'Nunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices.  \\n \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue',\n",
       " '3 \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet. Donec lacus nunc, viverra nec, blandit \\nvel, egestas et, augue. Vestibulum tincidunt malesuada tellus. Ut ultrices ultrices enim. Curabitur \\nsit amet mauris.  \\n \\nMorbi in dui quis est pulvinar ullamcorper. Nulla facilisi. Integer lacinia sollicitudin massa. Cras \\nmetus. Sed aliquet risus a tortor. Integer id quam. Morbi mi. Quisque nisl felis, venenatis tristique, \\ndignissim in, ultrices sit amet, augue. Proin sodales libero eget ante. Nulla quam. Aenean laoreet. \\nVestibulum nisi lectus, commodo ac, facilisis ac, ultricies eu, pede. Ut orci risus, accumsan \\nporttitor, cursus quis, aliquet eget, justo. Sed pretium blandit orci.  \\n \\nUt eu diam at pede suscipit sodales. Aenean lectus elit, fermentum non, convallis id, sagittis at, \\nneque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem',\n",
       " 'neque. Nullam mauris orci, aliquet et, iaculis et, viverra vitae, ligula. Nulla ut felis in purus \\naliquam imperdiet. Maecenas aliquet mollis lectus. Vivamus consectetuer risus et tortor. Lorem \\nipsum dolor sit amet, consectetur adipiscing elit. Integer nec odio. Praesent libero. Sed cursus ante \\ndapibus diam. Sed nisi. Nulla quis sem at nibh elementum imperdiet. Duis sagittis ipsum. \\nPraesent mauris.  \\n \\nFusce nec tellus sed augue semper porta. Mauris massa. Vestibulum lacinia arcu eget nulla. Class \\naptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Curabitur \\nsodales ligula in libero. Sed dignissim lacinia nunc. Curabitur tortor. Pellentesque nibh. Aenean \\nquam. In scelerisque sem at dolor. Maecenas mattis. Sed convallis tristique sem.  \\n \\nProin ut ligula vel nunc egestas porttitor. Morbi lectus risus, iaculis vel, suscipit quis, luctus non, \\nmassa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper',\n",
       " 'massa. Fusce ac turpis quis ligula lacinia aliquet. Mauris ipsum. Nulla metus metus, ullamcorper \\nvel, tincidunt sed, euismod in, nibh. Quisque volutpat condimentum velit. Class aptent taciti \\nsociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Nam nec ante. Sed lacinia, \\nurna non tincidunt mattis, tortor neque adipiscing diam, a cursus ipsum ante quis turpis. Nulla \\nfacilisi. Ut fringilla. Suspendisse potenti.  \\n \\nNunc feugiat mi a tellus consequat imperdiet. Vestibulum sapien. Proin quam. Etiam ultrices. \\nSuspendisse in justo eu magna luctus suscipit. Sed lectus. Integer euismod lacus luctus magna. \\nQuisque cursus, metus vitae pharetra auctor, sem massa mattis sem, at interdum magna augue \\neget diam. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; \\nMorbi lacinia molestie dui. Praesent blandit dolor. Sed non quam. In vel mi sit amet augue congue \\nelementum. Morbi in ipsum sit amet pede facilisis laoreet.',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nRAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nZirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang∗\\nThe University of Hong Kong\\nzrguo101@hku.hk\\nxubinrencs@gmail.com\\nchaohuang75@gmail.com\\nABSTRACT\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm\\nfor expanding Large Language Models beyond their static training limitations.\\nHowever, a critical misalignment exists between current RAG capabilities and\\nreal-world information environments. Modern knowledge repositories are inher-\\nently multimodal, containing rich combinations of textual content, visual elements,\\nstructured tables, and mathematical expressions. Yet existing RAG frameworks are\\nlimited to textual content, creating fundamental gaps when processing multimodal\\ndocuments. We present RAG-Anything, a unified framework that enables compre-\\nhensive knowledge retrieval across all modalities. Our approach reconceptualizes',\n",
       " 'documents. We present RAG-Anything, a unified framework that enables compre-\\nhensive knowledge retrieval across all modalities. Our approach reconceptualizes\\nmultimodal content as interconnected knowledge entities rather than isolated data\\ntypes. The framework introduces dual-graph construction to capture both cross-\\nmodal relationships and textual semantics within a unified representation. We\\ndevelop cross-modal hybrid retrieval that combines structural knowledge naviga-\\ntion with semantic matching. This enables effective reasoning over heterogeneous\\ncontent where relevant evidence spans multiple modalities. RAG-Anything demon-\\nstrates superior performance on challenging multimodal benchmarks, achieving\\nsignificant improvements over state-of-the-art methods. Performance gains become\\nparticularly pronounced on long documents where traditional approaches fail. Our\\nframework establishes a new paradigm for multimodal knowledge access, eliminat-',\n",
       " 'particularly pronounced on long documents where traditional approaches fail. Our\\nframework establishes a new paradigm for multimodal knowledge access, eliminat-\\ning the architectural fragmentation that constrains current systems. Our framework\\nis open-sourced at: https://github.com/HKUDS/RAG-Anything.\\n1\\nINTRODUCTION\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding\\nthe knowledge boundaries of Large Language Models (LLM) beyond their static training limita-\\ntions Zhang et al. (2025). By enabling dynamic retrieval and incorporation of external knowledge\\nduring inference, RAG systems transform static language models into adaptive, knowledge-aware\\nsystems. This capability has proven essential for applications requiring up-to-date information,\\ndomain-specific knowledge, or factual grounding that extends beyond pre-training corpora.\\nHowever, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the',\n",
       " 'domain-specific knowledge, or factual grounding that extends beyond pre-training corpora.\\nHowever, existing RAG frameworks focus exclusively on text-only knowledge while neglecting the\\nrich multimodal information present in real-world documents. This limitation fundamentally mis-\\naligns with how information exists in authentic environments. Real-world knowledge repositories are\\ninherently heterogeneous and multimodal Abootorabi et al. (2025). They contain rich combinations\\nof textual content, visual elements, structured tables, and mathematical expressions across diverse\\ndocument formats. This textual assumption forces existing RAG systems to either discard non-textual\\ninformation entirely or flatten complex multimodal content into inadequate textual approximations.\\nThe consequences of this limitation become particularly severe in document-intensive domains\\nwhere multimodal content carries essential meaning. Academic research, financial analysis, and',\n",
       " 'The consequences of this limitation become particularly severe in document-intensive domains\\nwhere multimodal content carries essential meaning. Academic research, financial analysis, and\\ntechnical documentation represent prime examples of knowledge-rich environments. These domains\\nfundamentally depend on visual and structured information. Critical insights are often encoded\\nexclusively in non-textual formats. Such formats resist meaningful conversion to plain text.\\nThe consequences of this limitation become particularly severe in knowledge-intensive domains where\\nmultimodal content carries essential meaning. Three representative scenarios illustrate the critical\\n∗Corresponding Author: Chao Huang\\n1\\narXiv:2510.12323v1  [cs.AI]  14 Oct 2025',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nneed for multimodal RAG capabilities. In Scientific Research, experimental results are primarily\\ncommunicated through plots, diagrams, and statistical visualizations. These contain core discoveries\\nthat remain invisible to text-only systems. Financial Analysis relies heavily on market charts,\\ncorrelation matrices, and performance tables. Investment insights are encoded in visual patterns\\nrather than textual descriptions. Additionally, Medical Literature Analysis depends on radiological\\nimages, diagnostic charts, and clinical data tables. These contain life-critical information essential for\\naccurate diagnosis and treatment decisions. Current RAG frameworks systematically exclude these\\nvital knowledge sources across all three scenarios. This creates fundamental gaps that render them\\ninadequate for real-world applications requiring comprehensive information understanding. Therefore,',\n",
       " 'vital knowledge sources across all three scenarios. This creates fundamental gaps that render them\\ninadequate for real-world applications requiring comprehensive information understanding. Therefore,\\nmultimodal RAG emerges as a critical advancement. It is necessary to bridge these knowledge gaps\\nand enable truly comprehensive intelligence across all modalities of human knowledge representation.\\nAddressing multimodal RAG presents three fundamental technical challenges that demand principled\\nsolutions. This makes it significantly more complex than traditional text-only approaches. The naive\\nsolution of converting all multimodal content to textual descriptions introduces severe information\\nloss. Visual elements such as charts, diagrams, and spatial layouts contain semantic richness that\\ncannot be adequately captured through text alone. These inherent limitations necessitate the design of\\neffective technical components. Such components must be specifically designed to handle multimodal',\n",
       " 'effective technical components. Such components must be specifically designed to handle multimodal\\ncomplexity and preserve the full spectrum of information contained within diverse content types.\\nTechnical Challenges. • First, the unified multimodal representation challenge requires seam-\\nlessly integrating diverse information types. The system must preserve their unique characteristics\\nand cross-modal relationships. This demands advanced multimodal encoders that can capture both\\nintra-modal and inter-modal dependencies without losing essential visual semantics. • Second, the\\nstructure-aware decomposition challenge demands intelligent parsing of complex layouts. The\\nsystem must maintain spatial and hierarchical relationships crucial for understanding. This requires\\nspecialized layout-aware parsing modules that can interpret document structure and preserve contex-\\ntual positioning of multimodal elements. • Third, the cross-modal retrieval challenge necessitates',\n",
       " 'specialized layout-aware parsing modules that can interpret document structure and preserve contex-\\ntual positioning of multimodal elements. • Third, the cross-modal retrieval challenge necessitates\\nsophisticated mechanisms that can navigate between different modalities. These mechanisms must\\nreason over their interconnections during retrieval. This calls for cross-modal alignment systems\\ncapable of understanding semantic correspondences across text, images, and structured data. These\\nchallenges are amplified in long-context scenarios. Relevant evidence is dispersed across multiple\\nmodalities and sections, requiring coordinated reasoning across heterogeneous information sources.\\nOur Contributions. To address these challenges, we introduce RAG-Anything, a unified framework\\nthat fundamentally reimagines multimodal knowledge representation and retrieval. Our approach\\nemploys a dual-graph construction strategy that elegantly bridges the gap between cross-modal',\n",
       " 'that fundamentally reimagines multimodal knowledge representation and retrieval. Our approach\\nemploys a dual-graph construction strategy that elegantly bridges the gap between cross-modal\\nunderstanding and fine-grained textual semantics. Rather than forcing diverse modalities into text-\\ncentric pipelines, RAG-Anything constructs complementary knowledge graphs that preserve both\\nmultimodal contextual relationships and detailed textual knowledge. This design enables seamless\\nintegration of visual elements, structured data, and mathematical expressions within a unified retrieval\\nframework. The system maintains semantic integrity across modalities while ensuring efficient\\ncross-modal reasoning capabilities throughout the process.\\nOur cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav-\\nigation with semantic similarity matching. This architecture addresses the fundamental limita-',\n",
       " 'Our cross-modal hybrid retrieval mechanism strategically combines structural knowledge nav-\\nigation with semantic similarity matching. This architecture addresses the fundamental limita-\\ntion of existing approaches that rely solely on embedding-based retrieval or keyword matching.\\nRAG-Anything leverages explicit graph relationships to capture multi-hop reasoning patterns. It\\nsimultaneously employs dense vector representations to identify semantically relevant content that\\nlacks direct structural connections. The framework introduces modality-aware query processing\\nand cross-modal alignment systems. These enable textual queries to effectively access visual and\\nstructured information. This unified approach eliminates the architectural fragmentation that plagues\\ncurrent multimodal RAG systems. It delivers superior performance particularly on long-context\\ndocuments where relevant evidence spans multiple modalities and document sections.',\n",
       " 'current multimodal RAG systems. It delivers superior performance particularly on long-context\\ndocuments where relevant evidence spans multiple modalities and document sections.\\nExperimental Validation. To validate the effectiveness of our proposed approach, we conduct com-\\nprehensive experiments on two challenging multimodal benchmarks: DocBench and MMLongBench.\\nOur evaluation demonstrates that RAG-Anything achieves superior performance across diverse do-\\nmains. The framework represents substantial improvements over state-of-the-art baselines. Notably,\\nour performance gains become increasingly significant as content length increases. We observe\\nparticularly pronounced advantages on long-context materials. This validates our core hypothesis\\n2',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nthat dual-graph construction and cross-modal hybrid retrieval are essential for handling complex\\nmultimodal materials. Our ablation studies reveal that graph-based knowledge representation provides\\nthe primary performance gains. Traditional chunk-based approaches fail to capture the structural\\nrelationships critical for multimodal reasoning. Case studies further demonstrate that our framework\\nexcels at precise localization within complex layouts. The system effectively disambiguates similar\\nterminology and navigates multi-panel visualizations through structure-aware retrieval mechanisms.\\n2\\nTHE RAG-ANYTHING FRAMEWORK\\n2.1\\nPRELIMINARY\\nRetrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for dynamically\\nexpanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning\\ncapabilities, their knowledge remains static and bounded by training data cutoffs. This creates an',\n",
       " 'expanding the knowledge boundaries of LLMs. While LLMs demonstrate exceptional reasoning\\ncapabilities, their knowledge remains static and bounded by training data cutoffs. This creates an\\never-widening gap with the rapidly evolving information landscape. RAG systems address this critical\\nlimitation by enabling LLMs to retrieve and incorporate external knowledge sources during inference.\\nThis transforms them from static repositories into adaptive, knowledge-aware systems.\\nThe Multimodal Reality: Beyond Text-Only RAG. Current RAG systems face a critical limitation\\nthat severely restricts their real-world deployment. Existing frameworks operate under the restrictive\\nassumption that knowledge corpus consists exclusively of plain textual documents. This assump-\\ntion fundamentally misaligns with how information exists in authentic environments. Real-world\\nknowledge repositories are inherently heterogeneous and multimodal, containing rich combinations',\n",
       " 'tion fundamentally misaligns with how information exists in authentic environments. Real-world\\nknowledge repositories are inherently heterogeneous and multimodal, containing rich combinations\\nof textual content, visual elements, structured data, and mathematical expressions. These diverse\\nknowledge sources span multiple document formats and presentation mediums, from research papers\\nand technical slides to web pages and interactive documents.\\n2.1.1\\nMOTIVATING RAG-ANYTHING\\nThis multimodal reality introduces fundamental technical challenges that expose the inadequacy of\\ncurrent text-only RAG approaches. Effective multimodal RAG requires unified indexing strategies\\nthat can handle disparate data types, cross-modal retrieval mechanisms that preserve semantic\\nrelationships across modalities, and sophisticated synthesis techniques that can coherently integrate\\ndiverse information sources. These challenges demand a fundamentally different architectural',\n",
       " 'relationships across modalities, and sophisticated synthesis techniques that can coherently integrate\\ndiverse information sources. These challenges demand a fundamentally different architectural\\napproach rather than incremental improvements to existing systems.\\nThe RAG-Anything framework introduces a unified approach for retrieving and processing knowl-\\nedge from heterogeneous multimodal information sources. Our system addresses the fundamental\\nchallenge of handling diverse data modalities and document formats within a retrieval pipeline.\\nThe framework comprises three core components: universal indexing for multimodal knowledge,\\ncross-modal adaptive retrieval, and knowledge-enhanced response generation. This integrated design\\nenables effective knowledge utilization across modalities while maintaining computational efficiency.\\n2.2\\nUNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\\nA key requirement for universal knowledge access is the ability to represent heterogeneous multimodal',\n",
       " '2.2\\nUNIVERSAL REPRESENTATION FOR HETEROGENEOUS KNOWLEDGE\\nA key requirement for universal knowledge access is the ability to represent heterogeneous multimodal\\ncontent in a unified, retrieval-oriented abstraction. Unlike existing pipelines that simply parse\\ndocuments into text segments, RAG-Anything introduces Multimodal Knowledge Unification. This\\nprocess decomposes raw inputs into atomic knowledge units while preserving their structural context\\nand semantic alignment. For instance, RAG-Anything ensures that figures remain grounded in their\\ncaptions, equations remain linked to surrounding definitions, and tables stay connected to explanatory\\nnarratives. This transforms heterogeneous files into a coherent substrate for cross-modal retrieval.\\nFormally, each knowledge source ki ∈K (e.g., a web page) is decomposed into atomic content units:\\nki\\nDecompose\\n−−−−−−−→{cj = (tj, xj)}ni\\nj=1,\\n(1)\\nwhere each unit cj consists of a modality type tj ∈text, image, table, equation, . . . and its corre-',\n",
       " 'ki\\nDecompose\\n−−−−−−−→{cj = (tj, xj)}ni\\nj=1,\\n(1)\\nwhere each unit cj consists of a modality type tj ∈text, image, table, equation, . . . and its corre-\\nsponding raw content xj. The content xj represents the extracted information from the original\\nknowledge source, processed in a modality-aware manner to preserve semantic integrity.\\n3',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nParallel\\nParser\\nMultimodal Knowledge Unification\\n...\\nImage Info\\n...\\nEquation Info\\nText Info\\n...\\nTable Info\\nHierarchical Text \\nExtraction\\nImage Caption & \\nMetadata Extraction\\nLaTeX Equation \\nRecognition\\nTable Structure & \\nContent Parsing\\nDual-Graph Construction for Multimodal Knowledge \\n(each document) \\nStructured Content List\\nText Encoder\\nMulti-modal \\nProcessors\\nEntity & Relation\\nExtraction\\nBeekeeper\\nBee\\nObserve\\nKnowledge Graph\\nMerged Node\\nChild Node\\nParent Node: Multi-\\nmodal Instance\\nVLM/LLM\\nMerged\\n...\\nTextual Multi-modal Info\\nCross-Modal Knowledge\\nGraph\\nText-Based Knowledge Graph\\nKG over All Documents\\nMerged\\nText VDB\\nMulti-modal VDB\\nVDB over All\\nDocuments\\nQuery\\nCould you share insights \\non the experimental \\nresults and data tables?\\nResponse\\nBased on the experimental \\ndata, the results revealed...\\nSemantic Similarity \\nMatching\\nStructural Knowledge \\nNavigation\\nQuery High-/Low-level Keys Extraction\\nHybrid Retrieved Info\\n...',\n",
       " 'Response\\nBased on the experimental \\ndata, the results revealed...\\nSemantic Similarity \\nMatching\\nStructural Knowledge \\nNavigation\\nQuery High-/Low-level Keys Extraction\\nHybrid Retrieved Info\\n...\\nMulti-modal Info Processing\\nText Info Processing\\nLLM\\nText Encoder\\nVector Database\\nFigure 1: Overview of our proposed universal RAG framework RAG-Anything.\\nTo ensure high-fidelity extraction, RAG-Anything leverages specialized parsers for different content\\ntypes. Text is segmented into coherent paragraphs or list items. Figures are extracted with associated\\nmetadata such as captions and cross-references. Tables are parsed into structured cells with headers\\nand values. Mathematical expressions are converted into symbolic representations. The resulting xj\\npreserves both content and structural context within the source. This provides a faithful, modality-\\nconsistent representation. The decomposition abstracts diverse file formats into atomic units while',\n",
       " 'consistent representation. The decomposition abstracts diverse file formats into atomic units while\\nmaintaining their hierarchical order and contextual relationships. This canonicalization enables\\nuniform processing, indexing, and retrieval of multimodal content within our framework.\\n2.2.1\\nDUAL-GRAPH CONSTRUCTION FOR MULTIMODAL KNOWLEDGE\\nWhile multimodal knowledge unification provides a uniform abstraction across modalities, directly\\nconstructing a single unified graph often risks overlooking modality-specific structural signals. The\\nproposed RAG-Anything addresses this challenge through a dual-graph construction strategy. The\\nsystem first builds a cross-modal knowledge graph that faithfully grounds non-textual modalities\\nwithin their contextual environment. It then constructs a text-based knowledge graph using es-\\ntablished text-centric extraction pipelines. These complementary graphs are merged through entity',\n",
       " 'within their contextual environment. It then constructs a text-based knowledge graph using es-\\ntablished text-centric extraction pipelines. These complementary graphs are merged through entity\\nalignment. This design ensures accurate cross-modal grounding and comprehensive coverage of\\ntextual semantics, enabling richer knowledge representation and robust retrieval.\\n• Cross-Modal Knowledge Graph: Non-textual content like images, tables, and equations contains\\nrich semantic information that traditional text-only approaches often overlook. To preserve this\\nknowledge, RAG-Anything constructs a multimodal knowledge graph where non-text atomic\\nunits are transformed into structured graph entities. RAG-Anything leverages multimodal large\\nlanguage models to derive two complementary textual representations from each atomic content\\nunit. The first is a detailed description dchunk\\nj\\noptimized for cross-modal retrieval. The second is\\nan entity summary eentity\\nj',\n",
       " 'unit. The first is a detailed description dchunk\\nj\\noptimized for cross-modal retrieval. The second is\\nan entity summary eentity\\nj\\ncontaining key attributes such as entity name, type, and description for\\ngraph construction. The generation process is context-aware, processing each unit with its local\\nneighborhood Cj = {ck | |k −j| ≤δ}, where δ controls the contextual window size. This ensures\\nrepresentations accurately reflect each unit’s role within the broader document structure.\\nBuilding on these textual representations, RAG-Anything constructs the graph structure using non-\\ntext units as anchor points. For each non-text unit cj, the graph extraction routine R(·) processes\\nits description dchunk\\nj\\nto identify fine-grained entities and relations:\\n(Vj, Ej) = R(dchunk\\nj\\n),\\n(2)\\nwhere Vj and Ej denote the sets of intra-chunk entities and their relations, respectively. Each\\natomic non-text unit is associated with a multimodal entity node vmm\\nj\\nthat serves as an anchor for\\n4',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nits intra-chunk entities through explicit belongs_to edges:\\n˜V = {vmm\\nj\\n}j ∪\\n[\\nj\\nVj,\\n(3)\\n˜E =\\n[\\nj\\nEj ∪\\n[\\nj\\n{(u\\nbelongs_to\\n−−−−−−−→vmm\\nj\\n) : u ∈Vj}.\\n(4)\\nThis construction preserves modality-specific grounding while ensuring non-textual content is con-\\ntextualized by its textual neighborhood. This enables reliable cross-modal retrieval and reasoning.\\n• Text-based Knowledge Graph: For text modality chunks, we construct a traditional text-based\\nknowledge graph following established methodologies similar to LightRAG (Guo et al., 2024)\\nand GraphRAG (Edge et al., 2024). The extraction process operates directly on textual content xj\\nwhere tj = text, leveraging named entity recognition and relation extraction techniques to identify\\nentities and their semantic relationships. Given the rich semantic information inherent in textual\\ncontent, multimodal context integration is not required for this component. The resulting text-based',\n",
       " 'entities and their semantic relationships. Given the rich semantic information inherent in textual\\ncontent, multimodal context integration is not required for this component. The resulting text-based\\nknowledge graph captures explicit knowledge and semantic connections present in textual portions\\nof documents, complementing the multimodal graph’s cross-modal grounding capabilities.\\n2.2.2\\nGRAPH FUSION AND INDEX CREATION\\nThe separate cross-modal and text-based knowledge graphs capture complementary aspects of\\ndocument semantics. Integrating them creates a unified representation leveraging visual-textual\\nassociations and fine-grained textual relationships for enhanced retrieval.\\n• (i) Entity Alignment and Graph Fusion. To create a unified knowledge representation, we\\nmerge the multimodal knowledge graph ( ˜V , ˜E) and text-based knowledge graph through entity align-\\nment. This process uses entity names as primary matching keys to identify semantically equivalent',\n",
       " 'merge the multimodal knowledge graph ( ˜V , ˜E) and text-based knowledge graph through entity align-\\nment. This process uses entity names as primary matching keys to identify semantically equivalent\\nentities across both graph structures. The integration consolidates their representations, creating\\na comprehensive knowledge graph G = (V, E). This graph captures both multimodal contextual\\nrelationships and text-based semantic connections. The merged graph provides a holistic view of the\\ndocument collection. This enables effective retrieval by leveraging visual-textual associations from\\nthe multimodal graph and fine-grained textual knowledge relationships from the text-based graph.\\n• (ii) Dense Representation Generation. To enable efficient similarity-based retrieval, we construct\\na comprehensive embedding table T that encompasses all components generated during the indexing\\nprocess. We encode dense representations for all graph entities, relationships, and atomic content',\n",
       " 'a comprehensive embedding table T that encompasses all components generated during the indexing\\nprocess. We encode dense representations for all graph entities, relationships, and atomic content\\nchunks across modalities using an appropriate encoder. This creates a unified embedding space where\\neach component s ∈entities, relations, chunks is mapped to its corresponding dense representation:\\nT = emb(s) : s ∈V ∪E ∪cjj,\\n(5)\\nwhere emb(·) denotes the embedding function tailored for each component type. Together, the\\nunified knowledge graph G and the embedding table T constitute the complete retrieval index\\nI = (G, T ). This provides both structural knowledge representation and dense vector space for\\nefficient cross-modal similarity search during the subsequent retrieval stage.\\n2.3\\nCROSS-MODAL HYBRID RETRIEVAL\\nThe retrieval stage operates on the index I = (G, T ) to identify relevant knowledge components for a',\n",
       " '2.3\\nCROSS-MODAL HYBRID RETRIEVAL\\nThe retrieval stage operates on the index I = (G, T ) to identify relevant knowledge components for a\\ngiven user query. Traditional RAG methods face significant limitations when dealing with multimodal\\ndocuments. They typically rely on semantic similarity within single modalities and fail to capture the\\nrich interconnections between visual, mathematical, tabular, and textual elements. To address these\\nchallenges, our framework introduces a cross-modal hybrid retrieval mechanism. This mechanism\\nleverages structural knowledge and semantic representations across heterogeneous modalities.\\nModality-Aware Query Encoding. Given a user query q, we first perform modality-aware query\\nanalysis to extract lexical cues and potential modality preferences embedded within the query.\\nFor instance, queries containing terms such as \"figure,\" \"chart,\" \"table,\" or \"equation\" provide',\n",
       " 'analysis to extract lexical cues and potential modality preferences embedded within the query.\\nFor instance, queries containing terms such as \"figure,\" \"chart,\" \"table,\" or \"equation\" provide\\nexplicit signals about the expected modality of relevant information. We then compute a unified text\\nembedding eq using the same encoder employed during indexing, ensuring consistency between\\n5',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nquery and knowledge representations. This embedding-based approach enables cross-modal retrieval\\ncapabilities where textual queries can effectively access multimodal content through their shared\\nrepresentations, maintaining retrieval consistency while preserving cross-modal accessibility.\\nHybrid Knowledge Retrieval Architecture. Recognizing that knowledge relevance manifests\\nthrough both explicit structural connections and implicit semantic relationships, we design a hybrid\\nretrieval architecture that strategically combines two complementary mechanisms.\\n• (i) Structural Knowledge Navigation. This mechanism addresses the challenge of capturing\\nexplicit relationships and multi-hop reasoning patterns. Traditional keyword-based retrieval often\\nfails to identify knowledge connected through intermediate entities or cross-modal relationships. To\\novercome this limitation, we exploit the structural properties encoded within our unified knowledge',\n",
       " 'overcome this limitation, we exploit the structural properties encoded within our unified knowledge\\ngraph G. We employ keyword matching and entity recognition to locate relevant graph components.\\nThe retrieval process begins with exact entity matching against query terms.\\nWe then perform strategic neighborhood expansion to include related entities and relationships within\\na specified hop distance. This structural approach proves particularly effective at uncovering high-\\nlevel semantic connections and entity-relation patterns that span multiple modalities. It capitalizes\\non the rich cross-modal linkages established in our multimodal knowledge graph. The structural\\nnavigation yields candidate set Cstru(q) containing relevant entities, relationships, and their associated\\ncontent chunks that provide comprehensive contextual information.\\n• (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying',\n",
       " 'content chunks that provide comprehensive contextual information.\\n• (ii) Semantic Similarity Matching. This mechanism addresses the challenge of identifying\\nsemantically relevant knowledge that lacks explicit structural connections. While structural navigation\\nexcels at following explicit relationships, it may miss relevant content that is semantically related but\\nnot directly connected in the graph topology. To bridge this gap, we conduct dense vector similarity\\nsearch between the query embedding eq and all components stored in embedding table T .\\nThis approach encompasses atomic content chunks across all modalities, graph entities, and relation-\\nship representations, enabling fine-grained semantic matching that can surface relevant knowledge\\neven when traditional lexical or structural signals are absent. The learned embedding space captures\\nnuanced semantic relationships and contextual similarities that complement the explicit structural',\n",
       " 'even when traditional lexical or structural signals are absent. The learned embedding space captures\\nnuanced semantic relationships and contextual similarities that complement the explicit structural\\nsignals from the navigation mechanism. This retrieval pathway returns the top-k most semantically\\nsimilar chunks Cseman(q) ranked by cosine similarity scores, ensuring comprehensive coverage of\\nboth structurally and semantically relevant knowledge.\\nCandidate Pool Unification. Both retrieval pathways may return overlapping candidates with\\ndiffering relevance signals. This necessitates a principled approach to unify and rank results. Retrieval\\ncandidates from both pathways are unified into a comprehensive candidate pool: C(q) = Cstru(q) ∪\\nCseman(q). Simply merging candidates would ignore distinct evidence each pathway provides. It\\nwould fail to account for redundancy between retrieved content.\\n• (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion',\n",
       " 'would fail to account for redundancy between retrieved content.\\n• (i) Multi-Signal Fusion Scoring. To address these challenges, we apply a sophisticated fusion\\nscoring mechanism integrating multiple complementary relevance signals. These include structural\\nimportance derived from graph topology, semantic similarity scores from embedding space, and query-\\ninferred modality preferences obtained through lexical analysis. This multi-faceted scoring approach\\nensures that final ranked candidates C⋆(q) effectively balance structural knowledge relationships with\\nsemantic relevance while appropriately weighting different modalities based on query characteristics.\\n• (ii) Hybrid Retrieval Integration. The resulting hybrid retrieval mechanism enables our framework\\nto leverage the complementary strengths of both knowledge graphs and dense representations. This\\nprovides comprehensive coverage of relevant multimodal knowledge for response generation.\\n2.4\\nFROM RETRIEVAL TO SYNTHESIS',\n",
       " 'provides comprehensive coverage of relevant multimodal knowledge for response generation.\\n2.4\\nFROM RETRIEVAL TO SYNTHESIS\\nEffective multimodal question answering requires preserving rich visual semantics while maintaining\\ncoherent grounding across heterogeneous knowledge sources. Simple text-only approaches lose\\ncrucial visual information, while naive multimodal methods struggle with coherent cross-modal\\nintegration. Our synthesis stage addresses these challenges by systematically combining retrieved\\nmultimodal knowledge into comprehensive, evidence-grounded responses.\\n• (i) Building Textual Context. Given the top-ranked retrieval candidates C⋆(q), we construct a\\nstructured textual context. We concatenate textual representations of all retrieved components, includ-\\n6',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nTable 1: Statistics of Experimental Datasets.\\nDataset\\n# Documents\\n# Avg. Pages\\n# Avg. Tokens\\n# Doc Types\\n# Questions\\nDocBench\\n229\\n66\\n46377\\n5\\n1102\\nMMLongBench\\n135\\n47.5\\n21214\\n7\\n1082\\ning entity summaries, relationship descriptions, and chunk contents. The concatenation incorporates\\nappropriate delimiters to indicate modality types and hierarchical origins. This approach ensures the\\nlanguage model can effectively parse and reason over heterogeneous knowledge components.\\n• (ii) Recovering Visual Content. For multimodal chunks corresponding to visual artifacts, we\\nperform dereferencing to recover original visual content, creating V⋆(q). This design maintains con-\\nsistency with our unified embedding strategy. Textual proxies enable efficient retrieval while authentic\\nvisual content provides rich semantics necessary for sophisticated reasoning during synthesis.',\n",
       " 'sistency with our unified embedding strategy. Textual proxies enable efficient retrieval while authentic\\nvisual content provides rich semantics necessary for sophisticated reasoning during synthesis.\\nThe synthesis process jointly conditions on both the assembled comprehensive textual context and\\ndereferenced visual artifacts using a vision-language model:\\nResponse = VLM(q, P(q), V⋆(q)),\\n(6)\\nwhere the VLM integrates information from query, textual context, and visual content. This unified\\nconditioning enables sophisticated visual interpretation while maintaining grounding in retrieved\\nevidence. The resulting responses are both visually informed and factually grounded.\\n3\\nEVALUATION\\n3.1\\nEXPERIMENTAL SETTINGS\\nEvaluation Datasets. We conduct comprehensive evaluations on two challenging multimodal\\nDocument Question Answering (DQA) benchmarks that reflect real-world complexity and diversity.\\nDocBench (Zou et al., 2024) provides a rigorous testbed with 229 multimodal documents spanning',\n",
       " 'Document Question Answering (DQA) benchmarks that reflect real-world complexity and diversity.\\nDocBench (Zou et al., 2024) provides a rigorous testbed with 229 multimodal documents spanning\\nfive critical domains: Academia, Finance, Government, Laws, and News. The dataset includes 1,102\\nexpert-crafted question-answer pairs. These documents are notably extensive, averaging 66 pages and\\napproximately 46,377 tokens, which presents substantial challenges for long-context understanding.\\nMMLongBench (Ma et al., 2024) complements this evaluation by focusing specifically on long-\\ncontext multimodal document comprehension. It features 135 documents across 7 diverse document\\ntypes with 1,082 expert-annotated questions. Together, these benchmarks provide comprehensive\\ncoverage of the multimodal document understanding challenges that RAG-Anything aims to address.\\nThey ensure our evaluation captures both breadth across domains and depth in document complexity.',\n",
       " 'coverage of the multimodal document understanding challenges that RAG-Anything aims to address.\\nThey ensure our evaluation captures both breadth across domains and depth in document complexity.\\nDetailed dataset statistics and characteristics are provided in Appendix A.1.\\nBaselines. We compare RAG-Anything against the following methods for performance evaluation:\\n• GPT-4o-mini: A powerful multimodal language model with native text and image understanding\\ncapabilities. Its 128K token context window enables direct processing of entire documents. We\\nevaluate this model as a strong baseline for long-context multimodal understanding.\\n• LightRAG (Guo et al., 2024): A graph-enhanced RAG system that integrates structured knowledge\\nrepresentation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation\\ninformation and broader semantic context, improving retrieval precision and response coherence.',\n",
       " 'representation with dual-level retrieval mechanisms. It captures both fine-grained entity-relation\\ninformation and broader semantic context, improving retrieval precision and response coherence.\\n• MMGraphRAG (Wan & Yu, 2025): A multimodal retrieval framework that constructs unified\\nknowledge graphs spanning textual and visual content. This method employs spectral clustering\\nfor multimodal entity analysis and retrieves context along reasoning paths to guide generation.\\nExperimental Settings. In our experiments, we implement all baselines using GPT-4o-mini as\\nthe backbone LLM. Documents are parsed using MinerU (Wang et al., 2024) to extract text, im-\\nages, tables, and equations for downstream RAG processing. For the retrieval pipeline, we em-\\nploy the text-embedding-3-large model with 3072-dimensional embeddings. We use the\\nbge-reranker-v2-m3 model for reranking. For graph-based RAG methods, we enforce a com-',\n",
       " 'ploy the text-embedding-3-large model with 3072-dimensional embeddings. We use the\\nbge-reranker-v2-m3 model for reranking. For graph-based RAG methods, we enforce a com-\\nbined entity-and-relation token limit of 20,000 tokens and a chunk token limit of 12,000 tokens.\\n7',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nTable 2: Accuracy (%) on DocBench Dataset. Performance results with best scores highlighted in\\ndark blue and second-best in light blue. Domain categories include Academia (Aca.), Finance\\n(Fin.), Government (Gov.), Legal Documents (Law), and News Articles (News). Document types are\\ncategorized as Text-only (Txt.), Multimodal (Mm.), and Unanswerable queries (Una.).\\nMethod\\nDomains\\nTypes\\nOverall\\nAca.\\nFin.\\nGov.\\nLaw.\\nNews\\nTxt.\\nMm.\\nUna.\\nGPT-4o-mini\\n40.3\\n46.9\\n60.3\\n59.2\\n61.0\\n61.0\\n43.8\\n49.6\\n51.2\\nLightRAG\\n53.8\\n56.2\\n59.5\\n61.8\\n65.7\\n85.0\\n59.7\\n46.8\\n58.4\\nMMGraphRAG\\n64.3\\n52.8\\n64.9\\n40.0\\n61.5\\n67.6\\n66.0\\n60.5\\n61.0\\nRAGAnything\\n61.4\\n67.0\\n61.5\\n60.2\\n66.3\\n85.0\\n76.3\\n46.0\\n63.4\\nTable 3: Accuracy (%) on MMLongBench across different domains and overall performance. Best re-\\nsults are highlighted in dark blue and second-best in light blue.. Domain categories include Research\\nReports/Introductions (Res.), Tutorials/Workshops (Tut.), Academic Papers (Acad.), Guidebooks',\n",
       " 'sults are highlighted in dark blue and second-best in light blue.. Domain categories include Research\\nReports/Introductions (Res.), Tutorials/Workshops (Tut.), Academic Papers (Acad.), Guidebooks\\n(Guid.), Brochures (Broch.), Administration/Industry Files (Admin.), and Financial Reports (Fin.).\\nMethod\\nDomains\\nOverall\\nRes.\\nTut.\\nAcad.\\nGuid.\\nBroch.\\nAdmin.\\nFin.\\nGPT-4o-mini\\n35.5\\n44.0\\n24.6\\n33.1\\n29.5\\n46.8\\n31.1\\n33.5\\nLightRAG\\n40.8\\n34.1\\n36.2\\n39.4\\n41.0\\n44.4\\n38.3\\n38.9\\nMMGraphRAG\\n40.8\\n36.5\\n35.7\\n35.8\\n28.2\\n46.9\\n38.5\\n37.7\\nRAGAnything\\n46.6\\n43.5\\n38.7\\n43.9\\n34.0\\n45.7\\n43.6\\n42.8\\nOutputs are constrained to a one-sentence format. For the baseline GPT-4o-mini in our QA scenario,\\ndocuments are concatenated into image form with a maximum of 50 pages per document, rendered at\\n144 dpi. Finally, all query results are evaluated for accuracy by GPT-4o-mini.\\n3.2\\nPERFORMANCE COMPARISON\\nSuperior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior',\n",
       " '144 dpi. Finally, all query results are evaluated for accuracy by GPT-4o-mini.\\n3.2\\nPERFORMANCE COMPARISON\\nSuperior Performance and Cross-Domain Generalization. RAG-Anything demonstrates superior\\noverall performance over baselines through its unified multimodal framework. Unlike LightRAG,\\nwhich is restricted to text-only content processing, RAG-Anything treats text, images, tables, and\\nequations as first-class entities. MMGraphRAG only adds basic image processing while treating\\ntables and equations as plain text, missing crucial structural information. RAG-Anything introduces\\na comprehensive dual-graph construction strategy that preserves structural relationships across all\\nmodalities. This unified approach enables superior performance across both evaluation benchmarks.\\nEnhanced Long-Context Performance. RAG-Anything demonstrates superior performance on\\nlong-context documents. The framework excels where relevant evidence is dispersed across multiple',\n",
       " 'Enhanced Long-Context Performance. RAG-Anything demonstrates superior performance on\\nlong-context documents. The framework excels where relevant evidence is dispersed across multiple\\nmodalities and sections. It achieves the best results in information-dense domains such as Research\\nReports and Financial Reports on MMLongBench. These improvements stem from the structured\\ncontext injection mechanism. This mechanism integrates dual-graph construction for cross-page entity\\nalignment. It combines semantic retrieval with structural navigation. The framework also employs\\nmodality-aware processing for efficient context window utilization. Unlike baselines that cannot\\nuniformly process diverse modalities, RAG-Anything effectively captures scattered multimodal\\nevidence. Its cross-modal hybrid retrieval architecture combines structural knowledge navigation\\nwith semantic similarity matching. This enables the framework to leverage both explicit relationships',\n",
       " 'evidence. Its cross-modal hybrid retrieval architecture combines structural knowledge navigation\\nwith semantic similarity matching. This enables the framework to leverage both explicit relationships\\nand implicit semantic connections across modalities.\\nTo systematically evaluate model performance across varying document lengths, we conducted\\ncomprehensive experiments on both datasets. As illustrated in Figure 2, RAG-Anything and MM-\\nGraphRAG exhibit comparable performance on shorter documents. However, RAG-Anything’s\\nadvantages become increasingly pronounced as document length grows. On DocBench, the perfor-\\nmance gap expands dramatically to over 13 points for documents exceeding 100 pages (68.2% vs.\\n8',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 2: Performance evaluation across documents of varying lengths.\\nTable 4: Ablation study results on DocBench. The “Chunk-only” variant bypasses dual-graph\\nconstruction and relies solely on traditional chunk-based retrieval, while “w/o Reranker” eliminates\\ncross-modal reranking but preserves the core graph-based architecture.\\nMethod\\nDomains\\nTypes\\nOverall\\nAca.\\nFin.\\nGov.\\nLaw.\\nNews\\nTxt.\\nMm.\\nUna.\\nChunk-only\\n55.8\\n61.5\\n60.1\\n60.7\\n64.0\\n81.6\\n66.2\\n43.5\\n60.0\\nw/o Reranker\\n60.9\\n63.5\\n58.8\\n60.2\\n68.6\\n81.7\\n74.7\\n45.4\\n62.4\\nRAGAnything\\n61.4\\n67.0\\n61.5\\n60.2\\n66.3\\n85.0\\n76.3\\n46.0\\n63.4\\n54.6% for 101–200 pages; 68.8% vs. 55.0% for 200+ pages). On MMLongBench, RAG-Anything\\ndemonstrates consistent improvements across all length categories, achieving accuracy gains of 3.4\\npoints for 11–50 pages, 9.3 points for 51–100 pages, and 7.9 points for 101–200 pages. These\\nfindings confirm that our dual-graph construction and cross-modal hybrid retrieval mechanism is',\n",
       " 'points for 11–50 pages, 9.3 points for 51–100 pages, and 7.9 points for 101–200 pages. These\\nfindings confirm that our dual-graph construction and cross-modal hybrid retrieval mechanism is\\nparticularly effective for long-document reasoning tasks.\\n3.3\\nARCHITECTURAL VALIDATION WITH ABLATION STUDIES\\nTo isolate and quantify the contributions of key architectural components in RAG-Anything, we\\nconducted systematic ablation studies examining two critical design choices. Given that our approach\\nfundamentally differs from existing methods through dual-graph construction and hybrid retrieval,\\nwe specifically evaluated: i) Chunk-only, which bypasses graph construction entirely and relies\\nsolely on traditional chunk-based retrieval, and ii) w/o Reranker, which eliminates the cross-modal\\nreranking component while preserving the core graph-based architecture.\\nAs demonstrated in Table 4, the results validate our architectural design through striking performance',\n",
       " 'reranking component while preserving the core graph-based architecture.\\nAs demonstrated in Table 4, the results validate our architectural design through striking performance\\nvariations. • Graph Construction is Essential. The chunk-only variant achieves merely 60.0%\\naccuracy with substantial cross-domain drops. This demonstrates that traditional chunking fails to\\ncapture structural and cross-modal relationships essential for multimodal documents. • Reranking\\nProvides Marginal Gains. Removing the reranker yields only a modest decline to 62.4%, while the\\nfull model achieves 63.4% accuracy. This indicates that cross-modal reranking provides valuable\\nrefinement, but primary gains stem from our graph-based retrieval and cross-modal integration.\\n3.4\\nCASE STUDIES\\nMultimodal documents contain rich structural information within each modality. Understanding\\nthese intra-modal structures is crucial for accurate reasoning. We analyze two representative cases',\n",
       " 'Multimodal documents contain rich structural information within each modality. Understanding\\nthese intra-modal structures is crucial for accurate reasoning. We analyze two representative cases\\nfrom DocBench to demonstrate how RAG-Anything leverages these structures. These cases highlight\\na key limitation of existing methods. Baselines either rely on superficial textual cues or flatten\\ncomplex visual elements into plain text. In contrast, RAG-Anything builds modality-aware graphs\\nthat preserve essential relationships (e.g., table header↔cell↔unit edges; panel↔caption↔axis\\nedges). This enables precise reasoning over complex document layouts.\\n• Case 1: Multi-panel Figure Interpretation. This case examines a common scenario in academic\\nliterature. Researchers often need to compare results across different experimental conditions. These\\nresults are typically presented in multi-panel visualizations. Figure 3 shows a challenging t-SNE\\n9',\n",
       " \"RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nMultimodel Document\\nEvidence figure in the document\\nQuestion: Which model's style space shows a clearer separation\\nbetween different styles according to Figure 2?\\nGPT-4o-mini🤔: \\nAccording to Figure 2, the VAE\\nmodel's style space shows a clearer\\nseparation between different styles.\\nMMGraphRAG🤔: \\nAccording to Figure 2, the model's style space\\nshows a clearer separation between different styles\\nin the Variational Autoencoder (VAE) compared to\\nthe Deterministic Autoencoder (DAE).\\nRAG-Anything(Correct😉):\\nThe DAE model's style space shows a clearer\\nseparation between different styles according to\\nFigure 2.\\nLightRAG🤔：\\nAccording to Figure 2, the Variational Autoencoder\\n(VAE) shows a clearer separation between different\\nstyles \\nin \\nits \\nstyle \\nspace \\ncompared \\nto \\nthe\\nDeterministic Autoencoder (DAE). \\n(DAE shows a clearer seperation than VAE in Style Space)\\nFigure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation\",\n",
       " 'to \\nthe\\nDeterministic Autoencoder (DAE). \\n(DAE shows a clearer seperation than VAE in Style Space)\\nFigure 3: Multi-panel figure interpretation case. The query requires identifying cluster separation\\npatterns from the style-space panel, while avoiding confusion from the adjacent content-space panel.\\nvisualization with multiple subpanels. The query requires distinguishing between two related but\\ndistinct panels. RAG-Anything constructs a visual-layout graph where panels, axis titles, legends,\\nand captions become nodes. Key edges encode semantic relationships. Panels contain specific plots.\\nCaptions provide contextual information. Subfigures relate hierarchically. This structure guides the\\nretriever to focus on the style-space panel for comparing cluster separation patterns. The system\\navoids confusion from the adjacent content space panel. This panel shows less clear distinctions.\\nMultimodel Document\\nEvidence table in the document',\n",
       " \"avoids confusion from the adjacent content space panel. This panel shows less clear distinctions.\\nMultimodel Document\\nEvidence table in the document\\nQuestion: What was Novo Nordisk's total amount spent on wages and salaries in 2020?\\nGPT-4o-mini🤔: \\nNovo Nordisk's total amount\\nspent on wages and salaries in\\n2020 was DKK 32,928 million.\\nMMGraphRAG🤔: \\nNovo Nordisk spent a total of\\n11,503 million DKK on wages\\nand salaries in 2020.\\nRAG-Anything(Correct😉):\\nNovo Nordisk's total amount spent on wages\\nand salaries in 2020 was DKK 26,778 million. \\nLightRAG🤔：\\nNovo Nordisk spent DKK 11,503 million\\non wages and salaries in 2020.\\n(Identifying the true evidence is the key)\\nFigure 4: Financial table navigation case. The query involves locating the specific intersection of\\n“Wages and salaries” row and “2020” column amid similar terminological entries.\\n• Case 2: Financial Table Navigation. This case addresses a common challenge in financial\",\n",
       " '“Wages and salaries” row and “2020” column amid similar terminological entries.\\n• Case 2: Financial Table Navigation. This case addresses a common challenge in financial\\ndocument analysis. Analysts must extract specific metrics from tables with similar terminology\\nand multiple time periods. Figure 4 shows this scenario. The query involves resolving ambiguous\\nfinancial terms and selecting the correct column for a specified year.\\nRAG-Anything transforms the financial report table into a structured graph. Each row header, column\\nheader (year), data cell, and unit becomes a node. The edges capture key relationships: row-of,\\ncolumn-of, header-applies-to, and unit-of. This structure enables precise navigation. The retriever\\nfocuses on the row “Wages and salaries” and the column for “2020”. It directs attention to the\\ntarget cell (26,778 million). The system successfully disambiguates nearby entries like “Share-based',\n",
       " 'focuses on the row “Wages and salaries” and the column for “2020”. It directs attention to the\\ntarget cell (26,778 million). The system successfully disambiguates nearby entries like “Share-based\\npayments.” Competing methods treat tables as linear text. They often confuse numerical spans and\\nyears. This leads to significantly inaccurate answers. RAG-Anything explicitly models relationships\\nwithin the table. It achieves precise selection and numeric grounding. This ensures accurate responses.\\n• Key Insights. Both cases demonstrate how RAG-Anything’s structure-aware design delivers\\ntargeted advantages. Our approach transforms documents into explicit graph representations. These\\ngraphs capture intra-modal relationships that traditional methods miss. In figures, connections\\nbetween panels, captions, and axes enable panel-level comparisons. This goes beyond keyword\\nmatching. In tables, row–column–unit graphs ensure accurate identification through modeling.',\n",
       " 'between panels, captions, and axes enable panel-level comparisons. This goes beyond keyword\\nmatching. In tables, row–column–unit graphs ensure accurate identification through modeling.\\nThis structure-aware retrieval design reduces confusion from repeated terminology and complex\\nlayouts. Traditional RAG systems struggle with these scenarios due to lack of structural understanding.\\nEven MMGraphRAG fails here because it only considers image modality entities. It ignores other\\nmodality entities like table cells, row headers, and column headers. RAG-Anything’s comprehensive\\ngraph representation captures all modality-specific entities and their relationships. This enables\\nprecise, modality-specific grounding that leads to consistent improvements in document Q&A tasks\\nrequiring fine-grained localization. Additional cases are available in Appendix A.2.\\n4\\nRELATED WORK\\n• Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with',\n",
       " 'requiring fine-grained localization. Additional cases are available in Appendix A.2.\\n4\\nRELATED WORK\\n• Graph-Enhanced Retrieval-Augmented Generation. Large language models struggle with\\nlong-context inputs and multi-hop queries, failing to precisely locate dispersed evidence (Zhang et al.,\\n10',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\n2025). Graph structures address this limitation by introducing explicit relational modeling, improving\\nboth retrieval efficiency and reasoning accuracy (Bei et al., 2025).\\nSince GraphRAG (Edge et al., 2024), research has evolved along two complementary directions.\\nFirst, graph construction approaches optimize structures for retrieval efficiency, ranging from Ligh-\\ntRAG’s (Guo et al., 2024) sparsified indices to neural models like GNN-RAG (Mavromatis & Karypis,\\n2024) and memory-augmented variants like HippoRAG (Jimenez Gutierrez et al., 2024). Second,\\nknowledge aggregation approaches integrate information for multi-level reasoning through hier-\\narchical methods like RAPTOR (Sarthi et al., 2024) and ArchRAG (Wang et al., 2025). Despite\\nthese advances, existing systems remain text-centric with homogeneous inputs. This limits their\\napplicability to multimodal documents and constrains robust reasoning over heterogeneous content.',\n",
       " 'these advances, existing systems remain text-centric with homogeneous inputs. This limits their\\napplicability to multimodal documents and constrains robust reasoning over heterogeneous content.\\nRAG-Anything addresses this gap by extending GraphRAG to all modalities.\\n• Multimodal Retrieval-Augmented Generation. Multimodal RAG represents a natural evolution\\nfrom text-based RAG systems, addressing the need to integrate external knowledge from diverse\\ndata modalities for comprehensive response generation (Abootorabi et al., 2025). However, current\\napproaches are fundamentally constrained by their reliance on modality-specific architectures. Exist-\\ning methods demonstrate these constraints across domains: VideoRAG (Ren et al., 2025) employs\\ndual-channel architectures for video understanding while MM-VID (Lin et al., 2023) converts videos\\nto text, losing visual information; VisRAG (Yu et al., 2025) preserves document layouts as images',\n",
       " 'dual-channel architectures for video understanding while MM-VID (Lin et al., 2023) converts videos\\nto text, losing visual information; VisRAG (Yu et al., 2025) preserves document layouts as images\\nbut misses granular relationships; MMGraphRAG (Wan & Yu, 2025) links scene graphs with textual\\nrepresentations but suffers from structural blindness—treating tables and formulas as plain text\\nwithout proper entity extraction, losing structural information for reasoning.\\nThe fundamental problem underlying these limitations is architectural fragmentation. Current systems\\nrequire specialized processing pipelines for each modality. This creates poor generalizability as new\\nmodalities demand custom architectures and fusion mechanisms. Such fragmentation introduces\\ncross-modal alignment difficulties, modality biases, and information bottlenecks. These issues\\nsystematically compromise system performance and scalability. RAG-Anything addresses this',\n",
       " 'cross-modal alignment difficulties, modality biases, and information bottlenecks. These issues\\nsystematically compromise system performance and scalability. RAG-Anything addresses this\\nfragmentation through a unified graph-based framework. Our approach processes all modalities with\\nconsistent structured modeling. This eliminates architectural constraints while preserving multimodal\\ninformation integrity. The result is seamless cross-modal reasoning across heterogeneous content.\\n5\\nCONCLUSION\\nRAG-Anything introduces a paradigm shift in multimodal retrieval through its unified graph-based\\nframework. Our core technical innovation is the dual-graph construction strategy that seamlessly\\nintegrates cross-modal and text-based knowledge graphs. Rather than forcing diverse modalities into\\ntext-centric pipelines that lose critical structural information, our approach fundamentally reconcep-\\ntualizes multimodal content as interconnected knowledge entities with rich semantic relationships.',\n",
       " 'tualizes multimodal content as interconnected knowledge entities with rich semantic relationships.\\nThe hybrid retrieval mechanism strategically combines structural navigation with semantic matching,\\nenabling precise reasoning over complex document layouts. Comprehensive evaluation demonstrates\\nsuperior performance on long-context documents, particularly those exceeding 100 pages where\\ntraditional methods fail. This work establishes a new foundation for multimodal RAG systems that\\ncan handle the heterogeneous nature of diverse information landscapes.\\nOur analysis in Appendix A.5 reveals critical challenges facing current multimodal RAG systems.\\nTwo fundamental issues emerge through systematic failure case examination. First, systems exhibit\\ntext-centric retrieval bias, preferentially accessing textual sources even when queries explicitly\\nrequire visual information. Second, rigid spatial processing patterns fail to adapt to non-standard',\n",
       " 'text-centric retrieval bias, preferentially accessing textual sources even when queries explicitly\\nrequire visual information. Second, rigid spatial processing patterns fail to adapt to non-standard\\ndocument layouts. These limitations manifest in cross-modal misalignment scenarios and structurally\\nambiguous tables. The findings highlight the need for adaptive spatial reasoning and layout-aware\\nparsing mechanisms to handle real-world multimodal document complexity.\\n11',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nREFERENCES\\nMohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammad-\\nkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin\\nAsgari. Ask in any modality: A comprehensive survey on multimodal retrieval-augmented genera-\\ntion. arXiv preprint arXiv:2502.08826, 2025.\\nYuanchen Bei, Weizhi Zhang, Siwen Wang, Weizhi Chen, Sheng Zhou, Hao Chen, Yong Li, Jiajun Bu,\\nShirui Pan, Yizhou Yu, et al. Graphs meet ai agents: Taxonomy, progress, and future opportunities.\\narXiv preprint arXiv:2506.18019, 2025.\\nDarren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt,\\nDasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: A\\ngraph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-',\n",
       " 'graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024.\\nZirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-\\naugmented generation. arXiv preprint arXiv:2410.05779, 2024.\\nBernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neuro-\\nbiologically inspired long-term memory for large language models. NeurIPS, 37:59532–59569,\\n2024.\\nKevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng\\nWang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang. Mm-vid: Advancing video\\nunderstanding with gpt-4v(ision). arXiv preprint arXiv:2310.19773, 2023.\\nYubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan\\nMa, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information Processing Systems, 37:95963–96010, 2024.',\n",
       " 'Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context document understanding\\nwith visualizations. Advances in Neural Information Processing Systems, 37:95963–96010, 2024.\\nCostas Mavromatis and George Karypis. Gnn-rag: Graph neural retrieval for large language model\\nreasoning. arXiv preprint arXiv:2405.20139, 2024.\\nXubin Ren, Lingrui Xu, Long Xia, Shuaiqiang Wang, Dawei Yin, and Chao Huang.\\nVide-\\norag:\\nRetrieval-augmented generation with extreme long-context videos.\\narXiv preprint\\narXiv:2502.01549, 2025.\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.\\nRaptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International\\nConference on Learning Representations, 2024.\\nXueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal\\nknowledge graphs. arXiv preprint arXiv:2507.20804, 2025.',\n",
       " 'Conference on Learning Representations, 2024.\\nXueyao Wan and Hang Yu. Mmgraphrag: Bridging vision and language with interpretable multimodal\\nknowledge graphs. arXiv preprint arXiv:2507.20804, 2025.\\nBin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu,\\nYuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content\\nextraction. arXiv preprint arXiv:2409.18839, 2024.\\nShu Wang, Yixiang Fang, Yingli Zhou, Xilin Liu, and Yuchi Ma. Archrag: Attributed community-\\nbased hierarchical retrieval-augmented generation. arXiv preprint arXiv:2502.09891, 2025.\\nShi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang,\\nXu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented generation on\\nmulti-modality documents. arXiv preprint arXiv:2410.10594, 2025.\\nQinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen,',\n",
       " 'multi-modality documents. arXiv preprint arXiv:2410.10594, 2025.\\nQinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen,\\nYilin Xiao, Chuang Zhou, Yi Chang, and Xiao Huang. A survey of graph retrieval-augmented\\ngeneration for customized large language models. arXiv preprint arXiv:2501.13958, 2025.\\nAnni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and\\nDong Yu. Docbench: A benchmark for evaluating llm-based document reading systems. arXiv\\npreprint arXiv:2407.10701, 2024.\\n12',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nA\\nAPPENDIX\\nThis appendix provides comprehensive supporting materials for our experimental evaluation and\\nimplementation details. Section A.1 presents detailed dataset statistics for the DocBench and\\nMMLongBench multi-modal benchmarks, including document type distributions and complexity\\nmetrics. Section A.2 showcases additional case studies that demonstrate RAG-Anything’s structure-\\naware capabilities across diverse multimodal content understanding tasks. Section A.3 documents the\\ncomplete set of multimodal analysis prompts for vision, table, and equation processing that enable\\ncontext-aware interpretation. Section A.4 provides the standardized accuracy evaluation prompt used\\nfor consistent response assessment across all experimental conditions.\\nA.1\\nDATASET CHARACTERISTICS AND STATISTICS\\nTable 5: Document type distribution and statistics for the DocBench benchmark.\\nType\\nAcad.\\nFin.\\nGov.\\nLaw.\\nNews\\n# Docs\\n49\\n40\\n44\\n46\\n50\\n# Questions\\n303\\n288\\n148\\n191',\n",
       " 'DATASET CHARACTERISTICS AND STATISTICS\\nTable 5: Document type distribution and statistics for the DocBench benchmark.\\nType\\nAcad.\\nFin.\\nGov.\\nLaw.\\nNews\\n# Docs\\n49\\n40\\n44\\n46\\n50\\n# Questions\\n303\\n288\\n148\\n191\\n172\\nAvg. Pages\\n11\\n192\\n69\\n58\\n1\\nTable 6: Document type distribution and statistics for the MMLongBench benchmark.\\nType\\nRes.\\nTut.\\nAcad.\\nGuid.\\nBroch.\\nAdmin.\\nFin.\\n# Docs\\n34\\n17\\n26\\n22\\n15\\n10\\n11\\n# Questions\\n292\\n138\\n199\\n155\\n100\\n81\\n117\\nAvg. Pages\\n39\\n58\\n35\\n78\\n30\\n17\\n87\\nTables 5 and 6 present the distribution of document types across the DocBench and MMLong-\\nBench benchmarks. • DocBench encompasses medium- to long-length documents spanning various\\ndomains, including legal, governmental, and financial files. Financial reports represent the most\\nextensive category, averaging 192 pages per document, while the News category consists of concise\\nsingle-page newspapers. • MMLongBench demonstrates a broader spectrum of document types and',\n",
       " 'extensive category, averaging 192 pages per document, while the News category consists of concise\\nsingle-page newspapers. • MMLongBench demonstrates a broader spectrum of document types and\\nlengths. Research reports, tutorials, and academic papers maintain moderate lengths of 35–58 pages\\non average, while guidebooks extend to approximately 78 pages. Brochures and administrative files\\nremain relatively compact, whereas financial reports again emerge as the longest category.\\nCollectively, these two benchmarks provide comprehensive coverage ranging from brief news arti-\\ncles to extensive technical and financial documentation. This establishes diverse and challenging\\nevaluation contexts for multimodal document understanding tasks.\\nA.2\\nADDITIONAL CASE STUDIES\\nMultimodel Document\\nEvidence figure in the document\\nQuestion: Which GCAN sub-model component removal\\nresulted in the lowest accuracy for Twitter15?\\nGPT-4o-mini🤔: \\nThe removal of the dual co-\\nattention mechanism from the',\n",
       " 'Evidence figure in the document\\nQuestion: Which GCAN sub-model component removal\\nresulted in the lowest accuracy for Twitter15?\\nGPT-4o-mini🤔: \\nThe removal of the dual co-\\nattention mechanism from the\\nGCAN sub-model resulted in the\\nlowest accuracy for Twitter15.\\nMMGraphRAG🤔: \\nThe removal of the source tweet from the GCAN\\nmodel resulted in the lowest accuracy for\\nTwitter15, as indicated by significant accuracy\\ndrops in the ablation analysis in Figure 4.\\nRAG-Anything(Correct😉):\\nThe removal of the source tweet embeddings\\nand dual co-attention, indicated as model\\nconfiguration \"-S-A,\" resulted in the lowest\\naccuracy for Twitter15.\\nLightRAG🤔：\\nRemoving the source tweet from the GCAN model\\nresulted in the lowest accuracy for Twitter15, as\\nindicated by a significant drop in performance when\\nusing the sub-model that excluded both source tweet\\nembeddings and dual co-attention. \\nFigure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy',\n",
       " 'using the sub-model that excluded both source tweet\\nembeddings and dual co-attention. \\nFigure 5: Visual reasoning case. RAG-Anything correctly identifies \"-S-A\" as the lowest accuracy\\nconfiguration, while baselines misinterpret spatial relationships.\\n• Demonstrating Visual Reasoning Capabilities. Figure 5 illustrates how RAG-Anything handles\\ncomplex visual reasoning tasks involving chart interpretation. The query asks which GCAN sub-\\nmodel component removal yields the lowest accuracy on Twitter15. Traditional approaches struggle\\n13',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nwith spatial relationships between visual elements. RAG-Anything addresses this challenge by\\nconstructing a structured graph representation of the bar plot. Bars, axis labels, and legends become\\ninterconnected nodes. These are linked by semantic relations such as bar-of and label-applies-to.\\nThis graph-based approach enables precise alignment between visual and textual elements. The\\nsystem correctly identifies the bar labeled \"-S-A\" (removing source tweet embeddings and dual\\nco-attention) and its corresponding accuracy value as the lowest performer. Baseline methods that\\nflatten visual information often misinterpret spatial relationships. They frequently conflate nearby\\ncomponents. RAG-Anything’s structured representation preserves critical visual-textual associations.\\nThis leads to accurate query resolution and proper attribution of performance drops to \"-S-A\".\\nMultimodel Document\\nEvidence table in the document',\n",
       " 'This leads to accurate query resolution and proper attribution of performance drops to \"-S-A\".\\nMultimodel Document\\nEvidence table in the document\\nQuestion: Which model combination for the Evidence Inference\\ndataset has the highest AUPRC value?\\nGPT-4o-mini🤔: \\nThe model combination for the Evidence Inference\\ndataset with the highest AUPRC value is the BERT-\\nto-BERT model, achieving an AUPRC of 0.455.\\nMMGraphRAG🤔: \\nThe model combination for the Evidence Inference\\ndataset with the highest AUPRC value is the \"Bert-To-\\nBert\" model, which achieved an AUPRC score of\\n0.708.\\nRAG-Anything(Correct😉):\\nThe model combination for the Evidence\\nInference dataset with the highest AUPRC\\nvalue is GloVe + LSTM - Attention, achieving\\na value of 0.506. \\nLightRAG🤔：\\nThe model combination \"BERT + LSTM -\\nAttention\" has the highest AUPRC value for the\\nEvidence Inference dataset at 0.429. \\n(Table with spatial ambiguity)\\nFigure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while',\n",
       " 'Evidence Inference dataset at 0.429. \\n(Table with spatial ambiguity)\\nFigure 6: Tabular navigation case. RAG-Anything locates the highest AUPRC value (0.506), while\\nthe compared approaches struggle with structural ambiguity.\\n• Handling Complex Tabular Structures. Figure 6 showcases RAG-Anything’s ability to navigate\\nintricate tabular data where structural disambiguation is crucial. The query seeks the model combi-\\nnation achieving the highest AUPRC value for the Evidence Inference dataset—a task complicated\\nby repeated row labels across multiple datasets within the same table. This scenario highlights a\\nfundamental limitation of conventional approaches that struggle with structural ambiguity in data.\\nRAG-Anything overcomes this by parsing the table into a comprehensive relational graph where\\nheaders and data cells become nodes connected through explicit row-of and column-of relationships.',\n",
       " 'RAG-Anything overcomes this by parsing the table into a comprehensive relational graph where\\nheaders and data cells become nodes connected through explicit row-of and column-of relationships.\\nThis structured representation enables the system to correctly isolate the Evidence Inference dataset\\ncontext and identify \"GloVe + LSTM – Attention\" with a score of 0.506 as the optimal configuration.\\nBy explicitly preserving hierarchical table constraints that other methods often collapse or misinterpret,\\nRAG-Anything ensures reliable reasoning across complex multi-dataset tabular structures.\\nA.3\\nCONTEXT-AWARE MULTIMODAL PROMPTING\\nThese three prompts orchestrate structured, context-aware multimodal analysis with JSON-formatted\\noutputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular,\\nand mathematical content while maintaining explicit alignment with surrounding information.',\n",
       " 'outputs. They systematically guide the model to extract comprehensive descriptions of visual, tabular,\\nand mathematical content while maintaining explicit alignment with surrounding information.\\nVision Analysis Prompt. Figure 7 orchestrates comprehensive image-context integration. The\\nprompt directs the model to systematically capture compositional elements, object relationships,\\nvisual attributes, stylistic features, dynamic actions, and technical components (e.g., charts), while es-\\ntablishing explicit connections to accompanying text. This approach transcends superficial description,\\nenabling contextually-grounded interpretations that enhance knowledge retrieval and substantiation.\\nTable Analysis Prompt. Figure 8 structures systematic tabular content decomposition across multiple\\nanalytical dimensions: structural organization, column semantics, critical values, statistical patterns,\\nand contextual relevance. Through precise terminology and numerical accuracy requirements, the',\n",
       " 'analytical dimensions: structural organization, column semantics, critical values, statistical patterns,\\nand contextual relevance. Through precise terminology and numerical accuracy requirements, the\\nprompt eliminates ambiguous generalizations and ensures faithful preservation of key indicators\\nwhile maintaining coherent alignment with surrounding discourse.\\nEquation Analysis Prompt. Figure 9 prioritizes semantic interpretation over syntactic restatement\\nof mathematical expressions. The prompt instructs comprehensive analysis of variable definitions,\\noperational logic, theoretical foundations, inter-formula relationships, and practical applications. This\\nmethodology ensures mathematical content becomes integral to broader argumentative frameworks,\\nsupporting enhanced retrieval accuracy, analytical traceability, and reasoning coherence.\\n14',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 7: Vision analysis prompt for context-aware image interpretation and knowledge extraction.\\nFigure 8: Table analysis prompt for structured content decomposition and semantic understanding.\\n15',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nFigure 9: Equation analysis prompt for mathematical expression interpretation and integration.\\nFigure 10: Accuracy evaluation prompt for consistent factual assessment across question types.\\nA.4\\nACCURACY EVALUATION PROMPT DESIGN\\nFigure 10 presents the standardized prompt specifically designed for systematic factual accuracy as-\\nsessment of generated responses across multiple domains. The prompt establishes explicit evaluation\\ncriteria that prioritize content correctness over stylistic considerations, producing binary accuracy\\n16',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nclassifications accompanied by concise analytical justifications. All accuracy evaluations throughout\\nour comprehensive experimental framework were conducted using GPT-4o-mini, ensuring consistent\\nand reliable assessment standards across diverse question categories and specialized domains.\\nA.5\\nCHALLENGES AND FUTURE DIRECTIONS FOR MULTI-MODAL RAG\\nWhile current multimodal RAG systems demonstrate promising capabilities, their limitations emerge\\nmost clearly through systematic analysis of failure cases. Understanding where and why these systems\\nbreak down is crucial for advancing the field beyond current performance plateaus. Examining failure\\npatterns helps identify fundamental architectural bottlenecks and design principles for more robust\\nmultimodal systems. Our investigation reveals two critical failure patterns exposing deeper systemic\\nissues in multimodal RAG architectures. These patterns are not merely edge cases but reflect',\n",
       " 'multimodal systems. Our investigation reveals two critical failure patterns exposing deeper systemic\\nissues in multimodal RAG architectures. These patterns are not merely edge cases but reflect\\nfundamental challenges in cross-modal information integration and structural reasoning:\\n• Text-Centric Retrieval Bias: Systems exhibit strong preference for textual sources, even when\\nqueries explicitly demand visual information. This reveals inadequate cross-modal attention.\\n• Document Structure Processing Challenges: Systems struggle with complex layouts and non-\\nlinear information flows. This exposes limitations in spatial reasoning and contextual understanding.\\nThese failure modes illuminate key insights about current multimodal AI. They provide concrete\\ndirections for architectural innovations that could substantially improve system robustness.\\nQuestion: According to Figure 3, what is the sequence followed to generate the\\nfinal labels for Named Entity Recognition?',\n",
       " 'Question: According to Figure 3, what is the sequence followed to generate the\\nfinal labels for Named Entity Recognition?\\nExpected answer: The sequence is Characters -> Char Embedding -> Char\\nLSTM-B/F -> Word Features -> Word Representation -> Word LSTM-F/B ->\\nWord CRF -> Label.\\nMultimodel Document\\nEvidence figure in the document\\nGPT-4o-mini🤔: \\n...involves inputting words and their representations\\ninto a Word CRF layer, followed by Word LSTM-B,\\nWord LSTM-F, and character-level representations,\\nculminating in the final prediction of entity labels.\\nMMGraphRAG🤔: \\n...involves using a combined representation of word\\nembeddings and character-level convolution, which is\\nthen processed through a Bi-LSTM layer, followed by\\na softmax or CRF layer to produce the final labels.\\nRAG-Anything🤔:\\n...involves processing input characters through\\nbi-directional LSTM layers (Char LSTM-B\\nand Char LSTM-F) followed by a softmax or\\nCRF layer that produces the final labels.\\nLightRAG🤔：',\n",
       " 'RAG-Anything🤔:\\n...involves processing input characters through\\nbi-directional LSTM layers (Char LSTM-B\\nand Char LSTM-F) followed by a softmax or\\nCRF layer that produces the final labels.\\nLightRAG🤔：\\n...a sequence where word representations are first\\nprocessed through a convolution layer, followed by a\\nBi-LSTM layer and then a softmax or CRF layer to\\nproduce the final labels. \\n(The order in the figure is from bottom to top)\\nFigure 11: Cross-modal noise case. All methods fail to retrieve the correct answer from the specified\\nimage, instead retrieving noisy textual evidence that misaligns with the structured visual content.\\nMultimodel Document\\nEvidence table in the document\\nQuestion: Which model\\'s style space shows a clearer separation\\nbetween different styles according to Figure 2?\\nGPT-4o-mini🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved by 6.26% when using the GEM fine-tuning\\nstrategy compared to training from scratch, achieving\\n65.35% versus 59.09%.\\nMMGraphRAG🤔:',\n",
       " 'GPT-4o-mini🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved by 6.26% when using the GEM fine-tuning\\nstrategy compared to training from scratch, achieving\\n65.35% versus 59.09%.\\nMMGraphRAG🤔: \\nThe Joint goal accuracy in the \"Train\" domain\\nimproved from 44.24% when training from scratch to\\n59.83% when using the GEM fine-tuning strategy,\\nresulting in an improvement of 15.59%.\\nRAG-Anything🤔:\\nThe Joint Goal Accuracy in the \"Train\" domain\\nimproved from 44.24% with training from\\nscratch to 58.10% using the GEM fine-tuning\\nstrategy, resulting in an improvement of\\n13.86%. \\nLightRAG🤔：\\nThe Joint goal accuracy in the \"Train\" domain\\nimproved from 44.24% using training from scratch to\\n50.51% with GEM fine-tuning, indicating an\\nimprovement of 6.27%. \\n(An irregular table)\\nFigure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table\\nlayout with merged cells and unclear column boundaries, leading to incorrect data extraction.',\n",
       " 'Figure 12: Ambiguous table structure case. All methods fail to correctly parse the confusing table\\nlayout with merged cells and unclear column boundaries, leading to incorrect data extraction.\\nCase 1: Cross-Modal Misalignment. Figure 11 presents a particularly revealing failure scenario\\nwhere all evaluated methods consistently produce incorrect answers despite having access to the\\nnecessary information. This universal failure across different architectures suggests fundamental\\nlimitations in how current systems handle noisy, heterogeneous multimodal data—a critical challenge\\nas real-world applications inevitably involve imperfect, inconsistent information sources. The failure\\nexposes two interconnected systemic issues that compound each other:\\nIssue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward\\ntextual passages. This occurs particularly when visual content lacks exact keyword matches. The',\n",
       " 'Issue 1: Retrieval Bias Toward Text. Current RAG systems demonstrate pronounced bias toward\\ntextual passages. This occurs particularly when visual content lacks exact keyword matches. The\\nbias persists even when queries contain explicit instructions to prioritize visual sources. This reveals\\na fundamental weakness in cross-modal attention mechanisms.\\nThe retrieved textual information, while topically related, often operates at a different granularity level\\nthan visual content. Images may contain precise, structured data such as specific numerical values,\\n17',\n",
       " 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\ndetailed diagrams, or exact spatial relationships. Corresponding text typically provides general,\\nconceptual descriptions. This semantic misalignment introduces noise that actively misleads the\\nreasoning process. The system attempts to reconcile incompatible levels of detail and specificity.\\nIssue 2: Rigid Spatial Processing Patterns. Current visual processing models exhibit fundamental\\nrigidity in spatial interpretation. Most systems default to sequential scanning patterns—top-to-\\nbottom and left-to-right—that mirror natural reading conventions. While effective for simple text\\ndocuments, this approach creates systematic failures with structurally complex real-world content.\\nMany documents require non-conventional processing strategies. Tables demand column-wise\\ninterpretation, technical diagrams follow specific directional flows, and scientific figures embed',\n",
       " 'Many documents require non-conventional processing strategies. Tables demand column-wise\\ninterpretation, technical diagrams follow specific directional flows, and scientific figures embed\\ncritical information in unexpectedly positioned annotations. These structural variations are prevalent\\nin professional documents, making adaptive spatial reasoning essential.\\nIn the observed failure case, the correct answer required integrating visual elements in reverse order\\nfrom the model’s default processing sequence. The system’s inability to recognize and adapt to this\\nstructural requirement led to systematic misinterpretation. This represents a fundamental architectural\\nlimitation where spatial reasoning remains static regardless of document context or query intent.\\nWhen spatial processing patterns are misaligned with document structure, the extracted information\\nbecomes not merely incomplete but actively misleading. This structural noise compounds other',\n",
       " 'When spatial processing patterns are misaligned with document structure, the extracted information\\nbecomes not merely incomplete but actively misleading. This structural noise compounds other\\nprocessing errors and can lead to confident but entirely incorrect conclusions.\\nCase 2: Structural Noise in Ambiguous Table Layouts. As shown in Figure 12, all methods failed\\nwhen confronted with a structurally ambiguous table. The primary failure stems from the table’s\\nconfusing design: the GEM row lacks dedicated cell boundaries, and the \"Joint\" and \"Slot\" columns\\nmerge without clear separation. These structural irregularities create parsing ambiguities that system-\\natically mislead extraction algorithms. This failure pattern reveals a critical vulnerability in current\\nRAG systems. When table structures deviate from standard formatting conventions—through merged\\ncells, unclear boundaries, or non-standard layouts—extraction methods consistently misinterpret cell',\n",
       " 'RAG systems. When table structures deviate from standard formatting conventions—through merged\\ncells, unclear boundaries, or non-standard layouts—extraction methods consistently misinterpret cell\\nrelationships and conflate distinct data values. This exposes the brittleness of current approaches\\nwhen faced with real-world document variations that deviate from clean, structured formats.\\nThe case highlights two essential directions for enhancing robustness. RAG systems require layout-\\naware parsing mechanisms that can recognize and adapt to structural irregularities rather than\\nimposing rigid formatting assumptions. Additionally, integrating visual processing capabilities\\ncould significantly improve noise resilience, as visual models can leverage spatial relationships and\\ncontextual design cues that are lost in purely structural representations.\\n18']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts= [doc.page_content for doc in chunks]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7a37ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:01<00:00, 12.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (506, 384)\n",
      "Adding 506 documents to vector store...\n",
      "Successfully added 506 documents to vector store\n",
      "Total documents in collection: 1071\n"
     ]
    }
   ],
   "source": [
    "texts= [doc.page_content for doc in chunks]\n",
    "\n",
    "#Generate the embeddings\n",
    "embeddings= embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "#store it in vecto DB\n",
    "vector_store.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d5c9a",
   "metadata": {},
   "source": [
    "# RETRIVAL PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb2c26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from vector_store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"Initialize the retriever\n",
    "        vector_store: Vector store from ChromaDB\n",
    "        embedding_manager: Manager for generating embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Retrieve relevant documents for a query\n",
    "        query: Search query\n",
    "        top_k: Number of top results to return\n",
    "        score_threshold: Minimum similarity score threshold\n",
    "        \n",
    "        returns: list of dicts containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving for query: '{query}'\")\n",
    "        print(f\"Top k: {top_k}, score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "\n",
    "            else:\n",
    "                print(\"No documents found!\")\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving!: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever= RAGRetriever(vector_store, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea3e173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x15266bac0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0011f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving for query: 'What is attetion is all you need?'\n",
      "Top k: 5, score threshold: 0.0\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 156.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"What is attetion is all you need?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4130a",
   "metadata": {},
   "source": [
    "Integration of VectorDb context pipeline with LLM output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd50d62",
   "metadata": {},
   "source": [
    "### Simple RAG pipeline with Grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e59d8838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grok API key loaded successfully! (first 10 chars: gsk_2...)\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "groq_key = os.getenv(\"groq_api_key\")\n",
    "if groq_key:\n",
    "    print(\"Grok API key loaded successfully! (first 10 chars:\", groq_key[:5] + \"...)\")\n",
    "else:\n",
    "    print(\"Failed to load Grok API key. Check your .env file.\")\n",
    "\n",
    "llm= ChatGroq(groq_api_key= groq_key, model_name= \"gemma2-9b-it\", temperature=0, max_tokens= 1024)\n",
    "\n",
    "#Simple RAG function: retrive context and generate response\n",
    "\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    results= retriever.retrieve(query, top_k= top_k)\n",
    "    context= \"\\n\\n\".join([doc['content']for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found!\"\n",
    "    \n",
    "    #Generate answer using groq llm\n",
    "\n",
    "    prompt=f\"\"\" Use the following contex to answer the question correctly.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\n",
    "      \"\"\"\n",
    "    response= llm.invoke([prompt.format(context=context, query= query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7d2633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving for query: 'What is attention mechanism?'\n",
      "Top k: 3, score threshold: 0.0\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "No relevant context found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"What is attention mechanism?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5496c72",
   "metadata": {},
   "source": [
    "Enhance RAF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df1e1810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving for query: 'What is attention mechanism?'\n",
      "Top k: 3, score threshold: 0.0\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering)\n",
      "Answer: No relevant context found\n",
      "Confidence: 0.0\n",
      "Sources: []\n",
      "Context Preview: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def rag_advance(\n",
    "    query,\n",
    "    retriever,\n",
    "    llm,\n",
    "    top_k=5,\n",
    "    min_score=0.2,\n",
    "    return_context=False\n",
    "):\n",
    "    # Retrieve documents\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "\n",
    "    if not results:\n",
    "        return {\n",
    "            'answer': 'No relevant context found',\n",
    "            'sources': [],\n",
    "            'confidence': 0.0,\n",
    "            'context': '' if return_context else None\n",
    "        }\n",
    "\n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "    # Extract sources safely\n",
    "    sources = []\n",
    "    scores = []\n",
    "\n",
    "    for doc in results:\n",
    "        score = doc.metadata.get(\"score\", 0.0)\n",
    "        scores.append(score)\n",
    "\n",
    "        sources.append({\n",
    "            'source': doc.metadata.get('source_file', doc.metadata.get('source', 'unknown')),\n",
    "            'page': doc.metadata.get('page', 'unknown'),\n",
    "            'score': score,\n",
    "            'preview': doc.page_content[:120] + \"...\"\n",
    "        })\n",
    "\n",
    "    confidence = max(scores) if scores else 0.0\n",
    "\n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"\n",
    "Use the following context to answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "result = rag_advance(\n",
    "    \"What is attention mechanism?\",\n",
    "    rag_retriever,\n",
    "    llm,\n",
    "    top_k=3,\n",
    "    return_context=True\n",
    ")\n",
    "\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Context Preview:\", result['context'][:120])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
